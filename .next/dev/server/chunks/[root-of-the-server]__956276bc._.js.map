{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 280, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/embeddings.ts"],"sourcesContent":["import { OpenAIEmbeddings } from \"@langchain/openai\";\n\n// Use OpenAI's text-embedding-3-small model (1536 dimensions)\nconst embeddings = new OpenAIEmbeddings({\n  modelName: \"text-embedding-3-small\",\n  openAIApiKey: process.env.OPENAI_API_KEY,\n});\n\n/**\n * Generate embedding for a single text\n */\nexport async function embedText(text: string): Promise<number[]> {\n  const result = await embeddings.embedQuery(text);\n  return result;\n}\n\n/**\n * Generate embeddings for multiple texts in batch\n */\nexport async function embedTexts(texts: string[]): Promise<number[][]> {\n  const results = await embeddings.embedDocuments(texts);\n  return results;\n}\n\n/**\n * Get the embedding dimension (for Qdrant collection setup)\n */\nexport function getEmbeddingDimension(): number {\n  return 1536;\n}\n"],"names":[],"mappings":";;;;;;;;AAAA;AAAA;;AAEA,8DAA8D;AAC9D,MAAM,aAAa,IAAI,iLAAgB,CAAC;IACtC,WAAW;IACX,cAAc,QAAQ,GAAG,CAAC,cAAc;AAC1C;AAKO,eAAe,UAAU,IAAY;IAC1C,MAAM,SAAS,MAAM,WAAW,UAAU,CAAC;IAC3C,OAAO;AACT;AAKO,eAAe,WAAW,KAAe;IAC9C,MAAM,UAAU,MAAM,WAAW,cAAc,CAAC;IAChD,OAAO;AACT;AAKO,SAAS;IACd,OAAO;AACT"}},
    {"offset": {"line": 377, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/qdrant.ts"],"sourcesContent":["import { QdrantClient } from \"@qdrant/js-client-rest\";\n\n// Qdrant configuration\nconst QDRANT_URL = process.env.QDRANT_URL || \"http://localhost:6333\";\nconst QDRANT_API_KEY = process.env.QDRANT_API_KEY;\nconst COLLECTION_NAME = process.env.QDRANT_COLLECTION_NAME || \"knowledge_base\";\n\n// OpenAI embedding dimension (text-embedding-3-small)\nconst EMBEDDING_DIMENSION = 1536;\n\nlet client: QdrantClient | null = null;\n\n/**\n * Get or create the Qdrant client singleton\n */\nexport function getClient(): QdrantClient {\n  if (!client) {\n    client = new QdrantClient({\n      url: QDRANT_URL,\n      apiKey: QDRANT_API_KEY,\n      // Skip version compatibility check (can fail behind reverse proxies)\n      checkCompatibility: false,\n      // Add headers for Cloudflare compatibility (undici/fetch needs these)\n      headers: {\n        \"User-Agent\": \"qdrant-js-client/1.16.1\",\n      },\n    });\n  }\n  return client;\n}\n\n/**\n * Get the collection name\n */\nexport function getCollectionName(): string {\n  return COLLECTION_NAME;\n}\n\n/**\n * Ensure the knowledge base collection exists with proper configuration\n */\nexport async function ensureCollection(): Promise<void> {\n  const qdrant = getClient();\n\n  try {\n    // Check if collection exists\n    const collections = await qdrant.getCollections();\n    const exists = collections.collections.some(\n      (c) => c.name === COLLECTION_NAME\n    );\n\n    if (!exists) {\n      // Create collection with cosine similarity for OpenAI embeddings\n      await qdrant.createCollection(COLLECTION_NAME, {\n        vectors: {\n          size: EMBEDDING_DIMENSION,\n          distance: \"Cosine\",\n        },\n        // Enable payload indexing for common filter fields\n        optimizers_config: {\n          indexing_threshold: 0,\n        },\n      });\n\n      // Create payload indexes for efficient filtering\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"doc_id\",\n        field_schema: \"keyword\",\n      });\n\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"status\",\n        field_schema: \"keyword\",\n      });\n\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"doc_type\",\n        field_schema: \"keyword\",\n      });\n\n      console.log(`Created Qdrant collection: ${COLLECTION_NAME}`);\n    }\n  } catch (error) {\n    console.error(\"Error ensuring Qdrant collection:\", error);\n    throw error;\n  }\n}\n\n/**\n * Upsert vectors into the collection\n */\nexport async function upsertVectors(\n  points: Array<{\n    id: string;\n    vector: number[];\n    payload: Record<string, unknown>;\n  }>\n): Promise<void> {\n  const qdrant = getClient();\n\n  await qdrant.upsert(COLLECTION_NAME, {\n    points: points.map((p) => ({\n      id: p.id,\n      vector: p.vector,\n      payload: p.payload,\n    })),\n  });\n}\n\n/**\n * Search for similar vectors\n */\nexport async function searchVectors(\n  vector: number[],\n  limit: number = 10,\n  filter?: Record<string, unknown>\n): Promise<\n  Array<{\n    id: string;\n    score: number;\n    payload: Record<string, unknown>;\n  }>\n> {\n  const qdrant = getClient();\n\n  const results = await qdrant.search(COLLECTION_NAME, {\n    vector,\n    limit,\n    filter: filter as never,\n    with_payload: true,\n    score_threshold: 0.5, // Only return relevant results\n  });\n\n  return results.map((r) => ({\n    id: String(r.id),\n    score: r.score,\n    payload: (r.payload as Record<string, unknown>) || {},\n  }));\n}\n\n/**\n * Delete vectors by filter (e.g., by doc_id)\n */\nexport async function deleteVectorsByFilter(\n  filter: Record<string, unknown>\n): Promise<void> {\n  const qdrant = getClient();\n\n  await qdrant.delete(COLLECTION_NAME, {\n    filter: filter as never,\n  });\n}\n\n/**\n * Get collection info for debugging\n */\nexport async function getCollectionInfo(): Promise<unknown> {\n  const qdrant = getClient();\n  return qdrant.getCollection(COLLECTION_NAME);\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;AAAA;;AAEA,uBAAuB;AACvB,MAAM,aAAa,QAAQ,GAAG,CAAC,UAAU,IAAI;AAC7C,MAAM,iBAAiB,QAAQ,GAAG,CAAC,cAAc;AACjD,MAAM,kBAAkB,QAAQ,GAAG,CAAC,sBAAsB,IAAI;AAE9D,sDAAsD;AACtD,MAAM,sBAAsB;AAE5B,IAAI,SAA8B;AAK3B,SAAS;IACd,IAAI,CAAC,QAAQ;QACX,SAAS,IAAI,qMAAY,CAAC;YACxB,KAAK;YACL,QAAQ;YACR,qEAAqE;YACrE,oBAAoB;YACpB,sEAAsE;YACtE,SAAS;gBACP,cAAc;YAChB;QACF;IACF;IACA,OAAO;AACT;AAKO,SAAS;IACd,OAAO;AACT;AAKO,eAAe;IACpB,MAAM,SAAS;IAEf,IAAI;QACF,6BAA6B;QAC7B,MAAM,cAAc,MAAM,OAAO,cAAc;QAC/C,MAAM,SAAS,YAAY,WAAW,CAAC,IAAI,CACzC,CAAC,IAAM,EAAE,IAAI,KAAK;QAGpB,IAAI,CAAC,QAAQ;YACX,iEAAiE;YACjE,MAAM,OAAO,gBAAgB,CAAC,iBAAiB;gBAC7C,SAAS;oBACP,MAAM;oBACN,UAAU;gBACZ;gBACA,mDAAmD;gBACnD,mBAAmB;oBACjB,oBAAoB;gBACtB;YACF;YAEA,iDAAiD;YACjD,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,QAAQ,GAAG,CAAC,CAAC,2BAA2B,EAAE,iBAAiB;QAC7D;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,qCAAqC;QACnD,MAAM;IACR;AACF;AAKO,eAAe,cACpB,MAIE;IAEF,MAAM,SAAS;IAEf,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnC,QAAQ,OAAO,GAAG,CAAC,CAAC,IAAM,CAAC;gBACzB,IAAI,EAAE,EAAE;gBACR,QAAQ,EAAE,MAAM;gBAChB,SAAS,EAAE,OAAO;YACpB,CAAC;IACH;AACF;AAKO,eAAe,cACpB,MAAgB,EAChB,QAAgB,EAAE,EAClB,MAAgC;IAQhC,MAAM,SAAS;IAEf,MAAM,UAAU,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnD;QACA;QACA,QAAQ;QACR,cAAc;QACd,iBAAiB;IACnB;IAEA,OAAO,QAAQ,GAAG,CAAC,CAAC,IAAM,CAAC;YACzB,IAAI,OAAO,EAAE,EAAE;YACf,OAAO,EAAE,KAAK;YACd,SAAS,AAAC,EAAE,OAAO,IAAgC,CAAC;QACtD,CAAC;AACH;AAKO,eAAe,sBACpB,MAA+B;IAE/B,MAAM,SAAS;IAEf,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnC,QAAQ;IACV;AACF;AAKO,eAAe;IACpB,MAAM,SAAS;IACf,OAAO,OAAO,aAAa,CAAC;AAC9B"}},
    {"offset": {"line": 509, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/storage.ts"],"sourcesContent":["import * as Minio from \"minio\";\n\n// MinIO configuration\nconst MINIO_ENDPOINT = process.env.MINIO_ENDPOINT || \"localhost\";\nconst MINIO_PORT = parseInt(process.env.MINIO_PORT || \"9000\", 10);\n// Support both typical MinIO env names and username/password aliases\nconst MINIO_ACCESS_KEY =\n  process.env.MINIO_ACCESS_KEY ||\n  process.env.MINIO_USERNAME || // alias for username\n  \"\";\nconst MINIO_SECRET_KEY =\n  process.env.MINIO_SECRET_KEY ||\n  process.env.MINIO_PASSWORD || // alias for password\n  \"\";\nconst MINIO_BUCKET = process.env.MINIO_BUCKET || \"knowledge-docs\";\nconst MINIO_USE_SSL = process.env.MINIO_USE_SSL === \"true\";\n\nlet client: Minio.Client | null = null;\n\n/**\n * Get or create the MinIO client singleton\n */\nexport function getStorageClient(): Minio.Client {\n  if (!client) {\n    const { endPoint, port, useSSL } = resolveMinioEndpoint();\n\n    client = new Minio.Client({\n      endPoint,\n      port,\n      useSSL,\n      accessKey: MINIO_ACCESS_KEY,\n      secretKey: MINIO_SECRET_KEY,\n    });\n  }\n  return client;\n}\n\nfunction resolveMinioEndpoint(): {\n  endPoint: string;\n  port: number;\n  useSSL: boolean;\n} {\n  // If MINIO_ENDPOINT includes scheme, parse it; otherwise use raw values.\n  if (\n    MINIO_ENDPOINT.startsWith(\"http://\") ||\n    MINIO_ENDPOINT.startsWith(\"https://\")\n  ) {\n    try {\n      const url = new URL(MINIO_ENDPOINT);\n      const endPoint = url.hostname;\n      const useSSL = url.protocol === \"https:\";\n      const port =\n        url.port && Number(url.port) > 0\n          ? Number(url.port)\n          : useSSL\n            ? 443\n            : 80;\n      return { endPoint, port, useSSL };\n    } catch {\n      // Fall through to defaults\n    }\n  }\n\n  return {\n    endPoint: MINIO_ENDPOINT,\n    port: MINIO_PORT,\n    useSSL: MINIO_USE_SSL,\n  };\n}\n\n/**\n * Get the bucket name\n */\nexport function getBucketName(): string {\n  return MINIO_BUCKET;\n}\n\n/**\n * Ensure the bucket exists\n */\nexport async function ensureBucket(): Promise<void> {\n  const minio = getStorageClient();\n\n  const exists = await minio.bucketExists(MINIO_BUCKET);\n  if (!exists) {\n    await minio.makeBucket(MINIO_BUCKET);\n    console.log(`Created MinIO bucket: ${MINIO_BUCKET}`);\n  }\n}\n\n/**\n * Upload a file to storage\n */\nexport async function uploadFile(\n  objectName: string,\n  buffer: Buffer,\n  contentType: string = \"application/octet-stream\"\n): Promise<string> {\n  const minio = getStorageClient();\n\n  await minio.putObject(MINIO_BUCKET, objectName, buffer, buffer.length, {\n    \"Content-Type\": contentType,\n  });\n\n  return objectName;\n}\n\n/**\n * Upload original PDF document\n * Path: documents/{doc_id}/original.pdf\n */\nexport async function uploadDocument(\n  docId: string,\n  buffer: Buffer,\n  originalFilename: string\n): Promise<string> {\n  const ext = originalFilename.split(\".\").pop() || \"pdf\";\n  const objectName = `documents/${docId}/original.${ext}`;\n\n  await uploadFile(objectName, buffer, getMimeType(ext));\n  return objectName;\n}\n\n/**\n * Upload page image\n * Path: documents/{doc_id}/pages/page-{n}.png\n */\nexport async function uploadPageImage(\n  docId: string,\n  pageNumber: number,\n  buffer: Buffer\n): Promise<string> {\n  const objectName = `documents/${docId}/pages/page-${pageNumber}.png`;\n\n  await uploadFile(objectName, buffer, \"image/png\");\n  return objectName;\n}\n\n/**\n * Get a presigned URL for accessing a file\n * Default expiry: 1 hour (3600 seconds)\n */\nexport async function getSignedUrl(\n  objectName: string,\n  expirySeconds: number = 3600\n): Promise<string> {\n  const minio = getStorageClient();\n\n  const url = await minio.presignedGetObject(\n    MINIO_BUCKET,\n    objectName,\n    expirySeconds\n  );\n\n  return url;\n}\n\n/**\n * Delete a file from storage\n */\nexport async function deleteFile(objectName: string): Promise<void> {\n  const minio = getStorageClient();\n  await minio.removeObject(MINIO_BUCKET, objectName);\n}\n\n/**\n * Delete all files for a document\n */\nexport async function deleteDocumentFiles(docId: string): Promise<void> {\n  const minio = getStorageClient();\n  const prefix = `documents/${docId}/`;\n\n  const objectsList: string[] = [];\n  const stream = minio.listObjects(MINIO_BUCKET, prefix, true);\n\n  for await (const obj of stream) {\n    if (obj.name) {\n      objectsList.push(obj.name);\n    }\n  }\n\n  if (objectsList.length > 0) {\n    await minio.removeObjects(MINIO_BUCKET, objectsList);\n  }\n}\n\n/**\n * Check if a file exists\n */\nexport async function fileExists(objectName: string): Promise<boolean> {\n  const minio = getStorageClient();\n\n  try {\n    await minio.statObject(MINIO_BUCKET, objectName);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get file as buffer\n */\nexport async function getFile(objectName: string): Promise<Buffer> {\n  const minio = getStorageClient();\n\n  const stream = await minio.getObject(MINIO_BUCKET, objectName);\n  const chunks: Buffer[] = [];\n\n  for await (const chunk of stream) {\n    chunks.push(Buffer.from(chunk));\n  }\n\n  return Buffer.concat(chunks);\n}\n\n/**\n * Get MIME type from file extension\n */\nfunction getMimeType(ext: string): string {\n  const mimeTypes: Record<string, string> = {\n    pdf: \"application/pdf\",\n    png: \"image/png\",\n    jpg: \"image/jpeg\",\n    jpeg: \"image/jpeg\",\n    pptx: \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n    docx: \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n    xlsx: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n    txt: \"text/plain\",\n    json: \"application/json\",\n  };\n\n  return mimeTypes[ext.toLowerCase()] || \"application/octet-stream\";\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;AAAA;;;;;;AAEA,sBAAsB;AACtB,MAAM,iBAAiB,QAAQ,GAAG,CAAC,cAAc,IAAI;AACrD,MAAM,aAAa,SAAS,QAAQ,GAAG,CAAC,UAAU,IAAI,QAAQ;AAC9D,qEAAqE;AACrE,MAAM,mBACJ,QAAQ,GAAG,CAAC,gBAAgB,IAC5B,QAAQ,GAAG,CAAC,cAAc,IAAI,qBAAqB;AACnD;AACF,MAAM,mBACJ,QAAQ,GAAG,CAAC,gBAAgB,IAC5B,QAAQ,GAAG,CAAC,cAAc,IAAI,qBAAqB;AACnD;AACF,MAAM,eAAe,QAAQ,GAAG,CAAC,YAAY,IAAI;AACjD,MAAM,gBAAgB,QAAQ,GAAG,CAAC,aAAa,KAAK;AAEpD,IAAI,SAA8B;AAK3B,SAAS;IACd,IAAI,CAAC,QAAQ;QACX,MAAM,EAAE,QAAQ,EAAE,IAAI,EAAE,MAAM,EAAE,GAAG;QAEnC,SAAS,IAAI,oHAAY,CAAC;YACxB;YACA;YACA;YACA,WAAW;YACX,WAAW;QACb;IACF;IACA,OAAO;AACT;AAEA,SAAS;IAKP,yEAAyE;IACzE,IACE,eAAe,UAAU,CAAC,cAC1B,eAAe,UAAU,CAAC,aAC1B;QACA,IAAI;YACF,MAAM,MAAM,IAAI,IAAI;YACpB,MAAM,WAAW,IAAI,QAAQ;YAC7B,MAAM,SAAS,IAAI,QAAQ,KAAK;YAChC,MAAM,OACJ,IAAI,IAAI,IAAI,OAAO,IAAI,IAAI,IAAI,IAC3B,OAAO,IAAI,IAAI,IACf,SACE,MACA;YACR,OAAO;gBAAE;gBAAU;gBAAM;YAAO;QAClC,EAAE,OAAM;QACN,2BAA2B;QAC7B;IACF;IAEA,OAAO;QACL,UAAU;QACV,MAAM;QACN,QAAQ;IACV;AACF;AAKO,SAAS;IACd,OAAO;AACT;AAKO,eAAe;IACpB,MAAM,QAAQ;IAEd,MAAM,SAAS,MAAM,MAAM,YAAY,CAAC;IACxC,IAAI,CAAC,QAAQ;QACX,MAAM,MAAM,UAAU,CAAC;QACvB,QAAQ,GAAG,CAAC,CAAC,sBAAsB,EAAE,cAAc;IACrD;AACF;AAKO,eAAe,WACpB,UAAkB,EAClB,MAAc,EACd,cAAsB,0BAA0B;IAEhD,MAAM,QAAQ;IAEd,MAAM,MAAM,SAAS,CAAC,cAAc,YAAY,QAAQ,OAAO,MAAM,EAAE;QACrE,gBAAgB;IAClB;IAEA,OAAO;AACT;AAMO,eAAe,eACpB,KAAa,EACb,MAAc,EACd,gBAAwB;IAExB,MAAM,MAAM,iBAAiB,KAAK,CAAC,KAAK,GAAG,MAAM;IACjD,MAAM,aAAa,CAAC,UAAU,EAAE,MAAM,UAAU,EAAE,KAAK;IAEvD,MAAM,WAAW,YAAY,QAAQ,YAAY;IACjD,OAAO;AACT;AAMO,eAAe,gBACpB,KAAa,EACb,UAAkB,EAClB,MAAc;IAEd,MAAM,aAAa,CAAC,UAAU,EAAE,MAAM,YAAY,EAAE,WAAW,IAAI,CAAC;IAEpE,MAAM,WAAW,YAAY,QAAQ;IACrC,OAAO;AACT;AAMO,eAAe,aACpB,UAAkB,EAClB,gBAAwB,IAAI;IAE5B,MAAM,QAAQ;IAEd,MAAM,MAAM,MAAM,MAAM,kBAAkB,CACxC,cACA,YACA;IAGF,OAAO;AACT;AAKO,eAAe,WAAW,UAAkB;IACjD,MAAM,QAAQ;IACd,MAAM,MAAM,YAAY,CAAC,cAAc;AACzC;AAKO,eAAe,oBAAoB,KAAa;IACrD,MAAM,QAAQ;IACd,MAAM,SAAS,CAAC,UAAU,EAAE,MAAM,CAAC,CAAC;IAEpC,MAAM,cAAwB,EAAE;IAChC,MAAM,SAAS,MAAM,WAAW,CAAC,cAAc,QAAQ;IAEvD,WAAW,MAAM,OAAO,OAAQ;QAC9B,IAAI,IAAI,IAAI,EAAE;YACZ,YAAY,IAAI,CAAC,IAAI,IAAI;QAC3B;IACF;IAEA,IAAI,YAAY,MAAM,GAAG,GAAG;QAC1B,MAAM,MAAM,aAAa,CAAC,cAAc;IAC1C;AACF;AAKO,eAAe,WAAW,UAAkB;IACjD,MAAM,QAAQ;IAEd,IAAI;QACF,MAAM,MAAM,UAAU,CAAC,cAAc;QACrC,OAAO;IACT,EAAE,OAAM;QACN,OAAO;IACT;AACF;AAKO,eAAe,QAAQ,UAAkB;IAC9C,MAAM,QAAQ;IAEd,MAAM,SAAS,MAAM,MAAM,SAAS,CAAC,cAAc;IACnD,MAAM,SAAmB,EAAE;IAE3B,WAAW,MAAM,SAAS,OAAQ;QAChC,OAAO,IAAI,CAAC,OAAO,IAAI,CAAC;IAC1B;IAEA,OAAO,OAAO,MAAM,CAAC;AACvB;AAEA;;CAEC,GACD,SAAS,YAAY,GAAW;IAC9B,MAAM,YAAoC;QACxC,KAAK;QACL,KAAK;QACL,KAAK;QACL,MAAM;QACN,MAAM;QACN,MAAM;QACN,MAAM;QACN,KAAK;QACL,MAAM;IACR;IAEA,OAAO,SAAS,CAAC,IAAI,WAAW,GAAG,IAAI;AACzC"}},
    {"offset": {"line": 679, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/retrieval.ts"],"sourcesContent":["import { embedText } from \"./embeddings\";\nimport { searchVectors } from \"./qdrant\";\nimport { getSignedUrl } from \"./storage\";\n\nexport interface RetrievedChunk {\n  chunkId: string;\n  docId: string;\n  docName: string;\n  pageNumber: number;\n  chunkIndex: number;\n  text: string;\n  imageUrl: string;\n  score: number;\n  parserUsed: string;\n}\n\nexport interface RetrievalOptions {\n  topK?: number;\n  docId?: string; // Filter by specific document\n  status?: string; // Filter by status (default: \"ready\")\n  diversify?: boolean; // Apply MMR diversification\n  diversityWeight?: number; // MMR lambda (0-1, higher = more diverse)\n}\n\nexport interface CitationInfo {\n  docName: string;\n  pageNumber: number;\n  imageUrl?: string; // Signed URL for page image\n  snippet: string;\n}\n\n/**\n * Retrieve relevant context for a query\n */\nexport async function retrieveContext(\n  query: string,\n  options: RetrievalOptions = {}\n): Promise<RetrievedChunk[]> {\n  const {\n    topK = 10,\n    docId,\n    status = \"ready\",\n    diversify = true,\n    diversityWeight = 0.7,\n  } = options;\n\n  // Generate query embedding\n  const queryEmbedding = await embedText(query);\n\n  // Build filter\n  const filter: Record<string, unknown> = {\n    must: [{ key: \"status\", match: { value: status } }],\n  };\n\n  if (docId) {\n    (filter.must as Array<unknown>).push({\n      key: \"doc_id\",\n      match: { value: docId },\n    });\n  }\n\n  // Retrieve more candidates if we're going to diversify\n  const candidateCount = diversify ? topK * 3 : topK;\n\n  // Search Qdrant\n  const results = await searchVectors(queryEmbedding, candidateCount, filter);\n\n  // Map results to chunks\n  let chunks: RetrievedChunk[] = results.map((r) => ({\n    chunkId: r.id,\n    docId: (r.payload.doc_id as string) || \"\",\n    docName: (r.payload.doc_name as string) || \"\",\n    pageNumber: (r.payload.page_number as number) || 0,\n    chunkIndex: (r.payload.chunk_index as number) || 0,\n    text: (r.payload.text as string) || \"\",\n    imageUrl: (r.payload.image_url as string) || \"\",\n    score: r.score,\n    parserUsed: (r.payload.parser_used as string) || \"\",\n  }));\n\n  // Apply MMR diversification if enabled\n  if (diversify && chunks.length > topK) {\n    chunks = applyMMR(chunks, topK, diversityWeight);\n  }\n\n  // Deduplicate by page (keep highest scoring chunk per page)\n  chunks = deduplicateByPage(chunks);\n\n  return chunks.slice(0, topK);\n}\n\n/**\n * Apply Maximal Marginal Relevance (MMR) to diversify results\n * This balances relevance (score) with diversity (avoid redundant chunks)\n */\nfunction applyMMR(\n  chunks: RetrievedChunk[],\n  topK: number,\n  lambda: number\n): RetrievedChunk[] {\n  if (chunks.length <= topK) return chunks;\n\n  const selected: RetrievedChunk[] = [];\n  const remaining = new Set(chunks.map((_, i) => i));\n\n  // Start with the highest scoring chunk\n  const firstIdx = 0;\n  selected.push(chunks[firstIdx]);\n  remaining.delete(firstIdx);\n\n  while (selected.length < topK && remaining.size > 0) {\n    let bestIdx = -1;\n    let bestScore = -Infinity;\n\n    for (const idx of remaining) {\n      const candidate = chunks[idx];\n\n      // Calculate relevance score (normalized)\n      const relevance = candidate.score;\n\n      // Calculate max similarity to already selected chunks\n      let maxSimilarity = 0;\n      for (const sel of selected) {\n        const sim = textSimilarity(candidate.text, sel.text);\n        maxSimilarity = Math.max(maxSimilarity, sim);\n      }\n\n      // MMR score: λ * relevance - (1-λ) * max_similarity\n      const mmrScore = lambda * relevance - (1 - lambda) * maxSimilarity;\n\n      if (mmrScore > bestScore) {\n        bestScore = mmrScore;\n        bestIdx = idx;\n      }\n    }\n\n    if (bestIdx !== -1) {\n      selected.push(chunks[bestIdx]);\n      remaining.delete(bestIdx);\n    } else {\n      break;\n    }\n  }\n\n  return selected;\n}\n\n/**\n * Simple text similarity based on Jaccard coefficient of words\n */\nfunction textSimilarity(text1: string, text2: string): number {\n  const words1 = new Set(text1.toLowerCase().split(/\\s+/));\n  const words2 = new Set(text2.toLowerCase().split(/\\s+/));\n\n  const intersection = new Set([...words1].filter((w) => words2.has(w)));\n  const union = new Set([...words1, ...words2]);\n\n  return intersection.size / union.size;\n}\n\n/**\n * Deduplicate chunks by page, keeping the highest scoring chunk per page\n */\nfunction deduplicateByPage(chunks: RetrievedChunk[]): RetrievedChunk[] {\n  const pageMap = new Map<string, RetrievedChunk>();\n\n  for (const chunk of chunks) {\n    const key = `${chunk.docId}:${chunk.pageNumber}`;\n    const existing = pageMap.get(key);\n\n    if (!existing || chunk.score > existing.score) {\n      pageMap.set(key, chunk);\n    }\n  }\n\n  // Sort by original score descending\n  return Array.from(pageMap.values()).sort((a, b) => b.score - a.score);\n}\n\n/**\n * Format retrieved chunks for use in LLM context\n */\nexport function formatContextForLLM(chunks: RetrievedChunk[]): string {\n  if (chunks.length === 0) {\n    return \"No relevant context found in the knowledge base.\";\n  }\n\n  const formattedChunks = chunks.map((chunk, index) => {\n    return `[${index + 1}] Document: \"${chunk.docName}\", Page ${chunk.pageNumber}\n${chunk.text}`;\n  });\n\n  return formattedChunks.join(\"\\n\\n---\\n\\n\");\n}\n\n/**\n * Extract citation information from retrieved chunks\n */\nexport async function extractCitations(\n  chunks: RetrievedChunk[]\n): Promise<CitationInfo[]> {\n  const citations: CitationInfo[] = [];\n\n  for (const chunk of chunks) {\n    // Get signed URL for page image\n    let signedImageUrl: string | undefined;\n    if (chunk.imageUrl) {\n      try {\n        signedImageUrl = await getSignedUrl(chunk.imageUrl);\n      } catch {\n        // Image URL generation failed, continue without it\n      }\n    }\n\n    citations.push({\n      docName: chunk.docName,\n      pageNumber: chunk.pageNumber,\n      imageUrl: signedImageUrl,\n      snippet:\n        chunk.text.length > 200\n          ? chunk.text.substring(0, 200) + \"...\"\n          : chunk.text,\n    });\n  }\n\n  return citations;\n}\n\n/**\n * Check if query can be answered from the knowledge base\n */\nexport async function hasRelevantContext(\n  query: string,\n  threshold: number = 0.6\n): Promise<boolean> {\n  const chunks = await retrieveContext(query, { topK: 3 });\n  return chunks.length > 0 && chunks[0].score >= threshold;\n}\n"],"names":[],"mappings":";;;;;;;;;;AAAA;AACA;AACA;;;;;;;;AAgCO,eAAe,gBACpB,KAAa,EACb,UAA4B,CAAC,CAAC;IAE9B,MAAM,EACJ,OAAO,EAAE,EACT,KAAK,EACL,SAAS,OAAO,EAChB,YAAY,IAAI,EAChB,kBAAkB,GAAG,EACtB,GAAG;IAEJ,2BAA2B;IAC3B,MAAM,iBAAiB,MAAM,IAAA,uIAAS,EAAC;IAEvC,eAAe;IACf,MAAM,SAAkC;QACtC,MAAM;YAAC;gBAAE,KAAK;gBAAU,OAAO;oBAAE,OAAO;gBAAO;YAAE;SAAE;IACrD;IAEA,IAAI,OAAO;QACR,OAAO,IAAI,CAAoB,IAAI,CAAC;YACnC,KAAK;YACL,OAAO;gBAAE,OAAO;YAAM;QACxB;IACF;IAEA,uDAAuD;IACvD,MAAM,iBAAiB,YAAY,OAAO,IAAI;IAE9C,gBAAgB;IAChB,MAAM,UAAU,MAAM,IAAA,uIAAa,EAAC,gBAAgB,gBAAgB;IAEpE,wBAAwB;IACxB,IAAI,SAA2B,QAAQ,GAAG,CAAC,CAAC,IAAM,CAAC;YACjD,SAAS,EAAE,EAAE;YACb,OAAO,AAAC,EAAE,OAAO,CAAC,MAAM,IAAe;YACvC,SAAS,AAAC,EAAE,OAAO,CAAC,QAAQ,IAAe;YAC3C,YAAY,AAAC,EAAE,OAAO,CAAC,WAAW,IAAe;YACjD,YAAY,AAAC,EAAE,OAAO,CAAC,WAAW,IAAe;YACjD,MAAM,AAAC,EAAE,OAAO,CAAC,IAAI,IAAe;YACpC,UAAU,AAAC,EAAE,OAAO,CAAC,SAAS,IAAe;YAC7C,OAAO,EAAE,KAAK;YACd,YAAY,AAAC,EAAE,OAAO,CAAC,WAAW,IAAe;QACnD,CAAC;IAED,uCAAuC;IACvC,IAAI,aAAa,OAAO,MAAM,GAAG,MAAM;QACrC,SAAS,SAAS,QAAQ,MAAM;IAClC;IAEA,4DAA4D;IAC5D,SAAS,kBAAkB;IAE3B,OAAO,OAAO,KAAK,CAAC,GAAG;AACzB;AAEA;;;CAGC,GACD,SAAS,SACP,MAAwB,EACxB,IAAY,EACZ,MAAc;IAEd,IAAI,OAAO,MAAM,IAAI,MAAM,OAAO;IAElC,MAAM,WAA6B,EAAE;IACrC,MAAM,YAAY,IAAI,IAAI,OAAO,GAAG,CAAC,CAAC,GAAG,IAAM;IAE/C,uCAAuC;IACvC,MAAM,WAAW;IACjB,SAAS,IAAI,CAAC,MAAM,CAAC,SAAS;IAC9B,UAAU,MAAM,CAAC;IAEjB,MAAO,SAAS,MAAM,GAAG,QAAQ,UAAU,IAAI,GAAG,EAAG;QACnD,IAAI,UAAU,CAAC;QACf,IAAI,YAAY,CAAC;QAEjB,KAAK,MAAM,OAAO,UAAW;YAC3B,MAAM,YAAY,MAAM,CAAC,IAAI;YAE7B,yCAAyC;YACzC,MAAM,YAAY,UAAU,KAAK;YAEjC,sDAAsD;YACtD,IAAI,gBAAgB;YACpB,KAAK,MAAM,OAAO,SAAU;gBAC1B,MAAM,MAAM,eAAe,UAAU,IAAI,EAAE,IAAI,IAAI;gBACnD,gBAAgB,KAAK,GAAG,CAAC,eAAe;YAC1C;YAEA,oDAAoD;YACpD,MAAM,WAAW,SAAS,YAAY,CAAC,IAAI,MAAM,IAAI;YAErD,IAAI,WAAW,WAAW;gBACxB,YAAY;gBACZ,UAAU;YACZ;QACF;QAEA,IAAI,YAAY,CAAC,GAAG;YAClB,SAAS,IAAI,CAAC,MAAM,CAAC,QAAQ;YAC7B,UAAU,MAAM,CAAC;QACnB,OAAO;YACL;QACF;IACF;IAEA,OAAO;AACT;AAEA;;CAEC,GACD,SAAS,eAAe,KAAa,EAAE,KAAa;IAClD,MAAM,SAAS,IAAI,IAAI,MAAM,WAAW,GAAG,KAAK,CAAC;IACjD,MAAM,SAAS,IAAI,IAAI,MAAM,WAAW,GAAG,KAAK,CAAC;IAEjD,MAAM,eAAe,IAAI,IAAI;WAAI;KAAO,CAAC,MAAM,CAAC,CAAC,IAAM,OAAO,GAAG,CAAC;IAClE,MAAM,QAAQ,IAAI,IAAI;WAAI;WAAW;KAAO;IAE5C,OAAO,aAAa,IAAI,GAAG,MAAM,IAAI;AACvC;AAEA;;CAEC,GACD,SAAS,kBAAkB,MAAwB;IACjD,MAAM,UAAU,IAAI;IAEpB,KAAK,MAAM,SAAS,OAAQ;QAC1B,MAAM,MAAM,GAAG,MAAM,KAAK,CAAC,CAAC,EAAE,MAAM,UAAU,EAAE;QAChD,MAAM,WAAW,QAAQ,GAAG,CAAC;QAE7B,IAAI,CAAC,YAAY,MAAM,KAAK,GAAG,SAAS,KAAK,EAAE;YAC7C,QAAQ,GAAG,CAAC,KAAK;QACnB;IACF;IAEA,oCAAoC;IACpC,OAAO,MAAM,IAAI,CAAC,QAAQ,MAAM,IAAI,IAAI,CAAC,CAAC,GAAG,IAAM,EAAE,KAAK,GAAG,EAAE,KAAK;AACtE;AAKO,SAAS,oBAAoB,MAAwB;IAC1D,IAAI,OAAO,MAAM,KAAK,GAAG;QACvB,OAAO;IACT;IAEA,MAAM,kBAAkB,OAAO,GAAG,CAAC,CAAC,OAAO;QACzC,OAAO,CAAC,CAAC,EAAE,QAAQ,EAAE,aAAa,EAAE,MAAM,OAAO,CAAC,QAAQ,EAAE,MAAM,UAAU,CAAC;AACjF,EAAE,MAAM,IAAI,EAAE;IACZ;IAEA,OAAO,gBAAgB,IAAI,CAAC;AAC9B;AAKO,eAAe,iBACpB,MAAwB;IAExB,MAAM,YAA4B,EAAE;IAEpC,KAAK,MAAM,SAAS,OAAQ;QAC1B,gCAAgC;QAChC,IAAI;QACJ,IAAI,MAAM,QAAQ,EAAE;YAClB,IAAI;gBACF,iBAAiB,MAAM,IAAA,uIAAY,EAAC,MAAM,QAAQ;YACpD,EAAE,OAAM;YACN,mDAAmD;YACrD;QACF;QAEA,UAAU,IAAI,CAAC;YACb,SAAS,MAAM,OAAO;YACtB,YAAY,MAAM,UAAU;YAC5B,UAAU;YACV,SACE,MAAM,IAAI,CAAC,MAAM,GAAG,MAChB,MAAM,IAAI,CAAC,SAAS,CAAC,GAAG,OAAO,QAC/B,MAAM,IAAI;QAClB;IACF;IAEA,OAAO;AACT;AAKO,eAAe,mBACpB,KAAa,EACb,YAAoB,GAAG;IAEvB,MAAM,SAAS,MAAM,gBAAgB,OAAO;QAAE,MAAM;IAAE;IACtD,OAAO,OAAO,MAAM,GAAG,KAAK,MAAM,CAAC,EAAE,CAAC,KAAK,IAAI;AACjD"}},
    {"offset": {"line": 858, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/agentic-retrieval.ts"],"sourcesContent":["import { ChatOpenAI } from \"@langchain/openai\";\nimport { retrieveContext, type RetrievedChunk } from \"./retrieval\";\n\n/**\n * Agentic RAG with Self-Reflection\n *\n * Flow: Query → Analyze → Retrieve → Reflect → (Re-search if needed) → Answer\n */\n\n// Configuration\nconst MAX_ITERATIONS = 3;\nconst CONFIDENCE_THRESHOLD = 0.7;\nconst TOP_K_PER_SEARCH = 8;\n\n// LLM for agent reasoning\nconst agentLLM = new ChatOpenAI({\n  modelName: \"gpt-4.1-mini\", // Fast model for agent reasoning\n  temperature: 0,\n});\n\n/**\n * Query analysis result\n */\nexport interface QueryAnalysis {\n  originalQuery: string;\n  intent: string;\n  keyEntities: string[];\n  subQuestions: string[];\n  isMultiPart: boolean;\n  searchQueries: string[];\n}\n\n/**\n * Reflection result\n */\nexport interface ReflectionResult {\n  confidence: number; // 0-1\n  isSufficient: boolean;\n  answeredAspects: string[];\n  missingInformation: string[];\n  suggestedQueries: string[];\n  reasoning: string;\n}\n\n/**\n * Agent retrieval result\n */\nexport interface AgentRetrievalResult {\n  chunks: RetrievedChunk[];\n  iterations: number;\n  queryAnalysis: QueryAnalysis;\n  reflections: ReflectionResult[];\n  finalConfidence: number;\n  searchQueries: string[];\n}\n\n/**\n * Analyze the user's query to understand intent and decompose if needed\n */\nexport async function analyzeQuery(query: string): Promise<QueryAnalysis> {\n  const prompt = `You are a query analyzer for a knowledge base search system.\n\nIMPORTANT: The knowledge base contains REPORTS and DOCUMENTS from various organizations.\nWhen a user asks about \"impact of [Organization]\" or \"[Company] influence\", they likely want\ninformation FROM that organization's reports, not ABOUT the organization itself.\n\nFor example:\n- \"impact of Kantar on market\" → User likely wants market insights FROM Kantar's reports\n- \"What does McKinsey say about AI?\" → User wants AI insights from McKinsey reports\n- Rewrite these to search for the actual content: \"market trends\", \"media predictions\", etc.\n\nAnalyze this user query and provide:\n1. The user's TRUE intent (what information are they actually seeking?)\n2. Key entities or concepts (focus on topics, not publisher names)\n3. If this is a multi-part question, break it into sub-questions\n4. Generate 2-4 CONTENT-FOCUSED search queries (avoid searching for publisher names)\n\nUser Query: \"${query}\"\n\nRespond in JSON format:\n{\n  \"intent\": \"what the user actually wants to learn about\",\n  \"keyEntities\": [\"topic1\", \"topic2\"],\n  \"subQuestions\": [\"sub-question 1\"] or [],\n  \"isMultiPart\": true/false,\n  \"searchQueries\": [\"content-focused query 1\", \"content-focused query 2\"]\n}\n\nOnly output valid JSON, no markdown or explanation.`;\n\n  try {\n    const response = await agentLLM.invoke(prompt);\n    const content = typeof response.content === 'string'\n      ? response.content\n      : JSON.stringify(response.content);\n\n    // Parse JSON from response (handle potential markdown code blocks)\n    const jsonStr = content.replace(/```json\\n?|\\n?```/g, '').trim();\n    const analysis = JSON.parse(jsonStr);\n\n    return {\n      originalQuery: query,\n      intent: analysis.intent || query,\n      keyEntities: analysis.keyEntities || [],\n      subQuestions: analysis.subQuestions || [],\n      isMultiPart: analysis.isMultiPart || false,\n      searchQueries: analysis.searchQueries || [query],\n    };\n  } catch (error) {\n    console.warn(\"Query analysis failed, using original query:\", error);\n    return {\n      originalQuery: query,\n      intent: query,\n      keyEntities: [],\n      subQuestions: [],\n      isMultiPart: false,\n      searchQueries: [query],\n    };\n  }\n}\n\n/**\n * Evaluate if retrieved chunks sufficiently answer the query\n */\nexport async function reflectOnResults(\n  query: string,\n  analysis: QueryAnalysis,\n  chunks: RetrievedChunk[],\n  previousReflections: ReflectionResult[] = []\n): Promise<ReflectionResult> {\n  // Format chunks for evaluation\n  const chunksText = chunks\n    .slice(0, 10) // Limit for context window\n    .map((c, i) => `[${i + 1}] ${c.docName} (Page ${c.pageNumber}): ${c.text.slice(0, 500)}...`)\n    .join(\"\\n\\n\");\n\n  const previousAttempts = previousReflections.length > 0\n    ? `\\nPrevious search attempts:\\n${previousReflections.map((r, i) =>\n        `Attempt ${i + 1}: Confidence ${r.confidence}, Missing: ${r.missingInformation.join(\", \")}`\n      ).join(\"\\n\")}\\n`\n    : \"\";\n\n  const prompt = `You are evaluating if retrieved documents answer a user's question.\n\nUser Query: \"${query}\"\nIntent: ${analysis.intent}\n${analysis.subQuestions.length > 0 ? `Sub-questions: ${analysis.subQuestions.join(\", \")}` : \"\"}\n${previousAttempts}\n\nRetrieved Documents:\n${chunksText}\n\nEvaluate:\n1. Does this information answer the user's question? (confidence 0-1)\n2. What aspects of the question are answered?\n3. What information is still missing?\n4. If missing info, what search queries would help find it?\n\nRespond in JSON:\n{\n  \"confidence\": 0.0-1.0,\n  \"isSufficient\": true/false,\n  \"answeredAspects\": [\"aspect 1\", \"aspect 2\"],\n  \"missingInformation\": [\"missing info 1\"] or [],\n  \"suggestedQueries\": [\"refined query\"] or [],\n  \"reasoning\": \"brief explanation\"\n}\n\nOnly output valid JSON.`;\n\n  try {\n    const response = await agentLLM.invoke(prompt);\n    const content = typeof response.content === 'string'\n      ? response.content\n      : JSON.stringify(response.content);\n\n    const jsonStr = content.replace(/```json\\n?|\\n?```/g, '').trim();\n    const reflection = JSON.parse(jsonStr);\n\n    return {\n      confidence: Math.min(1, Math.max(0, reflection.confidence || 0)),\n      isSufficient: reflection.isSufficient ?? (reflection.confidence >= CONFIDENCE_THRESHOLD),\n      answeredAspects: reflection.answeredAspects || [],\n      missingInformation: reflection.missingInformation || [],\n      suggestedQueries: reflection.suggestedQueries || [],\n      reasoning: reflection.reasoning || \"\",\n    };\n  } catch (error) {\n    console.warn(\"Reflection failed:\", error);\n    // Default to accepting results if reflection fails\n    return {\n      confidence: 0.6,\n      isSufficient: true,\n      answeredAspects: [],\n      missingInformation: [],\n      suggestedQueries: [],\n      reasoning: \"Reflection failed, proceeding with available results\",\n    };\n  }\n}\n\n/**\n * Merge and deduplicate chunks from multiple searches\n */\nfunction mergeChunks(\n  existingChunks: RetrievedChunk[],\n  newChunks: RetrievedChunk[]\n): RetrievedChunk[] {\n  const seen = new Set<string>();\n  const merged: RetrievedChunk[] = [];\n\n  // Add existing chunks first (they have priority)\n  for (const chunk of existingChunks) {\n    const key = `${chunk.docId}-${chunk.pageNumber}-${chunk.chunkIndex}`;\n    if (!seen.has(key)) {\n      seen.add(key);\n      merged.push(chunk);\n    }\n  }\n\n  // Add new chunks that aren't duplicates\n  for (const chunk of newChunks) {\n    const key = `${chunk.docId}-${chunk.pageNumber}-${chunk.chunkIndex}`;\n    if (!seen.has(key)) {\n      seen.add(key);\n      merged.push(chunk);\n    }\n  }\n\n  // Sort by score and limit\n  return merged\n    .sort((a, b) => b.score - a.score)\n    .slice(0, TOP_K_PER_SEARCH * 2); // Keep more for multi-search\n}\n\n/**\n * Main agentic retrieval function\n *\n * Performs iterative retrieval with self-reflection until sufficient\n * information is gathered or max iterations reached.\n */\nexport async function agenticRetrieve(\n  query: string,\n  options: {\n    docId?: string;\n    maxIterations?: number;\n    confidenceThreshold?: number;\n    verbose?: boolean;\n  } = {}\n): Promise<AgentRetrievalResult> {\n  const {\n    docId,\n    maxIterations = MAX_ITERATIONS,\n    confidenceThreshold = CONFIDENCE_THRESHOLD,\n    verbose = true,\n  } = options;\n\n  const reflections: ReflectionResult[] = [];\n  const allSearchQueries: string[] = [];\n  let allChunks: RetrievedChunk[] = [];\n  let iteration = 0;\n  let finalConfidence = 0;\n\n  // Step 1: Analyze the query\n  if (verbose) console.log(`[Agent] Analyzing query: \"${query}\"`);\n  const analysis = await analyzeQuery(query);\n  if (verbose) {\n    console.log(`[Agent] Intent: ${analysis.intent}`);\n    console.log(`[Agent] Search queries: ${analysis.searchQueries.join(\", \")}`);\n  }\n\n  // Step 2: Initial retrieval with all search queries\n  for (const searchQuery of analysis.searchQueries) {\n    allSearchQueries.push(searchQuery);\n    if (verbose) console.log(`[Agent] Searching: \"${searchQuery}\"`);\n\n    const chunks = await retrieveContext(searchQuery, {\n      topK: TOP_K_PER_SEARCH,\n      docId,\n      diversify: true,\n    });\n\n    allChunks = mergeChunks(allChunks, chunks);\n  }\n\n  if (verbose) console.log(`[Agent] Retrieved ${allChunks.length} chunks`);\n\n  // Step 3: Self-reflection loop\n  while (iteration < maxIterations) {\n    iteration++;\n    if (verbose) console.log(`[Agent] Reflection iteration ${iteration}...`);\n\n    // Reflect on current results\n    const reflection = await reflectOnResults(\n      query,\n      analysis,\n      allChunks,\n      reflections\n    );\n    reflections.push(reflection);\n    finalConfidence = reflection.confidence;\n\n    if (verbose) {\n      console.log(`[Agent] Confidence: ${reflection.confidence.toFixed(2)}`);\n      console.log(`[Agent] Sufficient: ${reflection.isSufficient}`);\n      if (reflection.missingInformation.length > 0) {\n        console.log(`[Agent] Missing: ${reflection.missingInformation.join(\", \")}`);\n      }\n    }\n\n    // Check if we should stop\n    if (reflection.isSufficient || reflection.confidence >= confidenceThreshold) {\n      if (verbose) console.log(`[Agent] Results sufficient, stopping.`);\n      break;\n    }\n\n    // Check if we have new queries to try\n    if (reflection.suggestedQueries.length === 0) {\n      if (verbose) console.log(`[Agent] No new queries suggested, stopping.`);\n      break;\n    }\n\n    // Step 4: Re-search with suggested queries\n    for (const newQuery of reflection.suggestedQueries) {\n      // Avoid repeating the same query\n      if (allSearchQueries.includes(newQuery)) continue;\n\n      allSearchQueries.push(newQuery);\n      if (verbose) console.log(`[Agent] Re-searching: \"${newQuery}\"`);\n\n      const newChunks = await retrieveContext(newQuery, {\n        topK: TOP_K_PER_SEARCH,\n        docId,\n        diversify: true,\n      });\n\n      allChunks = mergeChunks(allChunks, newChunks);\n    }\n\n    if (verbose) console.log(`[Agent] Total chunks: ${allChunks.length}`);\n  }\n\n  if (verbose) {\n    console.log(`[Agent] Completed in ${iteration} iteration(s)`);\n    console.log(`[Agent] Final confidence: ${finalConfidence.toFixed(2)}`);\n    console.log(`[Agent] Total search queries: ${allSearchQueries.length}`);\n  }\n\n  return {\n    chunks: allChunks,\n    iterations: iteration,\n    queryAnalysis: analysis,\n    reflections,\n    finalConfidence,\n    searchQueries: allSearchQueries,\n  };\n}\n\n/**\n * Format agent retrieval result for LLM context\n */\nexport function formatAgentContextForLLM(result: AgentRetrievalResult): string {\n  if (result.chunks.length === 0) {\n    return \"No relevant documents found in the knowledge base.\";\n  }\n\n  const contextBlocks = result.chunks.map((chunk, index) => {\n    return `[${index + 1}] Document: \"${chunk.docName}\", Page ${chunk.pageNumber}\n${chunk.text}`;\n  });\n\n  const searchInfo = `Search conducted with ${result.iterations} iteration(s), ${result.searchQueries.length} queries, confidence ${result.finalConfidence.toFixed(2)}`;\n\n  return `${searchInfo}\\n\\n---\\n\\n${contextBlocks.join(\"\\n\\n---\\n\\n\")}`;\n}\n\n/**\n * Get agent metadata for response\n */\nexport function getAgentMetadata(result: AgentRetrievalResult): {\n  iterations: number;\n  confidence: number;\n  searchQueries: string[];\n  intent: string;\n} {\n  return {\n    iterations: result.iterations,\n    confidence: result.finalConfidence,\n    searchQueries: result.searchQueries,\n    intent: result.queryAnalysis.intent,\n  };\n}\n"],"names":[],"mappings":";;;;;;;;;;;;AAAA;AAAA;AACA;;;;;;;AAEA;;;;CAIC,GAED,gBAAgB;AAChB,MAAM,iBAAiB;AACvB,MAAM,uBAAuB;AAC7B,MAAM,mBAAmB;AAEzB,0BAA0B;AAC1B,MAAM,WAAW,IAAI,qLAAU,CAAC;IAC9B,WAAW;IACX,aAAa;AACf;AAyCO,eAAe,aAAa,KAAa;IAC9C,MAAM,SAAS,CAAC;;;;;;;;;;;;;;;;;aAiBL,EAAE,MAAM;;;;;;;;;;;mDAW8B,CAAC;IAElD,IAAI;QACF,MAAM,WAAW,MAAM,SAAS,MAAM,CAAC;QACvC,MAAM,UAAU,OAAO,SAAS,OAAO,KAAK,WACxC,SAAS,OAAO,GAChB,KAAK,SAAS,CAAC,SAAS,OAAO;QAEnC,mEAAmE;QACnE,MAAM,UAAU,QAAQ,OAAO,CAAC,sBAAsB,IAAI,IAAI;QAC9D,MAAM,WAAW,KAAK,KAAK,CAAC;QAE5B,OAAO;YACL,eAAe;YACf,QAAQ,SAAS,MAAM,IAAI;YAC3B,aAAa,SAAS,WAAW,IAAI,EAAE;YACvC,cAAc,SAAS,YAAY,IAAI,EAAE;YACzC,aAAa,SAAS,WAAW,IAAI;YACrC,eAAe,SAAS,aAAa,IAAI;gBAAC;aAAM;QAClD;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,IAAI,CAAC,gDAAgD;QAC7D,OAAO;YACL,eAAe;YACf,QAAQ;YACR,aAAa,EAAE;YACf,cAAc,EAAE;YAChB,aAAa;YACb,eAAe;gBAAC;aAAM;QACxB;IACF;AACF;AAKO,eAAe,iBACpB,KAAa,EACb,QAAuB,EACvB,MAAwB,EACxB,sBAA0C,EAAE;IAE5C,+BAA+B;IAC/B,MAAM,aAAa,OAChB,KAAK,CAAC,GAAG,IAAI,2BAA2B;KACxC,GAAG,CAAC,CAAC,GAAG,IAAM,CAAC,CAAC,EAAE,IAAI,EAAE,EAAE,EAAE,EAAE,OAAO,CAAC,OAAO,EAAE,EAAE,UAAU,CAAC,GAAG,EAAE,EAAE,IAAI,CAAC,KAAK,CAAC,GAAG,KAAK,GAAG,CAAC,EAC1F,IAAI,CAAC;IAER,MAAM,mBAAmB,oBAAoB,MAAM,GAAG,IAClD,CAAC,6BAA6B,EAAE,oBAAoB,GAAG,CAAC,CAAC,GAAG,IAC1D,CAAC,QAAQ,EAAE,IAAI,EAAE,aAAa,EAAE,EAAE,UAAU,CAAC,WAAW,EAAE,EAAE,kBAAkB,CAAC,IAAI,CAAC,OAAO,EAC3F,IAAI,CAAC,MAAM,EAAE,CAAC,GAChB;IAEJ,MAAM,SAAS,CAAC;;aAEL,EAAE,MAAM;QACb,EAAE,SAAS,MAAM,CAAC;AAC1B,EAAE,SAAS,YAAY,CAAC,MAAM,GAAG,IAAI,CAAC,eAAe,EAAE,SAAS,YAAY,CAAC,IAAI,CAAC,OAAO,GAAG,GAAG;AAC/F,EAAE,iBAAiB;;;AAGnB,EAAE,WAAW;;;;;;;;;;;;;;;;;;uBAkBU,CAAC;IAEtB,IAAI;QACF,MAAM,WAAW,MAAM,SAAS,MAAM,CAAC;QACvC,MAAM,UAAU,OAAO,SAAS,OAAO,KAAK,WACxC,SAAS,OAAO,GAChB,KAAK,SAAS,CAAC,SAAS,OAAO;QAEnC,MAAM,UAAU,QAAQ,OAAO,CAAC,sBAAsB,IAAI,IAAI;QAC9D,MAAM,aAAa,KAAK,KAAK,CAAC;QAE9B,OAAO;YACL,YAAY,KAAK,GAAG,CAAC,GAAG,KAAK,GAAG,CAAC,GAAG,WAAW,UAAU,IAAI;YAC7D,cAAc,WAAW,YAAY,IAAK,WAAW,UAAU,IAAI;YACnE,iBAAiB,WAAW,eAAe,IAAI,EAAE;YACjD,oBAAoB,WAAW,kBAAkB,IAAI,EAAE;YACvD,kBAAkB,WAAW,gBAAgB,IAAI,EAAE;YACnD,WAAW,WAAW,SAAS,IAAI;QACrC;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,IAAI,CAAC,sBAAsB;QACnC,mDAAmD;QACnD,OAAO;YACL,YAAY;YACZ,cAAc;YACd,iBAAiB,EAAE;YACnB,oBAAoB,EAAE;YACtB,kBAAkB,EAAE;YACpB,WAAW;QACb;IACF;AACF;AAEA;;CAEC,GACD,SAAS,YACP,cAAgC,EAChC,SAA2B;IAE3B,MAAM,OAAO,IAAI;IACjB,MAAM,SAA2B,EAAE;IAEnC,iDAAiD;IACjD,KAAK,MAAM,SAAS,eAAgB;QAClC,MAAM,MAAM,GAAG,MAAM,KAAK,CAAC,CAAC,EAAE,MAAM,UAAU,CAAC,CAAC,EAAE,MAAM,UAAU,EAAE;QACpE,IAAI,CAAC,KAAK,GAAG,CAAC,MAAM;YAClB,KAAK,GAAG,CAAC;YACT,OAAO,IAAI,CAAC;QACd;IACF;IAEA,wCAAwC;IACxC,KAAK,MAAM,SAAS,UAAW;QAC7B,MAAM,MAAM,GAAG,MAAM,KAAK,CAAC,CAAC,EAAE,MAAM,UAAU,CAAC,CAAC,EAAE,MAAM,UAAU,EAAE;QACpE,IAAI,CAAC,KAAK,GAAG,CAAC,MAAM;YAClB,KAAK,GAAG,CAAC;YACT,OAAO,IAAI,CAAC;QACd;IACF;IAEA,0BAA0B;IAC1B,OAAO,OACJ,IAAI,CAAC,CAAC,GAAG,IAAM,EAAE,KAAK,GAAG,EAAE,KAAK,EAChC,KAAK,CAAC,GAAG,mBAAmB,IAAI,6BAA6B;AAClE;AAQO,eAAe,gBACpB,KAAa,EACb,UAKI,CAAC,CAAC;IAEN,MAAM,EACJ,KAAK,EACL,gBAAgB,cAAc,EAC9B,sBAAsB,oBAAoB,EAC1C,UAAU,IAAI,EACf,GAAG;IAEJ,MAAM,cAAkC,EAAE;IAC1C,MAAM,mBAA6B,EAAE;IACrC,IAAI,YAA8B,EAAE;IACpC,IAAI,YAAY;IAChB,IAAI,kBAAkB;IAEtB,4BAA4B;IAC5B,IAAI,SAAS,QAAQ,GAAG,CAAC,CAAC,0BAA0B,EAAE,MAAM,CAAC,CAAC;IAC9D,MAAM,WAAW,MAAM,aAAa;IACpC,IAAI,SAAS;QACX,QAAQ,GAAG,CAAC,CAAC,gBAAgB,EAAE,SAAS,MAAM,EAAE;QAChD,QAAQ,GAAG,CAAC,CAAC,wBAAwB,EAAE,SAAS,aAAa,CAAC,IAAI,CAAC,OAAO;IAC5E;IAEA,oDAAoD;IACpD,KAAK,MAAM,eAAe,SAAS,aAAa,CAAE;QAChD,iBAAiB,IAAI,CAAC;QACtB,IAAI,SAAS,QAAQ,GAAG,CAAC,CAAC,oBAAoB,EAAE,YAAY,CAAC,CAAC;QAE9D,MAAM,SAAS,MAAM,IAAA,4IAAe,EAAC,aAAa;YAChD,MAAM;YACN;YACA,WAAW;QACb;QAEA,YAAY,YAAY,WAAW;IACrC;IAEA,IAAI,SAAS,QAAQ,GAAG,CAAC,CAAC,kBAAkB,EAAE,UAAU,MAAM,CAAC,OAAO,CAAC;IAEvE,+BAA+B;IAC/B,MAAO,YAAY,cAAe;QAChC;QACA,IAAI,SAAS,QAAQ,GAAG,CAAC,CAAC,6BAA6B,EAAE,UAAU,GAAG,CAAC;QAEvE,6BAA6B;QAC7B,MAAM,aAAa,MAAM,iBACvB,OACA,UACA,WACA;QAEF,YAAY,IAAI,CAAC;QACjB,kBAAkB,WAAW,UAAU;QAEvC,IAAI,SAAS;YACX,QAAQ,GAAG,CAAC,CAAC,oBAAoB,EAAE,WAAW,UAAU,CAAC,OAAO,CAAC,IAAI;YACrE,QAAQ,GAAG,CAAC,CAAC,oBAAoB,EAAE,WAAW,YAAY,EAAE;YAC5D,IAAI,WAAW,kBAAkB,CAAC,MAAM,GAAG,GAAG;gBAC5C,QAAQ,GAAG,CAAC,CAAC,iBAAiB,EAAE,WAAW,kBAAkB,CAAC,IAAI,CAAC,OAAO;YAC5E;QACF;QAEA,0BAA0B;QAC1B,IAAI,WAAW,YAAY,IAAI,WAAW,UAAU,IAAI,qBAAqB;YAC3E,IAAI,SAAS,QAAQ,GAAG,CAAC,CAAC,qCAAqC,CAAC;YAChE;QACF;QAEA,sCAAsC;QACtC,IAAI,WAAW,gBAAgB,CAAC,MAAM,KAAK,GAAG;YAC5C,IAAI,SAAS,QAAQ,GAAG,CAAC,CAAC,2CAA2C,CAAC;YACtE;QACF;QAEA,2CAA2C;QAC3C,KAAK,MAAM,YAAY,WAAW,gBAAgB,CAAE;YAClD,iCAAiC;YACjC,IAAI,iBAAiB,QAAQ,CAAC,WAAW;YAEzC,iBAAiB,IAAI,CAAC;YACtB,IAAI,SAAS,QAAQ,GAAG,CAAC,CAAC,uBAAuB,EAAE,SAAS,CAAC,CAAC;YAE9D,MAAM,YAAY,MAAM,IAAA,4IAAe,EAAC,UAAU;gBAChD,MAAM;gBACN;gBACA,WAAW;YACb;YAEA,YAAY,YAAY,WAAW;QACrC;QAEA,IAAI,SAAS,QAAQ,GAAG,CAAC,CAAC,sBAAsB,EAAE,UAAU,MAAM,EAAE;IACtE;IAEA,IAAI,SAAS;QACX,QAAQ,GAAG,CAAC,CAAC,qBAAqB,EAAE,UAAU,aAAa,CAAC;QAC5D,QAAQ,GAAG,CAAC,CAAC,0BAA0B,EAAE,gBAAgB,OAAO,CAAC,IAAI;QACrE,QAAQ,GAAG,CAAC,CAAC,8BAA8B,EAAE,iBAAiB,MAAM,EAAE;IACxE;IAEA,OAAO;QACL,QAAQ;QACR,YAAY;QACZ,eAAe;QACf;QACA;QACA,eAAe;IACjB;AACF;AAKO,SAAS,yBAAyB,MAA4B;IACnE,IAAI,OAAO,MAAM,CAAC,MAAM,KAAK,GAAG;QAC9B,OAAO;IACT;IAEA,MAAM,gBAAgB,OAAO,MAAM,CAAC,GAAG,CAAC,CAAC,OAAO;QAC9C,OAAO,CAAC,CAAC,EAAE,QAAQ,EAAE,aAAa,EAAE,MAAM,OAAO,CAAC,QAAQ,EAAE,MAAM,UAAU,CAAC;AACjF,EAAE,MAAM,IAAI,EAAE;IACZ;IAEA,MAAM,aAAa,CAAC,sBAAsB,EAAE,OAAO,UAAU,CAAC,eAAe,EAAE,OAAO,aAAa,CAAC,MAAM,CAAC,qBAAqB,EAAE,OAAO,eAAe,CAAC,OAAO,CAAC,IAAI;IAErK,OAAO,GAAG,WAAW,WAAW,EAAE,cAAc,IAAI,CAAC,gBAAgB;AACvE;AAKO,SAAS,iBAAiB,MAA4B;IAM3D,OAAO;QACL,YAAY,OAAO,UAAU;QAC7B,YAAY,OAAO,eAAe;QAClC,eAAe,OAAO,aAAa;QACnC,QAAQ,OAAO,aAAa,CAAC,MAAM;IACrC;AACF"}},
    {"offset": {"line": 1140, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/app/api/copilotkit/route.ts"],"sourcesContent":["import {\n  CopilotRuntime,\n  LangChainAdapter,\n  copilotRuntimeNextJSAppRouterEndpoint,\n} from \"@copilotkit/runtime\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage, SystemMessage, AIMessage } from \"@langchain/core/messages\";\nimport { NextRequest } from \"next/server\";\nimport { extractCitations } from \"@/lib/retrieval\";\nimport {\n  agenticRetrieve,\n  formatAgentContextForLLM,\n  getAgentMetadata,\n  type AgentRetrievalResult,\n} from \"@/lib/agentic-retrieval\";\n\n// Create the LangChain model\nconst model = new ChatOpenAI({\n  modelName: \"gpt-4.1\",\n  temperature: 0.3, // Lower temperature for more factual responses\n});\n\n// RAG System Prompt\nconst RAG_SYSTEM_PROMPT = `You are an internal knowledge assistant. Answer questions ONLY using the provided context from our knowledge base.\n\nSEARCH INFO: {searchInfo}\n\nRULES:\n1. Only answer from the context provided below - do not use external knowledge\n2. Cite your sources using this format: [Doc: {document name}, Page: {page number}]\n3. Include citations inline with your response for every piece of information\n4. If the answer is NOT in the context or confidence is low, say: \"I don't have sufficient information about that in my knowledge base.\"\n5. Be concise, accurate, and helpful\n6. If multiple documents discuss the same topic, synthesize the information and cite all relevant sources\n7. Do not make up or infer information not explicitly stated in the context\n8. Consider the search confidence when formulating your response\n\nCONTEXT FROM KNOWLEDGE BASE:\n{context}\n\nRemember: Only use information from the context above. Always cite your sources.`;\n\n// Store citations and agent metadata for the current response (used by the UI)\nlet lastCitations: Array<{\n  docName: string;\n  pageNumber: number;\n  imageUrl?: string;\n  snippet: string;\n}> = [];\n\nlet lastAgentMetadata: {\n  iterations: number;\n  confidence: number;\n  searchQueries: string[];\n  intent: string;\n} | null = null;\n\n// Create the service adapter using LangChain with RAG\nconst serviceAdapter = new LangChainAdapter({\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  chainFn: async ({ messages, tools }): Promise<any> => {\n    // Extract the last user message for retrieval\n    const lastMessage = messages[messages.length - 1];\n    const userQuery =\n      typeof lastMessage.content === \"string\"\n        ? lastMessage.content\n        : Array.isArray(lastMessage.content)\n          ? lastMessage.content\n              .filter((c): c is { type: \"text\"; text: string } => c.type === \"text\")\n              .map((c) => c.text)\n              .join(\" \")\n          : \"\";\n\n    // Retrieve relevant context using agentic retrieval\n    let agentResult: AgentRetrievalResult | null = null;\n    let contextText = \"\";\n    let searchInfo = \"No search performed\";\n\n    if (userQuery.trim()) {\n      try {\n        console.log(`[RAG] Starting agentic retrieval for: \"${userQuery}\"`);\n\n        // Use agentic retrieval with self-reflection\n        agentResult = await agenticRetrieve(userQuery, {\n          maxIterations: 3,\n          confidenceThreshold: 0.7,\n          verbose: true,\n        });\n\n        contextText = formatAgentContextForLLM(agentResult);\n        lastAgentMetadata = getAgentMetadata(agentResult);\n\n        // Create search info summary\n        searchInfo = `Searched with ${agentResult.iterations} iteration(s), ${agentResult.searchQueries.length} queries. Confidence: ${(agentResult.finalConfidence * 100).toFixed(0)}%. Intent: ${agentResult.queryAnalysis.intent}`;\n\n        // Extract citations for the UI\n        lastCitations = await extractCitations(agentResult.chunks);\n\n        console.log(`[RAG] Agentic retrieval complete. Confidence: ${agentResult.finalConfidence.toFixed(2)}, Chunks: ${agentResult.chunks.length}`);\n      } catch (error) {\n        console.error(\"Error in agentic retrieval:\", error);\n        contextText =\n          \"Error retrieving from knowledge base. Please try again.\";\n        searchInfo = \"Search failed\";\n        lastCitations = [];\n        lastAgentMetadata = null;\n      }\n    }\n\n    // Build the system prompt with context and search info\n    const systemPrompt = RAG_SYSTEM_PROMPT\n      .replace(\"{context}\", contextText)\n      .replace(\"{searchInfo}\", searchInfo);\n\n    // Prepend system message and transform messages\n    const langChainMessages = [\n      new SystemMessage(systemPrompt),\n      ...messages.slice(0, -1).map((msg) => {\n        const content =\n          typeof msg.content === \"string\"\n            ? msg.content\n            : Array.isArray(msg.content)\n              ? msg.content\n                  .filter((c): c is { type: \"text\"; text: string } => c.type === \"text\")\n                  .map((c) => c.text)\n                  .join(\" \")\n              : \"\";\n\n        // Check message type using getType() method or _getType()\n        const msgType = String(msg.getType?.() || (msg as unknown as { _getType?: () => string })._getType?.() || \"human\");\n        if (msgType === \"human\") {\n          return new HumanMessage(content);\n        } else if (msgType === \"ai\") {\n          return new AIMessage(content);\n        }\n        return new HumanMessage(content);\n      }),\n      new HumanMessage(userQuery),\n    ];\n\n    // Bind tools if available\n    const modelWithTools = tools?.length ? model.bindTools(tools) : model;\n    return modelWithTools.stream(langChainMessages);\n  },\n});\n\n// Create the CopilotKit runtime\nconst runtime = new CopilotRuntime();\n\nexport const POST = async (req: NextRequest) => {\n  const { handleRequest } = copilotRuntimeNextJSAppRouterEndpoint({\n    runtime,\n    serviceAdapter,\n    endpoint: \"/api/copilotkit\",\n  });\n\n  return handleRequest(req);\n};\n\n// Export citations and agent metadata endpoint for UI to fetch\nexport const GET = async () => {\n  return Response.json({\n    citations: lastCitations,\n    agentMetadata: lastAgentMetadata,\n  });\n};\n"],"names":[],"mappings":";;;;;;AAAA;AAKA;AAAA;AACA;AAAA;AAAA;AAAA;AAEA;AACA;;;;;;;;;;;AAOA,6BAA6B;AAC7B,MAAM,QAAQ,IAAI,qLAAU,CAAC;IAC3B,WAAW;IACX,aAAa;AACf;AAEA,oBAAoB;AACpB,MAAM,oBAAoB,CAAC;;;;;;;;;;;;;;;;;gFAiBqD,CAAC;AAEjF,+EAA+E;AAC/E,IAAI,gBAKC,EAAE;AAEP,IAAI,oBAKO;AAEX,sDAAsD;AACtD,MAAM,iBAAiB,IAAI,8KAAgB,CAAC;IAC1C,8DAA8D;IAC9D,SAAS,OAAO,EAAE,QAAQ,EAAE,KAAK,EAAE;QACjC,8CAA8C;QAC9C,MAAM,cAAc,QAAQ,CAAC,SAAS,MAAM,GAAG,EAAE;QACjD,MAAM,YACJ,OAAO,YAAY,OAAO,KAAK,WAC3B,YAAY,OAAO,GACnB,MAAM,OAAO,CAAC,YAAY,OAAO,IAC/B,YAAY,OAAO,CAChB,MAAM,CAAC,CAAC,IAA2C,EAAE,IAAI,KAAK,QAC9D,GAAG,CAAC,CAAC,IAAM,EAAE,IAAI,EACjB,IAAI,CAAC,OACR;QAER,oDAAoD;QACpD,IAAI,cAA2C;QAC/C,IAAI,cAAc;QAClB,IAAI,aAAa;QAEjB,IAAI,UAAU,IAAI,IAAI;YACpB,IAAI;gBACF,QAAQ,GAAG,CAAC,CAAC,uCAAuC,EAAE,UAAU,CAAC,CAAC;gBAElE,6CAA6C;gBAC7C,cAAc,MAAM,IAAA,uJAAe,EAAC,WAAW;oBAC7C,eAAe;oBACf,qBAAqB;oBACrB,SAAS;gBACX;gBAEA,cAAc,IAAA,gKAAwB,EAAC;gBACvC,oBAAoB,IAAA,wJAAgB,EAAC;gBAErC,6BAA6B;gBAC7B,aAAa,CAAC,cAAc,EAAE,YAAY,UAAU,CAAC,eAAe,EAAE,YAAY,aAAa,CAAC,MAAM,CAAC,sBAAsB,EAAE,CAAC,YAAY,eAAe,GAAG,GAAG,EAAE,OAAO,CAAC,GAAG,WAAW,EAAE,YAAY,aAAa,CAAC,MAAM,EAAE;gBAE7N,+BAA+B;gBAC/B,gBAAgB,MAAM,IAAA,6IAAgB,EAAC,YAAY,MAAM;gBAEzD,QAAQ,GAAG,CAAC,CAAC,8CAA8C,EAAE,YAAY,eAAe,CAAC,OAAO,CAAC,GAAG,UAAU,EAAE,YAAY,MAAM,CAAC,MAAM,EAAE;YAC7I,EAAE,OAAO,OAAO;gBACd,QAAQ,KAAK,CAAC,+BAA+B;gBAC7C,cACE;gBACF,aAAa;gBACb,gBAAgB,EAAE;gBAClB,oBAAoB;YACtB;QACF;QAEA,uDAAuD;QACvD,MAAM,eAAe,kBAClB,OAAO,CAAC,aAAa,aACrB,OAAO,CAAC,gBAAgB;QAE3B,gDAAgD;QAChD,MAAM,oBAAoB;YACxB,IAAI,oLAAa,CAAC;eACf,SAAS,KAAK,CAAC,GAAG,CAAC,GAAG,GAAG,CAAC,CAAC;gBAC5B,MAAM,UACJ,OAAO,IAAI,OAAO,KAAK,WACnB,IAAI,OAAO,GACX,MAAM,OAAO,CAAC,IAAI,OAAO,IACvB,IAAI,OAAO,CACR,MAAM,CAAC,CAAC,IAA2C,EAAE,IAAI,KAAK,QAC9D,GAAG,CAAC,CAAC,IAAM,EAAE,IAAI,EACjB,IAAI,CAAC,OACR;gBAER,0DAA0D;gBAC1D,MAAM,UAAU,OAAO,IAAI,OAAO,QAAQ,AAAC,IAA+C,QAAQ,QAAQ;gBAC1G,IAAI,YAAY,SAAS;oBACvB,OAAO,IAAI,kLAAY,CAAC;gBAC1B,OAAO,IAAI,YAAY,MAAM;oBAC3B,OAAO,IAAI,4KAAS,CAAC;gBACvB;gBACA,OAAO,IAAI,kLAAY,CAAC;YAC1B;YACA,IAAI,kLAAY,CAAC;SAClB;QAED,0BAA0B;QAC1B,MAAM,iBAAiB,OAAO,SAAS,MAAM,SAAS,CAAC,SAAS;QAChE,OAAO,eAAe,MAAM,CAAC;IAC/B;AACF;AAEA,gCAAgC;AAChC,MAAM,UAAU,IAAI,4KAAc;AAE3B,MAAM,OAAO,OAAO;IACzB,MAAM,EAAE,aAAa,EAAE,GAAG,IAAA,mMAAqC,EAAC;QAC9D;QACA;QACA,UAAU;IACZ;IAEA,OAAO,cAAc;AACvB;AAGO,MAAM,MAAM;IACjB,OAAO,SAAS,IAAI,CAAC;QACnB,WAAW;QACX,eAAe;IACjB;AACF"}}]
}