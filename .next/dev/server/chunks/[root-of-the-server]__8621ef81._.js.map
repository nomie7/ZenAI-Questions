{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 52, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/parsers/index.ts"],"sourcesContent":["/**\n * Document parser interface and factory\n */\n\nexport interface ParsedPage {\n  pageNumber: number;\n  text: string;\n  imageBuffer?: Buffer;\n}\n\nexport interface ParsedDocument {\n  pages: ParsedPage[];\n  metadata: {\n    title?: string;\n    author?: string;\n    pageCount: number;\n    parserUsed: ParserType;\n  };\n}\n\nexport type ParserType = \"basic\" | \"gemini\" | \"docling\";\n\nexport interface DocumentParser {\n  parse(file: Buffer, filename: string): Promise<ParsedDocument>;\n}\n\n// Lazy load parsers to avoid importing heavy dependencies\nexport async function getParser(type: ParserType): Promise<DocumentParser> {\n  switch (type) {\n    case \"docling\": {\n      const { DoclingParser } = await import(\"./docling-parser\");\n      return new DoclingParser();\n    }\n    case \"gemini\":\n      const { GeminiParser } = await import(\"./gemini-parser\");\n      return new GeminiParser();\n    case \"basic\":\n    default:\n      const { BasicParser } = await import(\"./basic-parser\");\n      return new BasicParser();\n  }\n}\n\n/**\n * Detect the best parser for a given file\n */\nexport function suggestParser(filename: string): ParserType {\n  const ext = filename.toLowerCase().split(\".\").pop();\n\n  // For scanned PDFs or images, suggest Gemini\n  // For text-based PDFs, suggest basic parser\n  // This is a simple heuristic - in practice you might want to analyze the PDF first\n\n  if (ext === \"pdf\") {\n    // Default to basic, user can override to gemini for scanned docs\n    return \"basic\";\n  }\n\n  return \"basic\";\n}\n"],"names":[],"mappings":"AAAA;;CAEC;;;;;;AAyBM,eAAe,UAAU,IAAgB;IAC9C,OAAQ;QACN,KAAK;YAAW;gBACd,MAAM,EAAE,aAAa,EAAE,GAAG;gBAC1B,OAAO,IAAI;YACb;QACA,KAAK;YACH,MAAM,EAAE,YAAY,EAAE,GAAG;YACzB,OAAO,IAAI;QACb,KAAK;QACL;YACE,MAAM,EAAE,WAAW,EAAE,GAAG;YACxB,OAAO,IAAI;IACf;AACF;AAKO,SAAS,cAAc,QAAgB;IAC5C,MAAM,MAAM,SAAS,WAAW,GAAG,KAAK,CAAC,KAAK,GAAG;IAEjD,6CAA6C;IAC7C,4CAA4C;IAC5C,mFAAmF;IAEnF,IAAI,QAAQ,OAAO;QACjB,iEAAiE;QACjE,OAAO;IACT;IAEA,OAAO;AACT"}},
    {"offset": {"line": 103, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/storage.ts"],"sourcesContent":["import * as Minio from \"minio\";\n\n// MinIO configuration\nconst MINIO_ENDPOINT = process.env.MINIO_ENDPOINT || \"localhost\";\nconst MINIO_PORT = parseInt(process.env.MINIO_PORT || \"9000\", 10);\n// Support both typical MinIO env names and username/password aliases\nconst MINIO_ACCESS_KEY =\n  process.env.MINIO_ACCESS_KEY ||\n  process.env.MINIO_USERNAME || // alias for username\n  \"\";\nconst MINIO_SECRET_KEY =\n  process.env.MINIO_SECRET_KEY ||\n  process.env.MINIO_PASSWORD || // alias for password\n  \"\";\nconst MINIO_BUCKET = process.env.MINIO_BUCKET || \"knowledge-docs\";\nconst MINIO_USE_SSL = process.env.MINIO_USE_SSL === \"true\";\n\nlet client: Minio.Client | null = null;\n\n/**\n * Get or create the MinIO client singleton\n */\nexport function getStorageClient(): Minio.Client {\n  if (!client) {\n    const { endPoint, port, useSSL } = resolveMinioEndpoint();\n\n    client = new Minio.Client({\n      endPoint,\n      port,\n      useSSL,\n      accessKey: MINIO_ACCESS_KEY,\n      secretKey: MINIO_SECRET_KEY,\n    });\n  }\n  return client;\n}\n\nfunction resolveMinioEndpoint(): {\n  endPoint: string;\n  port: number;\n  useSSL: boolean;\n} {\n  // If MINIO_ENDPOINT includes scheme, parse it; otherwise use raw values.\n  if (\n    MINIO_ENDPOINT.startsWith(\"http://\") ||\n    MINIO_ENDPOINT.startsWith(\"https://\")\n  ) {\n    try {\n      const url = new URL(MINIO_ENDPOINT);\n      const endPoint = url.hostname;\n      const useSSL = url.protocol === \"https:\";\n      const port =\n        url.port && Number(url.port) > 0\n          ? Number(url.port)\n          : useSSL\n            ? 443\n            : 80;\n      return { endPoint, port, useSSL };\n    } catch {\n      // Fall through to defaults\n    }\n  }\n\n  return {\n    endPoint: MINIO_ENDPOINT,\n    port: MINIO_PORT,\n    useSSL: MINIO_USE_SSL,\n  };\n}\n\n/**\n * Get the bucket name\n */\nexport function getBucketName(): string {\n  return MINIO_BUCKET;\n}\n\n/**\n * Ensure the bucket exists\n */\nexport async function ensureBucket(): Promise<void> {\n  const minio = getStorageClient();\n\n  const exists = await minio.bucketExists(MINIO_BUCKET);\n  if (!exists) {\n    await minio.makeBucket(MINIO_BUCKET);\n    console.log(`Created MinIO bucket: ${MINIO_BUCKET}`);\n  }\n}\n\n/**\n * Upload a file to storage\n */\nexport async function uploadFile(\n  objectName: string,\n  buffer: Buffer,\n  contentType: string = \"application/octet-stream\"\n): Promise<string> {\n  const minio = getStorageClient();\n\n  await minio.putObject(MINIO_BUCKET, objectName, buffer, buffer.length, {\n    \"Content-Type\": contentType,\n  });\n\n  return objectName;\n}\n\n/**\n * Upload original PDF document\n * Path: documents/{doc_id}/original.pdf\n */\nexport async function uploadDocument(\n  docId: string,\n  buffer: Buffer,\n  originalFilename: string\n): Promise<string> {\n  const ext = originalFilename.split(\".\").pop() || \"pdf\";\n  const objectName = `documents/${docId}/original.${ext}`;\n\n  await uploadFile(objectName, buffer, getMimeType(ext));\n  return objectName;\n}\n\n/**\n * Upload page image\n * Path: documents/{doc_id}/pages/page-{n}.png\n */\nexport async function uploadPageImage(\n  docId: string,\n  pageNumber: number,\n  buffer: Buffer\n): Promise<string> {\n  const objectName = `documents/${docId}/pages/page-${pageNumber}.png`;\n\n  await uploadFile(objectName, buffer, \"image/png\");\n  return objectName;\n}\n\n/**\n * Get a presigned URL for accessing a file\n * Default expiry: 1 hour (3600 seconds)\n */\nexport async function getSignedUrl(\n  objectName: string,\n  expirySeconds: number = 3600\n): Promise<string> {\n  const minio = getStorageClient();\n\n  const url = await minio.presignedGetObject(\n    MINIO_BUCKET,\n    objectName,\n    expirySeconds\n  );\n\n  return url;\n}\n\n/**\n * Delete a file from storage\n */\nexport async function deleteFile(objectName: string): Promise<void> {\n  const minio = getStorageClient();\n  await minio.removeObject(MINIO_BUCKET, objectName);\n}\n\n/**\n * Delete all files for a document\n */\nexport async function deleteDocumentFiles(docId: string): Promise<void> {\n  const minio = getStorageClient();\n  const prefix = `documents/${docId}/`;\n\n  const objectsList: string[] = [];\n  const stream = minio.listObjects(MINIO_BUCKET, prefix, true);\n\n  for await (const obj of stream) {\n    if (obj.name) {\n      objectsList.push(obj.name);\n    }\n  }\n\n  if (objectsList.length > 0) {\n    await minio.removeObjects(MINIO_BUCKET, objectsList);\n  }\n}\n\n/**\n * Check if a file exists\n */\nexport async function fileExists(objectName: string): Promise<boolean> {\n  const minio = getStorageClient();\n\n  try {\n    await minio.statObject(MINIO_BUCKET, objectName);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get file as buffer\n */\nexport async function getFile(objectName: string): Promise<Buffer> {\n  const minio = getStorageClient();\n\n  const stream = await minio.getObject(MINIO_BUCKET, objectName);\n  const chunks: Buffer[] = [];\n\n  for await (const chunk of stream) {\n    chunks.push(Buffer.from(chunk));\n  }\n\n  return Buffer.concat(chunks);\n}\n\n/**\n * Get MIME type from file extension\n */\nfunction getMimeType(ext: string): string {\n  const mimeTypes: Record<string, string> = {\n    pdf: \"application/pdf\",\n    png: \"image/png\",\n    jpg: \"image/jpeg\",\n    jpeg: \"image/jpeg\",\n    pptx: \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n    docx: \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n    xlsx: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n    txt: \"text/plain\",\n    json: \"application/json\",\n  };\n\n  return mimeTypes[ext.toLowerCase()] || \"application/octet-stream\";\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;AAAA;;;;;;AAEA,sBAAsB;AACtB,MAAM,iBAAiB,QAAQ,GAAG,CAAC,cAAc,IAAI;AACrD,MAAM,aAAa,SAAS,QAAQ,GAAG,CAAC,UAAU,IAAI,QAAQ;AAC9D,qEAAqE;AACrE,MAAM,mBACJ,QAAQ,GAAG,CAAC,gBAAgB,IAC5B,QAAQ,GAAG,CAAC,cAAc,IAAI,qBAAqB;AACnD;AACF,MAAM,mBACJ,QAAQ,GAAG,CAAC,gBAAgB,IAC5B,QAAQ,GAAG,CAAC,cAAc,IAAI,qBAAqB;AACnD;AACF,MAAM,eAAe,QAAQ,GAAG,CAAC,YAAY,IAAI;AACjD,MAAM,gBAAgB,QAAQ,GAAG,CAAC,aAAa,KAAK;AAEpD,IAAI,SAA8B;AAK3B,SAAS;IACd,IAAI,CAAC,QAAQ;QACX,MAAM,EAAE,QAAQ,EAAE,IAAI,EAAE,MAAM,EAAE,GAAG;QAEnC,SAAS,IAAI,oHAAY,CAAC;YACxB;YACA;YACA;YACA,WAAW;YACX,WAAW;QACb;IACF;IACA,OAAO;AACT;AAEA,SAAS;IAKP,yEAAyE;IACzE,IACE,eAAe,UAAU,CAAC,cAC1B,eAAe,UAAU,CAAC,aAC1B;QACA,IAAI;YACF,MAAM,MAAM,IAAI,IAAI;YACpB,MAAM,WAAW,IAAI,QAAQ;YAC7B,MAAM,SAAS,IAAI,QAAQ,KAAK;YAChC,MAAM,OACJ,IAAI,IAAI,IAAI,OAAO,IAAI,IAAI,IAAI,IAC3B,OAAO,IAAI,IAAI,IACf,SACE,MACA;YACR,OAAO;gBAAE;gBAAU;gBAAM;YAAO;QAClC,EAAE,OAAM;QACN,2BAA2B;QAC7B;IACF;IAEA,OAAO;QACL,UAAU;QACV,MAAM;QACN,QAAQ;IACV;AACF;AAKO,SAAS;IACd,OAAO;AACT;AAKO,eAAe;IACpB,MAAM,QAAQ;IAEd,MAAM,SAAS,MAAM,MAAM,YAAY,CAAC;IACxC,IAAI,CAAC,QAAQ;QACX,MAAM,MAAM,UAAU,CAAC;QACvB,QAAQ,GAAG,CAAC,CAAC,sBAAsB,EAAE,cAAc;IACrD;AACF;AAKO,eAAe,WACpB,UAAkB,EAClB,MAAc,EACd,cAAsB,0BAA0B;IAEhD,MAAM,QAAQ;IAEd,MAAM,MAAM,SAAS,CAAC,cAAc,YAAY,QAAQ,OAAO,MAAM,EAAE;QACrE,gBAAgB;IAClB;IAEA,OAAO;AACT;AAMO,eAAe,eACpB,KAAa,EACb,MAAc,EACd,gBAAwB;IAExB,MAAM,MAAM,iBAAiB,KAAK,CAAC,KAAK,GAAG,MAAM;IACjD,MAAM,aAAa,CAAC,UAAU,EAAE,MAAM,UAAU,EAAE,KAAK;IAEvD,MAAM,WAAW,YAAY,QAAQ,YAAY;IACjD,OAAO;AACT;AAMO,eAAe,gBACpB,KAAa,EACb,UAAkB,EAClB,MAAc;IAEd,MAAM,aAAa,CAAC,UAAU,EAAE,MAAM,YAAY,EAAE,WAAW,IAAI,CAAC;IAEpE,MAAM,WAAW,YAAY,QAAQ;IACrC,OAAO;AACT;AAMO,eAAe,aACpB,UAAkB,EAClB,gBAAwB,IAAI;IAE5B,MAAM,QAAQ;IAEd,MAAM,MAAM,MAAM,MAAM,kBAAkB,CACxC,cACA,YACA;IAGF,OAAO;AACT;AAKO,eAAe,WAAW,UAAkB;IACjD,MAAM,QAAQ;IACd,MAAM,MAAM,YAAY,CAAC,cAAc;AACzC;AAKO,eAAe,oBAAoB,KAAa;IACrD,MAAM,QAAQ;IACd,MAAM,SAAS,CAAC,UAAU,EAAE,MAAM,CAAC,CAAC;IAEpC,MAAM,cAAwB,EAAE;IAChC,MAAM,SAAS,MAAM,WAAW,CAAC,cAAc,QAAQ;IAEvD,WAAW,MAAM,OAAO,OAAQ;QAC9B,IAAI,IAAI,IAAI,EAAE;YACZ,YAAY,IAAI,CAAC,IAAI,IAAI;QAC3B;IACF;IAEA,IAAI,YAAY,MAAM,GAAG,GAAG;QAC1B,MAAM,MAAM,aAAa,CAAC,cAAc;IAC1C;AACF;AAKO,eAAe,WAAW,UAAkB;IACjD,MAAM,QAAQ;IAEd,IAAI;QACF,MAAM,MAAM,UAAU,CAAC,cAAc;QACrC,OAAO;IACT,EAAE,OAAM;QACN,OAAO;IACT;AACF;AAKO,eAAe,QAAQ,UAAkB;IAC9C,MAAM,QAAQ;IAEd,MAAM,SAAS,MAAM,MAAM,SAAS,CAAC,cAAc;IACnD,MAAM,SAAmB,EAAE;IAE3B,WAAW,MAAM,SAAS,OAAQ;QAChC,OAAO,IAAI,CAAC,OAAO,IAAI,CAAC;IAC1B;IAEA,OAAO,OAAO,MAAM,CAAC;AACvB;AAEA;;CAEC,GACD,SAAS,YAAY,GAAW;IAC9B,MAAM,YAAoC;QACxC,KAAK;QACL,KAAK;QACL,KAAK;QACL,MAAM;QACN,MAAM;QACN,MAAM;QACN,MAAM;QACN,KAAK;QACL,MAAM;IACR;IAEA,OAAO,SAAS,CAAC,IAAI,WAAW,GAAG,IAAI;AACzC"}},
    {"offset": {"line": 273, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/document-processor.ts"],"sourcesContent":["import { v4 as uuidv4 } from \"uuid\";\nimport { getParser, type ParserType, type ParsedDocument } from \"./parsers\";\nimport { uploadDocument, uploadPageImage, getSignedUrl } from \"./storage\";\n\nexport interface ProcessedChunk {\n  chunkId: string;\n  text: string;\n  pageNumber: number;\n  chunkIndex: number;\n  startChar: number;\n  endChar: number;\n}\n\nexport interface ProcessedPage {\n  pageNumber: number;\n  text: string;\n  imageUrl: string; // MinIO object path\n  chunks: ProcessedChunk[];\n}\n\nexport interface ProcessedDocument {\n  docId: string;\n  docName: string;\n  originalPath: string;\n  pages: ProcessedPage[];\n  parserUsed: ParserType;\n  totalChunks: number;\n  processedAt: Date;\n}\n\n// Chunking configuration\nconst CHUNK_SIZE = 500; // characters\nconst CHUNK_OVERLAP = 50; // characters\n\n/**\n * Process a document file through the full pipeline:\n * 1. Parse the document (extract text and images)\n * 2. Upload original file to MinIO\n * 3. Upload page images to MinIO\n * 4. Chunk the text with overlap\n */\nexport async function processDocument(\n  file: Buffer,\n  filename: string,\n  docName: string,\n  parserType: ParserType = \"basic\",\n  existingDocId?: string\n): Promise<ProcessedDocument> {\n  // Generate or use existing doc ID\n  const docId = existingDocId || uuidv4();\n\n  console.log(\n    `Processing document: ${docName} (${filename}) with ${parserType} parser`\n  );\n\n  // 1. Parse the document\n  const parser = await getParser(parserType);\n  const parsed: ParsedDocument = await parser.parse(file, filename);\n\n  console.log(`Parsed ${parsed.metadata.pageCount} pages`);\n\n  // 2. Upload original file\n  const originalPath = await uploadDocument(docId, file, filename);\n\n  // 3. Process each page\n  const pages: ProcessedPage[] = [];\n  let totalChunks = 0;\n\n  for (const page of parsed.pages) {\n    // Upload page image if available\n    let imageUrl = \"\";\n    if (page.imageBuffer) {\n      imageUrl = await uploadPageImage(docId, page.pageNumber, page.imageBuffer);\n    }\n\n    // Chunk the page text\n    const chunks = chunkText(page.text, page.pageNumber);\n    totalChunks += chunks.length;\n\n    pages.push({\n      pageNumber: page.pageNumber,\n      text: page.text,\n      imageUrl,\n      chunks,\n    });\n  }\n\n  console.log(`Created ${totalChunks} chunks across ${pages.length} pages`);\n\n  return {\n    docId,\n    docName,\n    originalPath,\n    pages,\n    parserUsed: parserType,\n    totalChunks,\n    processedAt: new Date(),\n  };\n}\n\n/**\n * Chunk text with overlap for better context preservation\n */\nexport function chunkText(\n  text: string,\n  pageNumber: number,\n  chunkSize: number = CHUNK_SIZE,\n  overlap: number = CHUNK_OVERLAP\n): ProcessedChunk[] {\n  const chunks: ProcessedChunk[] = [];\n\n  // Skip if text is too short\n  if (text.length <= chunkSize) {\n    chunks.push({\n      chunkId: uuidv4(),\n      text: text.trim(),\n      pageNumber,\n      chunkIndex: 0,\n      startChar: 0,\n      endChar: text.length,\n    });\n    return chunks;\n  }\n\n  // Create chunks with overlap\n  let startIndex = 0;\n  let chunkIndex = 0;\n\n  while (startIndex < text.length) {\n    let endIndex = startIndex + chunkSize;\n\n    // Try to end at a sentence or word boundary\n    if (endIndex < text.length) {\n      // Look for sentence ending within the last 50 characters\n      const searchStart = Math.max(endIndex - 50, startIndex);\n      const searchText = text.slice(searchStart, endIndex);\n      const sentenceEnd = searchText.search(/[.!?]\\s/);\n\n      if (sentenceEnd !== -1) {\n        endIndex = searchStart + sentenceEnd + 1;\n      } else {\n        // Fall back to word boundary\n        const wordEnd = searchText.lastIndexOf(\" \");\n        if (wordEnd !== -1) {\n          endIndex = searchStart + wordEnd;\n        }\n      }\n    } else {\n      endIndex = text.length;\n    }\n\n    const chunkText = text.slice(startIndex, endIndex).trim();\n\n    if (chunkText.length > 0) {\n      chunks.push({\n        chunkId: uuidv4(),\n        text: chunkText,\n        pageNumber,\n        chunkIndex,\n        startChar: startIndex,\n        endChar: endIndex,\n      });\n      chunkIndex++;\n    }\n\n    // Move to next chunk with overlap\n    startIndex = Math.max(startIndex + 1, endIndex - overlap);\n\n    // Prevent infinite loop\n    if (startIndex >= text.length) break;\n  }\n\n  return chunks;\n}\n\n/**\n * Get a signed URL for a page image\n */\nexport async function getPageImageUrl(\n  imageUrl: string,\n  expirySeconds: number = 3600\n): Promise<string> {\n  if (!imageUrl) return \"\";\n  return getSignedUrl(imageUrl, expirySeconds);\n}\n\n/**\n * Calculate document statistics\n */\nexport function getDocumentStats(doc: ProcessedDocument): {\n  pageCount: number;\n  chunkCount: number;\n  avgChunkSize: number;\n  totalCharacters: number;\n} {\n  const totalCharacters = doc.pages.reduce((sum, p) => sum + p.text.length, 0);\n  const avgChunkSize =\n    doc.totalChunks > 0\n      ? Math.round(\n          doc.pages.reduce(\n            (sum, p) =>\n              sum + p.chunks.reduce((cs, c) => cs + c.text.length, 0),\n            0\n          ) / doc.totalChunks\n        )\n      : 0;\n\n  return {\n    pageCount: doc.pages.length,\n    chunkCount: doc.totalChunks,\n    avgChunkSize,\n    totalCharacters,\n  };\n}\n"],"names":[],"mappings":";;;;;;;;;;AAAA;AACA;AACA;;;;;;;;AA4BA,yBAAyB;AACzB,MAAM,aAAa,KAAK,aAAa;AACrC,MAAM,gBAAgB,IAAI,aAAa;AAShC,eAAe,gBACpB,IAAY,EACZ,QAAgB,EAChB,OAAe,EACf,aAAyB,OAAO,EAChC,aAAsB;IAEtB,kCAAkC;IAClC,MAAM,QAAQ,iBAAiB,IAAA,mLAAM;IAErC,QAAQ,GAAG,CACT,CAAC,qBAAqB,EAAE,QAAQ,EAAE,EAAE,SAAS,OAAO,EAAE,WAAW,OAAO,CAAC;IAG3E,wBAAwB;IACxB,MAAM,SAAS,MAAM,IAAA,6IAAS,EAAC;IAC/B,MAAM,SAAyB,MAAM,OAAO,KAAK,CAAC,MAAM;IAExD,QAAQ,GAAG,CAAC,CAAC,OAAO,EAAE,OAAO,QAAQ,CAAC,SAAS,CAAC,MAAM,CAAC;IAEvD,0BAA0B;IAC1B,MAAM,eAAe,MAAM,IAAA,yIAAc,EAAC,OAAO,MAAM;IAEvD,uBAAuB;IACvB,MAAM,QAAyB,EAAE;IACjC,IAAI,cAAc;IAElB,KAAK,MAAM,QAAQ,OAAO,KAAK,CAAE;QAC/B,iCAAiC;QACjC,IAAI,WAAW;QACf,IAAI,KAAK,WAAW,EAAE;YACpB,WAAW,MAAM,IAAA,0IAAe,EAAC,OAAO,KAAK,UAAU,EAAE,KAAK,WAAW;QAC3E;QAEA,sBAAsB;QACtB,MAAM,SAAS,UAAU,KAAK,IAAI,EAAE,KAAK,UAAU;QACnD,eAAe,OAAO,MAAM;QAE5B,MAAM,IAAI,CAAC;YACT,YAAY,KAAK,UAAU;YAC3B,MAAM,KAAK,IAAI;YACf;YACA;QACF;IACF;IAEA,QAAQ,GAAG,CAAC,CAAC,QAAQ,EAAE,YAAY,eAAe,EAAE,MAAM,MAAM,CAAC,MAAM,CAAC;IAExE,OAAO;QACL;QACA;QACA;QACA;QACA,YAAY;QACZ;QACA,aAAa,IAAI;IACnB;AACF;AAKO,SAAS,UACd,IAAY,EACZ,UAAkB,EAClB,YAAoB,UAAU,EAC9B,UAAkB,aAAa;IAE/B,MAAM,SAA2B,EAAE;IAEnC,4BAA4B;IAC5B,IAAI,KAAK,MAAM,IAAI,WAAW;QAC5B,OAAO,IAAI,CAAC;YACV,SAAS,IAAA,mLAAM;YACf,MAAM,KAAK,IAAI;YACf;YACA,YAAY;YACZ,WAAW;YACX,SAAS,KAAK,MAAM;QACtB;QACA,OAAO;IACT;IAEA,6BAA6B;IAC7B,IAAI,aAAa;IACjB,IAAI,aAAa;IAEjB,MAAO,aAAa,KAAK,MAAM,CAAE;QAC/B,IAAI,WAAW,aAAa;QAE5B,4CAA4C;QAC5C,IAAI,WAAW,KAAK,MAAM,EAAE;YAC1B,yDAAyD;YACzD,MAAM,cAAc,KAAK,GAAG,CAAC,WAAW,IAAI;YAC5C,MAAM,aAAa,KAAK,KAAK,CAAC,aAAa;YAC3C,MAAM,cAAc,WAAW,MAAM,CAAC;YAEtC,IAAI,gBAAgB,CAAC,GAAG;gBACtB,WAAW,cAAc,cAAc;YACzC,OAAO;gBACL,6BAA6B;gBAC7B,MAAM,UAAU,WAAW,WAAW,CAAC;gBACvC,IAAI,YAAY,CAAC,GAAG;oBAClB,WAAW,cAAc;gBAC3B;YACF;QACF,OAAO;YACL,WAAW,KAAK,MAAM;QACxB;QAEA,MAAM,YAAY,KAAK,KAAK,CAAC,YAAY,UAAU,IAAI;QAEvD,IAAI,UAAU,MAAM,GAAG,GAAG;YACxB,OAAO,IAAI,CAAC;gBACV,SAAS,IAAA,mLAAM;gBACf,MAAM;gBACN;gBACA;gBACA,WAAW;gBACX,SAAS;YACX;YACA;QACF;QAEA,kCAAkC;QAClC,aAAa,KAAK,GAAG,CAAC,aAAa,GAAG,WAAW;QAEjD,wBAAwB;QACxB,IAAI,cAAc,KAAK,MAAM,EAAE;IACjC;IAEA,OAAO;AACT;AAKO,eAAe,gBACpB,QAAgB,EAChB,gBAAwB,IAAI;IAE5B,IAAI,CAAC,UAAU,OAAO;IACtB,OAAO,IAAA,uIAAY,EAAC,UAAU;AAChC;AAKO,SAAS,iBAAiB,GAAsB;IAMrD,MAAM,kBAAkB,IAAI,KAAK,CAAC,MAAM,CAAC,CAAC,KAAK,IAAM,MAAM,EAAE,IAAI,CAAC,MAAM,EAAE;IAC1E,MAAM,eACJ,IAAI,WAAW,GAAG,IACd,KAAK,KAAK,CACR,IAAI,KAAK,CAAC,MAAM,CACd,CAAC,KAAK,IACJ,MAAM,EAAE,MAAM,CAAC,MAAM,CAAC,CAAC,IAAI,IAAM,KAAK,EAAE,IAAI,CAAC,MAAM,EAAE,IACvD,KACE,IAAI,WAAW,IAErB;IAEN,OAAO;QACL,WAAW,IAAI,KAAK,CAAC,MAAM;QAC3B,YAAY,IAAI,WAAW;QAC3B;QACA;IACF;AACF"}},
    {"offset": {"line": 411, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/embeddings.ts"],"sourcesContent":["import { OpenAIEmbeddings } from \"@langchain/openai\";\n\n// Use OpenAI's text-embedding-3-small model (1536 dimensions)\nconst embeddings = new OpenAIEmbeddings({\n  modelName: \"text-embedding-3-small\",\n  openAIApiKey: process.env.OPENAI_API_KEY,\n});\n\n/**\n * Generate embedding for a single text\n */\nexport async function embedText(text: string): Promise<number[]> {\n  const result = await embeddings.embedQuery(text);\n  return result;\n}\n\n/**\n * Generate embeddings for multiple texts in batch\n */\nexport async function embedTexts(texts: string[]): Promise<number[][]> {\n  const results = await embeddings.embedDocuments(texts);\n  return results;\n}\n\n/**\n * Get the embedding dimension (for Qdrant collection setup)\n */\nexport function getEmbeddingDimension(): number {\n  return 1536;\n}\n"],"names":[],"mappings":";;;;;;;;AAAA;AAAA;;AAEA,8DAA8D;AAC9D,MAAM,aAAa,IAAI,iLAAgB,CAAC;IACtC,WAAW;IACX,cAAc,QAAQ,GAAG,CAAC,cAAc;AAC1C;AAKO,eAAe,UAAU,IAAY;IAC1C,MAAM,SAAS,MAAM,WAAW,UAAU,CAAC;IAC3C,OAAO;AACT;AAKO,eAAe,WAAW,KAAe;IAC9C,MAAM,UAAU,MAAM,WAAW,cAAc,CAAC;IAChD,OAAO;AACT;AAKO,SAAS;IACd,OAAO;AACT"}},
    {"offset": {"line": 568, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/qdrant.ts"],"sourcesContent":["import { QdrantClient } from \"@qdrant/js-client-rest\";\nimport https from \"https\";\n\n// Qdrant configuration\nconst QDRANT_URL = process.env.QDRANT_URL || \"http://localhost:6333\";\nconst QDRANT_API_KEY = process.env.QDRANT_API_KEY;\nconst COLLECTION_NAME = process.env.QDRANT_COLLECTION_NAME || \"knowledge_base\";\nconst QDRANT_INSECURE_TLS = process.env.QDRANT_INSECURE_TLS === \"true\";\n\n// OpenAI embedding dimension (text-embedding-3-small)\nconst EMBEDDING_DIMENSION = 1536;\n\nlet client: QdrantClient | null = null;\n\n/**\n * Get or create the Qdrant client singleton\n */\nexport function getClient(): QdrantClient {\n  if (!client) {\n    client = new QdrantClient({\n      url: QDRANT_URL,\n      apiKey: QDRANT_API_KEY,\n      // Allow self-signed certs if explicitly requested\n      httpsAgent: QDRANT_INSECURE_TLS\n        ? new https.Agent({ rejectUnauthorized: false })\n        : undefined,\n    });\n  }\n  return client;\n}\n\n/**\n * Get the collection name\n */\nexport function getCollectionName(): string {\n  return COLLECTION_NAME;\n}\n\n/**\n * Ensure the knowledge base collection exists with proper configuration\n */\nexport async function ensureCollection(): Promise<void> {\n  const qdrant = getClient();\n\n  try {\n    // Check if collection exists\n    const collections = await qdrant.getCollections();\n    const exists = collections.collections.some(\n      (c) => c.name === COLLECTION_NAME\n    );\n\n    if (!exists) {\n      // Create collection with cosine similarity for OpenAI embeddings\n      await qdrant.createCollection(COLLECTION_NAME, {\n        vectors: {\n          size: EMBEDDING_DIMENSION,\n          distance: \"Cosine\",\n        },\n        // Enable payload indexing for common filter fields\n        optimizers_config: {\n          indexing_threshold: 0,\n        },\n      });\n\n      // Create payload indexes for efficient filtering\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"doc_id\",\n        field_schema: \"keyword\",\n      });\n\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"status\",\n        field_schema: \"keyword\",\n      });\n\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"doc_type\",\n        field_schema: \"keyword\",\n      });\n\n      console.log(`Created Qdrant collection: ${COLLECTION_NAME}`);\n    }\n  } catch (error) {\n    console.error(\"Error ensuring Qdrant collection:\", error);\n    throw error;\n  }\n}\n\n/**\n * Upsert vectors into the collection\n */\nexport async function upsertVectors(\n  points: Array<{\n    id: string;\n    vector: number[];\n    payload: Record<string, unknown>;\n  }>\n): Promise<void> {\n  const qdrant = getClient();\n\n  await qdrant.upsert(COLLECTION_NAME, {\n    points: points.map((p) => ({\n      id: p.id,\n      vector: p.vector,\n      payload: p.payload,\n    })),\n  });\n}\n\n/**\n * Search for similar vectors\n */\nexport async function searchVectors(\n  vector: number[],\n  limit: number = 10,\n  filter?: Record<string, unknown>\n): Promise<\n  Array<{\n    id: string;\n    score: number;\n    payload: Record<string, unknown>;\n  }>\n> {\n  const qdrant = getClient();\n\n  const results = await qdrant.search(COLLECTION_NAME, {\n    vector,\n    limit,\n    filter: filter as never,\n    with_payload: true,\n    score_threshold: 0.5, // Only return relevant results\n  });\n\n  return results.map((r) => ({\n    id: String(r.id),\n    score: r.score,\n    payload: (r.payload as Record<string, unknown>) || {},\n  }));\n}\n\n/**\n * Delete vectors by filter (e.g., by doc_id)\n */\nexport async function deleteVectorsByFilter(\n  filter: Record<string, unknown>\n): Promise<void> {\n  const qdrant = getClient();\n\n  await qdrant.delete(COLLECTION_NAME, {\n    filter: filter as never,\n  });\n}\n\n/**\n * Get collection info for debugging\n */\nexport async function getCollectionInfo(): Promise<unknown> {\n  const qdrant = getClient();\n  return qdrant.getCollection(COLLECTION_NAME);\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;AAAA;AACA;;;AAEA,uBAAuB;AACvB,MAAM,aAAa,QAAQ,GAAG,CAAC,UAAU,IAAI;AAC7C,MAAM,iBAAiB,QAAQ,GAAG,CAAC,cAAc;AACjD,MAAM,kBAAkB,QAAQ,GAAG,CAAC,sBAAsB,IAAI;AAC9D,MAAM,sBAAsB,QAAQ,GAAG,CAAC,mBAAmB,KAAK;AAEhE,sDAAsD;AACtD,MAAM,sBAAsB;AAE5B,IAAI,SAA8B;AAK3B,SAAS;IACd,IAAI,CAAC,QAAQ;QACX,SAAS,IAAI,qMAAY,CAAC;YACxB,KAAK;YACL,QAAQ;YACR,kDAAkD;YAClD,YAAY,sBACR,IAAI,8GAAK,CAAC,KAAK,CAAC;gBAAE,oBAAoB;YAAM,KAC5C;QACN;IACF;IACA,OAAO;AACT;AAKO,SAAS;IACd,OAAO;AACT;AAKO,eAAe;IACpB,MAAM,SAAS;IAEf,IAAI;QACF,6BAA6B;QAC7B,MAAM,cAAc,MAAM,OAAO,cAAc;QAC/C,MAAM,SAAS,YAAY,WAAW,CAAC,IAAI,CACzC,CAAC,IAAM,EAAE,IAAI,KAAK;QAGpB,IAAI,CAAC,QAAQ;YACX,iEAAiE;YACjE,MAAM,OAAO,gBAAgB,CAAC,iBAAiB;gBAC7C,SAAS;oBACP,MAAM;oBACN,UAAU;gBACZ;gBACA,mDAAmD;gBACnD,mBAAmB;oBACjB,oBAAoB;gBACtB;YACF;YAEA,iDAAiD;YACjD,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,QAAQ,GAAG,CAAC,CAAC,2BAA2B,EAAE,iBAAiB;QAC7D;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,qCAAqC;QACnD,MAAM;IACR;AACF;AAKO,eAAe,cACpB,MAIE;IAEF,MAAM,SAAS;IAEf,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnC,QAAQ,OAAO,GAAG,CAAC,CAAC,IAAM,CAAC;gBACzB,IAAI,EAAE,EAAE;gBACR,QAAQ,EAAE,MAAM;gBAChB,SAAS,EAAE,OAAO;YACpB,CAAC;IACH;AACF;AAKO,eAAe,cACpB,MAAgB,EAChB,QAAgB,EAAE,EAClB,MAAgC;IAQhC,MAAM,SAAS;IAEf,MAAM,UAAU,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnD;QACA;QACA,QAAQ;QACR,cAAc;QACd,iBAAiB;IACnB;IAEA,OAAO,QAAQ,GAAG,CAAC,CAAC,IAAM,CAAC;YACzB,IAAI,OAAO,EAAE,EAAE;YACf,OAAO,EAAE,KAAK;YACd,SAAS,AAAC,EAAE,OAAO,IAAgC,CAAC;QACtD,CAAC;AACH;AAKO,eAAe,sBACpB,MAA+B;IAE/B,MAAM,SAAS;IAEf,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnC,QAAQ;IACV;AACF;AAKO,eAAe;IACpB,MAAM,SAAS;IACf,OAAO,OAAO,aAAa,CAAC;AAC9B"}},
    {"offset": {"line": 691, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/ingest.ts"],"sourcesContent":["import { v4 as uuidv4 } from \"uuid\";\nimport {\n  processDocument,\n  type ProcessedDocument,\n} from \"./document-processor\";\nimport { embedTexts } from \"./embeddings\";\nimport {\n  ensureCollection,\n  upsertVectors,\n  deleteVectorsByFilter,\n  getClient,\n  getCollectionName,\n} from \"./qdrant\";\nimport { ensureBucket, deleteDocumentFiles } from \"./storage\";\nimport type { ParserType } from \"./parsers\";\n\nexport interface IngestOptions {\n  parserType?: ParserType;\n  replaceDocId?: string; // If set, will replace existing document\n  metadata?: {\n    docType?: string;\n    topic?: string;\n    category?: string;\n    status?: \"ready\" | \"processing\" | \"archived\";\n  };\n}\n\nexport interface IngestResult {\n  docId: string;\n  docName: string;\n  pageCount: number;\n  chunkCount: number;\n  status: \"ready\" | \"failed\";\n  error?: string;\n  parserUsed: ParserType;\n  processedAt: Date;\n}\n\nexport interface DocumentRecord {\n  docId: string;\n  docName: string;\n  originalPath: string;\n  pageCount: number;\n  chunkCount: number;\n  parserUsed: ParserType;\n  status: \"processing\" | \"ready\" | \"failed\" | \"archived\";\n  createdAt: Date;\n  updatedAt: Date;\n  metadata?: Record<string, unknown>;\n}\n\n// In-memory document registry (in production, use a database)\nconst documentRegistry = new Map<string, DocumentRecord>();\nconst SKIP_REGISTRY_SYNC =\n  process.env.QDRANT_SKIP_REGISTRY_SYNC === \"true\" ||\n  process.env.SKIP_REGISTRY_SYNC === \"true\";\n\n/**\n * Ingest a document into the RAG system\n * 1. Process the document (parse, chunk, upload to MinIO)\n * 2. Generate embeddings for chunks\n * 3. Store vectors in Qdrant with metadata\n */\nexport async function ingestDocument(\n  file: Buffer,\n  filename: string,\n  docName: string,\n  options: IngestOptions = {}\n): Promise<IngestResult> {\n  const { parserType = \"basic\", replaceDocId, metadata = {} } = options;\n  const docId = replaceDocId || uuidv4();\n\n  try {\n    // Ensure infrastructure is ready\n    await Promise.all([ensureCollection(), ensureBucket()]);\n\n    // Update registry to show processing\n    documentRegistry.set(docId, {\n      docId,\n      docName,\n      originalPath: \"\",\n      pageCount: 0,\n      chunkCount: 0,\n      parserUsed: parserType,\n      status: \"processing\",\n      createdAt: replaceDocId\n        ? (documentRegistry.get(docId)?.createdAt || new Date())\n        : new Date(),\n      updatedAt: new Date(),\n      metadata,\n    });\n\n    // If replacing, clean up old data\n    if (replaceDocId) {\n      console.log(`Replacing document: ${replaceDocId}`);\n      await deleteVectorsByFilter({\n        must: [{ key: \"doc_id\", match: { value: replaceDocId } }],\n      });\n      await deleteDocumentFiles(replaceDocId);\n    }\n\n    // Process the document\n    const processed: ProcessedDocument = await processDocument(\n      file,\n      filename,\n      docName,\n      parserType,\n      docId\n    );\n\n    // Collect all chunks for embedding\n    const allChunks: Array<{\n      chunkId: string;\n      text: string;\n      pageNumber: number;\n      chunkIndex: number;\n      imageUrl: string;\n    }> = [];\n\n    for (const page of processed.pages) {\n      for (const chunk of page.chunks) {\n        allChunks.push({\n          chunkId: chunk.chunkId,\n          text: chunk.text,\n          pageNumber: page.pageNumber,\n          chunkIndex: chunk.chunkIndex,\n          imageUrl: page.imageUrl,\n        });\n      }\n    }\n\n    console.log(`Generating embeddings for ${allChunks.length} chunks...`);\n\n    // Generate embeddings in batches\n    const BATCH_SIZE = 100;\n    const points: Array<{\n      id: string;\n      vector: number[];\n      payload: Record<string, unknown>;\n    }> = [];\n\n    for (let i = 0; i < allChunks.length; i += BATCH_SIZE) {\n      const batch = allChunks.slice(i, i + BATCH_SIZE);\n      const texts = batch.map((c) => c.text);\n      const embeddings = await embedTexts(texts);\n\n      for (let j = 0; j < batch.length; j++) {\n        const chunk = batch[j];\n        points.push({\n          id: chunk.chunkId,\n          vector: embeddings[j],\n          payload: {\n            doc_id: docId,\n            doc_name: docName,\n            page_number: chunk.pageNumber,\n            chunk_index: chunk.chunkIndex,\n            text: chunk.text,\n            image_url: chunk.imageUrl,\n            parser_used: parserType,\n            status: \"ready\",\n            doc_type: metadata.docType || \"unknown\",\n            topic: metadata.topic || \"\",\n            category: metadata.category || \"\",\n            created_at: new Date().toISOString(),\n          },\n        });\n      }\n    }\n\n    // Store vectors in Qdrant\n    console.log(`Storing ${points.length} vectors in Qdrant...`);\n    await upsertVectors(points);\n\n    // Update registry with final state\n    const record: DocumentRecord = {\n      docId,\n      docName,\n      originalPath: processed.originalPath,\n      pageCount: processed.pages.length,\n      chunkCount: processed.totalChunks,\n      parserUsed: parserType,\n      status: \"ready\",\n      createdAt: replaceDocId\n        ? (documentRegistry.get(docId)?.createdAt || new Date())\n        : new Date(),\n      updatedAt: new Date(),\n      metadata,\n    };\n    documentRegistry.set(docId, record);\n\n    console.log(`Successfully ingested document: ${docName} (${docId})`);\n\n    return {\n      docId,\n      docName,\n      pageCount: processed.pages.length,\n      chunkCount: processed.totalChunks,\n      status: \"ready\",\n      parserUsed: parserType,\n      processedAt: new Date(),\n    };\n  } catch (error) {\n    console.error(`Error ingesting document:`, error);\n\n    // Update registry with failed state\n    const existingRecord = documentRegistry.get(docId);\n    if (existingRecord) {\n      existingRecord.status = \"failed\";\n      existingRecord.updatedAt = new Date();\n    }\n\n    return {\n      docId,\n      docName,\n      pageCount: 0,\n      chunkCount: 0,\n      status: \"failed\",\n      error: (error as Error).message,\n      parserUsed: parserType,\n      processedAt: new Date(),\n    };\n  }\n}\n\n/**\n * Archive a document (mark as archived but keep data)\n */\nexport async function archiveDocument(docId: string): Promise<void> {\n  const record = documentRegistry.get(docId);\n  if (record) {\n    record.status = \"archived\";\n    record.updatedAt = new Date();\n  }\n\n  // Update vectors to archived status\n  // Note: Qdrant doesn't support bulk updates, so we'd need to re-upsert\n  // For now, we just update the registry\n}\n\n/**\n * Delete a document completely\n */\nexport async function deleteDocument(docId: string): Promise<void> {\n  // Delete from Qdrant\n  await deleteVectorsByFilter({\n    must: [{ key: \"doc_id\", match: { value: docId } }],\n  });\n\n  // Delete from MinIO\n  await deleteDocumentFiles(docId);\n\n  // Remove from registry\n  documentRegistry.delete(docId);\n\n  console.log(`Deleted document: ${docId}`);\n}\n\n/**\n * Get all documents in the registry\n */\nexport async function listDocuments(): Promise<DocumentRecord[]> {\n  if (!SKIP_REGISTRY_SYNC && documentRegistry.size === 0) {\n    await syncRegistryFromQdrant();\n  }\n\n  return Array.from(documentRegistry.values()).sort(\n    (a, b) => b.updatedAt.getTime() - a.updatedAt.getTime()\n  );\n}\n\n/**\n * Get a specific document\n */\nexport function getDocument(docId: string): DocumentRecord | undefined {\n  const record = documentRegistry.get(docId);\n  return record;\n}\n\n/**\n * Initialize the document registry from Qdrant\n * Call this on startup to sync state\n */\nexport async function initializeRegistry(): Promise<void> {\n  if (SKIP_REGISTRY_SYNC) {\n    console.log(\"Document registry init skipped (QDRANT_SKIP_REGISTRY_SYNC=true)\");\n    return;\n  }\n  await syncRegistryFromQdrant();\n  console.log(\"Document registry initialized from Qdrant (in-memory cache)\");\n}\n\n/**\n * Rebuild the in-memory registry from Qdrant payloads\n * Useful after a restart so the Knowledge UI isn't empty\n */\nasync function syncRegistryFromQdrant(): Promise<void> {\n  try {\n    const qdrant = getClient();\n    const collection = getCollectionName();\n\n    let offset: number | string | undefined = undefined;\n    const aggregated = new Map<string, DocumentRecord>();\n\n    // Scroll through all points (chunks) and aggregate by doc_id\n    // Assumes small doc counts typical for Phase 1; adjust limit if needed\n    do {\n      const { points, next_page_offset } = await qdrant.scroll(collection, {\n        limit: 200,\n        with_payload: true,\n        with_vector: false,\n        offset,\n      });\n\n      for (const point of points) {\n        const payload = (point.payload as Record<string, unknown>) || {};\n        const docId = (payload.doc_id as string) || \"\";\n        if (!docId) continue;\n\n        const docName = (payload.doc_name as string) || \"Untitled\";\n        const pageNumber = Number(payload.page_number ?? 0);\n        const parserUsed = (payload.parser_used as ParserType) || \"basic\";\n        const status =\n          (payload.status as DocumentRecord[\"status\"]) || \"ready\";\n        const createdAtRaw = (payload.created_at as string) || undefined;\n        const createdAt = createdAtRaw ? new Date(createdAtRaw) : new Date();\n        const updatedAt = createdAt;\n\n        let record = aggregated.get(docId);\n        if (!record) {\n          record = {\n            docId,\n            docName,\n            originalPath: \"\",\n            pageCount: 0,\n            chunkCount: 0,\n            parserUsed,\n            status,\n            createdAt,\n            updatedAt,\n            metadata: payload,\n          };\n          aggregated.set(docId, record);\n        }\n\n        record.chunkCount += 1;\n        record.pageCount = Math.max(record.pageCount, pageNumber);\n        if (createdAt < record.createdAt) {\n          record.createdAt = createdAt;\n        }\n        if (updatedAt > record.updatedAt) {\n          record.updatedAt = updatedAt;\n        }\n      }\n\n      offset = next_page_offset || undefined;\n    } while (offset !== undefined);\n\n    // Replace in-memory registry\n    documentRegistry.clear();\n    for (const [docId, record] of aggregated.entries()) {\n      documentRegistry.set(docId, record);\n    }\n  } catch (error) {\n    console.warn(\n      \"Unable to sync registry from Qdrant, using in-memory only:\",\n      (error as Error).message\n    );\n  }\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;AAAA;AACA;AAIA;AACA;AAOA;;;;;;;;;;;AAsCA,8DAA8D;AAC9D,MAAM,mBAAmB,IAAI;AAC7B,MAAM,qBACJ,QAAQ,GAAG,CAAC,yBAAyB,KAAK,UAC1C,QAAQ,GAAG,CAAC,kBAAkB,KAAK;AAQ9B,eAAe,eACpB,IAAY,EACZ,QAAgB,EAChB,OAAe,EACf,UAAyB,CAAC,CAAC;IAE3B,MAAM,EAAE,aAAa,OAAO,EAAE,YAAY,EAAE,WAAW,CAAC,CAAC,EAAE,GAAG;IAC9D,MAAM,QAAQ,gBAAgB,IAAA,mLAAM;IAEpC,IAAI;QACF,iCAAiC;QACjC,MAAM,QAAQ,GAAG,CAAC;YAAC,IAAA,0IAAgB;YAAI,IAAA,uIAAY;SAAG;QAEtD,qCAAqC;QACrC,iBAAiB,GAAG,CAAC,OAAO;YAC1B;YACA;YACA,cAAc;YACd,WAAW;YACX,YAAY;YACZ,YAAY;YACZ,QAAQ;YACR,WAAW,eACN,iBAAiB,GAAG,CAAC,QAAQ,aAAa,IAAI,SAC/C,IAAI;YACR,WAAW,IAAI;YACf;QACF;QAEA,kCAAkC;QAClC,IAAI,cAAc;YAChB,QAAQ,GAAG,CAAC,CAAC,oBAAoB,EAAE,cAAc;YACjD,MAAM,IAAA,+IAAqB,EAAC;gBAC1B,MAAM;oBAAC;wBAAE,KAAK;wBAAU,OAAO;4BAAE,OAAO;wBAAa;oBAAE;iBAAE;YAC3D;YACA,MAAM,IAAA,8IAAmB,EAAC;QAC5B;QAEA,uBAAuB;QACvB,MAAM,YAA+B,MAAM,IAAA,wJAAe,EACxD,MACA,UACA,SACA,YACA;QAGF,mCAAmC;QACnC,MAAM,YAMD,EAAE;QAEP,KAAK,MAAM,QAAQ,UAAU,KAAK,CAAE;YAClC,KAAK,MAAM,SAAS,KAAK,MAAM,CAAE;gBAC/B,UAAU,IAAI,CAAC;oBACb,SAAS,MAAM,OAAO;oBACtB,MAAM,MAAM,IAAI;oBAChB,YAAY,KAAK,UAAU;oBAC3B,YAAY,MAAM,UAAU;oBAC5B,UAAU,KAAK,QAAQ;gBACzB;YACF;QACF;QAEA,QAAQ,GAAG,CAAC,CAAC,0BAA0B,EAAE,UAAU,MAAM,CAAC,UAAU,CAAC;QAErE,iCAAiC;QACjC,MAAM,aAAa;QACnB,MAAM,SAID,EAAE;QAEP,IAAK,IAAI,IAAI,GAAG,IAAI,UAAU,MAAM,EAAE,KAAK,WAAY;YACrD,MAAM,QAAQ,UAAU,KAAK,CAAC,GAAG,IAAI;YACrC,MAAM,QAAQ,MAAM,GAAG,CAAC,CAAC,IAAM,EAAE,IAAI;YACrC,MAAM,aAAa,MAAM,IAAA,wIAAU,EAAC;YAEpC,IAAK,IAAI,IAAI,GAAG,IAAI,MAAM,MAAM,EAAE,IAAK;gBACrC,MAAM,QAAQ,KAAK,CAAC,EAAE;gBACtB,OAAO,IAAI,CAAC;oBACV,IAAI,MAAM,OAAO;oBACjB,QAAQ,UAAU,CAAC,EAAE;oBACrB,SAAS;wBACP,QAAQ;wBACR,UAAU;wBACV,aAAa,MAAM,UAAU;wBAC7B,aAAa,MAAM,UAAU;wBAC7B,MAAM,MAAM,IAAI;wBAChB,WAAW,MAAM,QAAQ;wBACzB,aAAa;wBACb,QAAQ;wBACR,UAAU,SAAS,OAAO,IAAI;wBAC9B,OAAO,SAAS,KAAK,IAAI;wBACzB,UAAU,SAAS,QAAQ,IAAI;wBAC/B,YAAY,IAAI,OAAO,WAAW;oBACpC;gBACF;YACF;QACF;QAEA,0BAA0B;QAC1B,QAAQ,GAAG,CAAC,CAAC,QAAQ,EAAE,OAAO,MAAM,CAAC,qBAAqB,CAAC;QAC3D,MAAM,IAAA,uIAAa,EAAC;QAEpB,mCAAmC;QACnC,MAAM,SAAyB;YAC7B;YACA;YACA,cAAc,UAAU,YAAY;YACpC,WAAW,UAAU,KAAK,CAAC,MAAM;YACjC,YAAY,UAAU,WAAW;YACjC,YAAY;YACZ,QAAQ;YACR,WAAW,eACN,iBAAiB,GAAG,CAAC,QAAQ,aAAa,IAAI,SAC/C,IAAI;YACR,WAAW,IAAI;YACf;QACF;QACA,iBAAiB,GAAG,CAAC,OAAO;QAE5B,QAAQ,GAAG,CAAC,CAAC,gCAAgC,EAAE,QAAQ,EAAE,EAAE,MAAM,CAAC,CAAC;QAEnE,OAAO;YACL;YACA;YACA,WAAW,UAAU,KAAK,CAAC,MAAM;YACjC,YAAY,UAAU,WAAW;YACjC,QAAQ;YACR,YAAY;YACZ,aAAa,IAAI;QACnB;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,CAAC,yBAAyB,CAAC,EAAE;QAE3C,oCAAoC;QACpC,MAAM,iBAAiB,iBAAiB,GAAG,CAAC;QAC5C,IAAI,gBAAgB;YAClB,eAAe,MAAM,GAAG;YACxB,eAAe,SAAS,GAAG,IAAI;QACjC;QAEA,OAAO;YACL;YACA;YACA,WAAW;YACX,YAAY;YACZ,QAAQ;YACR,OAAO,AAAC,MAAgB,OAAO;YAC/B,YAAY;YACZ,aAAa,IAAI;QACnB;IACF;AACF;AAKO,eAAe,gBAAgB,KAAa;IACjD,MAAM,SAAS,iBAAiB,GAAG,CAAC;IACpC,IAAI,QAAQ;QACV,OAAO,MAAM,GAAG;QAChB,OAAO,SAAS,GAAG,IAAI;IACzB;AAEA,oCAAoC;AACpC,uEAAuE;AACvE,uCAAuC;AACzC;AAKO,eAAe,eAAe,KAAa;IAChD,qBAAqB;IACrB,MAAM,IAAA,+IAAqB,EAAC;QAC1B,MAAM;YAAC;gBAAE,KAAK;gBAAU,OAAO;oBAAE,OAAO;gBAAM;YAAE;SAAE;IACpD;IAEA,oBAAoB;IACpB,MAAM,IAAA,8IAAmB,EAAC;IAE1B,uBAAuB;IACvB,iBAAiB,MAAM,CAAC;IAExB,QAAQ,GAAG,CAAC,CAAC,kBAAkB,EAAE,OAAO;AAC1C;AAKO,eAAe;IACpB,IAAI,CAAC,sBAAsB,iBAAiB,IAAI,KAAK,GAAG;QACtD,MAAM;IACR;IAEA,OAAO,MAAM,IAAI,CAAC,iBAAiB,MAAM,IAAI,IAAI,CAC/C,CAAC,GAAG,IAAM,EAAE,SAAS,CAAC,OAAO,KAAK,EAAE,SAAS,CAAC,OAAO;AAEzD;AAKO,SAAS,YAAY,KAAa;IACvC,MAAM,SAAS,iBAAiB,GAAG,CAAC;IACpC,OAAO;AACT;AAMO,eAAe;IACpB,IAAI,oBAAoB;QACtB,QAAQ,GAAG,CAAC;QACZ;IACF;IACA,MAAM;IACN,QAAQ,GAAG,CAAC;AACd;AAEA;;;CAGC,GACD,eAAe;IACb,IAAI;QACF,MAAM,SAAS,IAAA,mIAAS;QACxB,MAAM,aAAa,IAAA,2IAAiB;QAEpC,IAAI,SAAsC;QAC1C,MAAM,aAAa,IAAI;QAEvB,6DAA6D;QAC7D,uEAAuE;QACvE,GAAG;YACD,MAAM,EAAE,MAAM,EAAE,gBAAgB,EAAE,GAAG,MAAM,OAAO,MAAM,CAAC,YAAY;gBACnE,OAAO;gBACP,cAAc;gBACd,aAAa;gBACb;YACF;YAEA,KAAK,MAAM,SAAS,OAAQ;gBAC1B,MAAM,UAAU,AAAC,MAAM,OAAO,IAAgC,CAAC;gBAC/D,MAAM,QAAQ,AAAC,QAAQ,MAAM,IAAe;gBAC5C,IAAI,CAAC,OAAO;gBAEZ,MAAM,UAAU,AAAC,QAAQ,QAAQ,IAAe;gBAChD,MAAM,aAAa,OAAO,QAAQ,WAAW,IAAI;gBACjD,MAAM,aAAa,AAAC,QAAQ,WAAW,IAAmB;gBAC1D,MAAM,SACJ,AAAC,QAAQ,MAAM,IAAiC;gBAClD,MAAM,eAAe,AAAC,QAAQ,UAAU,IAAe;gBACvD,MAAM,YAAY,eAAe,IAAI,KAAK,gBAAgB,IAAI;gBAC9D,MAAM,YAAY;gBAElB,IAAI,SAAS,WAAW,GAAG,CAAC;gBAC5B,IAAI,CAAC,QAAQ;oBACX,SAAS;wBACP;wBACA;wBACA,cAAc;wBACd,WAAW;wBACX,YAAY;wBACZ;wBACA;wBACA;wBACA;wBACA,UAAU;oBACZ;oBACA,WAAW,GAAG,CAAC,OAAO;gBACxB;gBAEA,OAAO,UAAU,IAAI;gBACrB,OAAO,SAAS,GAAG,KAAK,GAAG,CAAC,OAAO,SAAS,EAAE;gBAC9C,IAAI,YAAY,OAAO,SAAS,EAAE;oBAChC,OAAO,SAAS,GAAG;gBACrB;gBACA,IAAI,YAAY,OAAO,SAAS,EAAE;oBAChC,OAAO,SAAS,GAAG;gBACrB;YACF;YAEA,SAAS,oBAAoB;QAC/B,QAAS,WAAW,UAAW;QAE/B,6BAA6B;QAC7B,iBAAiB,KAAK;QACtB,KAAK,MAAM,CAAC,OAAO,OAAO,IAAI,WAAW,OAAO,GAAI;YAClD,iBAAiB,GAAG,CAAC,OAAO;QAC9B;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,IAAI,CACV,8DACA,AAAC,MAAgB,OAAO;IAE5B;AACF"}},
    {"offset": {"line": 970, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/app/api/ingest/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from \"next/server\";\nimport {\n  ingestDocument,\n  listDocuments,\n  getDocument,\n  deleteDocument,\n  type IngestResult,\n  initializeRegistry,\n} from \"@/lib/ingest\";\nimport type { ParserType } from \"@/lib/parsers\";\n\n/**\n * POST /api/ingest - Upload and ingest a document\n *\n * Form data:\n * - file: PDF file to ingest\n * - docName: Human-readable document name\n * - parserType: \"basic\" | \"gemini\" | \"docling\" (optional, default: \"basic\")\n * - replaceDocId: Document ID to replace (optional)\n * - docType: Document type for metadata (optional)\n * - topic: Topic/category for filtering (optional)\n */\nexport async function POST(req: NextRequest): Promise<NextResponse> {\n  try {\n    const formData = await req.formData();\n\n    // Get file\n    const file = formData.get(\"file\") as File | null;\n    if (!file) {\n      return NextResponse.json(\n        { error: \"No file provided\" },\n        { status: 400 }\n      );\n    }\n\n    // Get document name\n    const docName = (formData.get(\"docName\") as string) || file.name;\n\n    // Get parser type\n    const parserType =\n      (formData.get(\"parserType\") as ParserType) || \"basic\";\n    if (![\"basic\", \"gemini\", \"docling\"].includes(parserType)) {\n      return NextResponse.json(\n        { error: \"Invalid parser type. Use 'basic', 'gemini', or 'docling'\" },\n        { status: 400 }\n      );\n    }\n\n    // Get optional parameters\n    const replaceDocId = formData.get(\"replaceDocId\") as string | null;\n    const docType = formData.get(\"docType\") as string | null;\n    const topic = formData.get(\"topic\") as string | null;\n\n    // Validate file type\n    const allowedTypes = [\n      \"application/pdf\",\n      \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n    ];\n    if (!allowedTypes.includes(file.type) && !file.name.endsWith(\".pdf\")) {\n      return NextResponse.json(\n        { error: \"Only PDF files are currently supported\" },\n        { status: 400 }\n      );\n    }\n\n    // Convert File to Buffer\n    const arrayBuffer = await file.arrayBuffer();\n    const buffer = Buffer.from(arrayBuffer);\n\n    // Ingest the document\n    const result: IngestResult = await ingestDocument(\n      buffer,\n      file.name,\n      docName,\n      {\n        parserType,\n        replaceDocId: replaceDocId || undefined,\n        metadata: {\n          docType: docType || undefined,\n          topic: topic || undefined,\n          status: \"ready\",\n        },\n      }\n    );\n\n    if (result.status === \"failed\") {\n      return NextResponse.json(\n        { error: result.error || \"Ingestion failed\", result },\n        { status: 500 }\n      );\n    }\n\n    return NextResponse.json({\n      success: true,\n      message: replaceDocId\n        ? `Document replaced successfully`\n        : `Document ingested successfully`,\n      result,\n    });\n  } catch (error) {\n    console.error(\"Ingestion error:\", error);\n    return NextResponse.json(\n      { error: (error as Error).message },\n      { status: 500 }\n    );\n  }\n}\n\n/**\n * GET /api/ingest - List all documents or get a specific document\n *\n * Query params:\n * - docId: Get specific document (optional)\n */\nexport async function GET(req: NextRequest): Promise<NextResponse> {\n  try {\n    const { searchParams } = new URL(req.url);\n    const docId = searchParams.get(\"docId\");\n\n    if (docId) {\n      let doc = getDocument(docId);\n      if (!doc) {\n        // Try to rebuild registry from Qdrant in case of restart\n        await initializeRegistry();\n        doc = getDocument(docId);\n      }\n      if (!doc) {\n        return NextResponse.json(\n          { error: \"Document not found\" },\n          { status: 404 }\n        );\n      }\n      return NextResponse.json({ document: doc });\n    }\n\n    const documents = await listDocuments();\n    return NextResponse.json({ documents });\n  } catch (error) {\n    console.error(\"Error listing documents:\", error);\n    return NextResponse.json(\n      { error: (error as Error).message },\n      { status: 500 }\n    );\n  }\n}\n\n/**\n * DELETE /api/ingest - Delete a document\n *\n * Query params:\n * - docId: Document ID to delete (required)\n */\nexport async function DELETE(req: NextRequest): Promise<NextResponse> {\n  try {\n    const { searchParams } = new URL(req.url);\n    const docId = searchParams.get(\"docId\");\n\n    if (!docId) {\n      return NextResponse.json(\n        { error: \"docId is required\" },\n        { status: 400 }\n      );\n    }\n\n    await deleteDocument(docId);\n\n    return NextResponse.json({\n      success: true,\n      message: `Document ${docId} deleted successfully`,\n    });\n  } catch (error) {\n    console.error(\"Error deleting document:\", error);\n    return NextResponse.json(\n      { error: (error as Error).message },\n      { status: 500 }\n    );\n  }\n}\n"],"names":[],"mappings":";;;;;;;;AAAA;AACA;;;;;;;AAqBO,eAAe,KAAK,GAAgB;IACzC,IAAI;QACF,MAAM,WAAW,MAAM,IAAI,QAAQ;QAEnC,WAAW;QACX,MAAM,OAAO,SAAS,GAAG,CAAC;QAC1B,IAAI,CAAC,MAAM;YACT,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAmB,GAC5B;gBAAE,QAAQ;YAAI;QAElB;QAEA,oBAAoB;QACpB,MAAM,UAAU,AAAC,SAAS,GAAG,CAAC,cAAyB,KAAK,IAAI;QAEhE,kBAAkB;QAClB,MAAM,aACJ,AAAC,SAAS,GAAG,CAAC,iBAAgC;QAChD,IAAI,CAAC;YAAC;YAAS;YAAU;SAAU,CAAC,QAAQ,CAAC,aAAa;YACxD,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAA2D,GACpE;gBAAE,QAAQ;YAAI;QAElB;QAEA,0BAA0B;QAC1B,MAAM,eAAe,SAAS,GAAG,CAAC;QAClC,MAAM,UAAU,SAAS,GAAG,CAAC;QAC7B,MAAM,QAAQ,SAAS,GAAG,CAAC;QAE3B,qBAAqB;QACrB,MAAM,eAAe;YACnB;YACA;SACD;QACD,IAAI,CAAC,aAAa,QAAQ,CAAC,KAAK,IAAI,KAAK,CAAC,KAAK,IAAI,CAAC,QAAQ,CAAC,SAAS;YACpE,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAyC,GAClD;gBAAE,QAAQ;YAAI;QAElB;QAEA,yBAAyB;QACzB,MAAM,cAAc,MAAM,KAAK,WAAW;QAC1C,MAAM,SAAS,OAAO,IAAI,CAAC;QAE3B,sBAAsB;QACtB,MAAM,SAAuB,MAAM,IAAA,wIAAc,EAC/C,QACA,KAAK,IAAI,EACT,SACA;YACE;YACA,cAAc,gBAAgB;YAC9B,UAAU;gBACR,SAAS,WAAW;gBACpB,OAAO,SAAS;gBAChB,QAAQ;YACV;QACF;QAGF,IAAI,OAAO,MAAM,KAAK,UAAU;YAC9B,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO,OAAO,KAAK,IAAI;gBAAoB;YAAO,GACpD;gBAAE,QAAQ;YAAI;QAElB;QAEA,OAAO,gJAAY,CAAC,IAAI,CAAC;YACvB,SAAS;YACT,SAAS,eACL,CAAC,8BAA8B,CAAC,GAChC,CAAC,8BAA8B,CAAC;YACpC;QACF;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,oBAAoB;QAClC,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO,AAAC,MAAgB,OAAO;QAAC,GAClC;YAAE,QAAQ;QAAI;IAElB;AACF;AAQO,eAAe,IAAI,GAAgB;IACxC,IAAI;QACF,MAAM,EAAE,YAAY,EAAE,GAAG,IAAI,IAAI,IAAI,GAAG;QACxC,MAAM,QAAQ,aAAa,GAAG,CAAC;QAE/B,IAAI,OAAO;YACT,IAAI,MAAM,IAAA,qIAAW,EAAC;YACtB,IAAI,CAAC,KAAK;gBACR,yDAAyD;gBACzD,MAAM,IAAA,4IAAkB;gBACxB,MAAM,IAAA,qIAAW,EAAC;YACpB;YACA,IAAI,CAAC,KAAK;gBACR,OAAO,gJAAY,CAAC,IAAI,CACtB;oBAAE,OAAO;gBAAqB,GAC9B;oBAAE,QAAQ;gBAAI;YAElB;YACA,OAAO,gJAAY,CAAC,IAAI,CAAC;gBAAE,UAAU;YAAI;QAC3C;QAEA,MAAM,YAAY,MAAM,IAAA,uIAAa;QACrC,OAAO,gJAAY,CAAC,IAAI,CAAC;YAAE;QAAU;IACvC,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,4BAA4B;QAC1C,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO,AAAC,MAAgB,OAAO;QAAC,GAClC;YAAE,QAAQ;QAAI;IAElB;AACF;AAQO,eAAe,OAAO,GAAgB;IAC3C,IAAI;QACF,MAAM,EAAE,YAAY,EAAE,GAAG,IAAI,IAAI,IAAI,GAAG;QACxC,MAAM,QAAQ,aAAa,GAAG,CAAC;QAE/B,IAAI,CAAC,OAAO;YACV,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAoB,GAC7B;gBAAE,QAAQ;YAAI;QAElB;QAEA,MAAM,IAAA,wIAAc,EAAC;QAErB,OAAO,gJAAY,CAAC,IAAI,CAAC;YACvB,SAAS;YACT,SAAS,CAAC,SAAS,EAAE,MAAM,qBAAqB,CAAC;QACnD;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,4BAA4B;QAC1C,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO,AAAC,MAAgB,OAAO;QAAC,GAClC;YAAE,QAAQ;QAAI;IAElB;AACF"}}]
}