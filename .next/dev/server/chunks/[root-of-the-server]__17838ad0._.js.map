{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 16, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/parsers/basic-parser.ts"],"sourcesContent":["import { pdf } from \"pdf-to-img\";\nimport type { DocumentParser, ParsedDocument, ParsedPage } from \"./index\";\n\n/**\n * Basic PDF parser using pdf-to-img for page rendering\n * and basic text extraction\n *\n * Good for: Text-based PDFs, simple documents\n * Limitations: May not handle complex layouts or scanned PDFs well\n */\nexport class BasicParser implements DocumentParser {\n  async parse(file: Buffer, filename: string): Promise<ParsedDocument> {\n    const ext = filename.toLowerCase().split(\".\").pop();\n\n    if (ext !== \"pdf\") {\n      throw new Error(`Unsupported file type: ${ext}. Only PDF is supported.`);\n    }\n\n    const pages: ParsedPage[] = [];\n\n    try {\n      // Convert PDF pages to images using pdf-to-img\n      const document = await pdf(file, {\n        scale: 2.0, // Higher quality for OCR if needed\n      });\n\n      let pageNumber = 0;\n      for await (const image of document) {\n        pageNumber++;\n\n        // pdf-to-img returns PNG buffers\n        const imageBuffer = Buffer.from(image);\n\n        // For basic parser, we'll extract text from the PDF metadata if available\n        // The actual text extraction is limited - use Gemini parser for better OCR\n        pages.push({\n          pageNumber,\n          text: `[Page ${pageNumber} - Use Gemini parser for text extraction]`,\n          imageBuffer,\n        });\n      }\n\n      // Try to extract actual text using a simple approach\n      // We'll enhance this with proper text extraction\n      const textPages = await this.extractTextFromPdf(file);\n      for (let i = 0; i < pages.length && i < textPages.length; i++) {\n        if (textPages[i].trim()) {\n          pages[i].text = textPages[i];\n        }\n      }\n\n      return {\n        pages,\n        metadata: {\n          pageCount: pages.length,\n          parserUsed: \"basic\",\n        },\n      };\n    } catch (error) {\n      console.error(\"Error parsing PDF:\", error);\n      throw new Error(`Failed to parse PDF: ${(error as Error).message}`);\n    }\n  }\n\n  /**\n   * Extract text from PDF using basic string matching\n   * This is a fallback - use Gemini for better extraction\n   */\n  private async extractTextFromPdf(buffer: Buffer): Promise<string[]> {\n    // Simple text extraction from PDF buffer\n    // This looks for text streams in the PDF\n    const content = buffer.toString(\"latin1\");\n    const pages: string[] = [];\n\n    // Find text between BT (begin text) and ET (end text) markers\n    const textMatches = content.match(/BT[\\s\\S]*?ET/g) || [];\n\n    let currentPageText = \"\";\n    let pageBreakCount = 0;\n\n    for (const match of textMatches) {\n      // Extract text from Tj and TJ operators\n      const tjMatches = match.match(/\\(([^)]*)\\)\\s*Tj/g) || [];\n      const tjArrayMatches = match.match(/\\[([^\\]]*)\\]\\s*TJ/g) || [];\n\n      for (const tj of tjMatches) {\n        const text = tj.match(/\\(([^)]*)\\)/)?.[1] || \"\";\n        currentPageText += this.decodeText(text) + \" \";\n      }\n\n      for (const tjArray of tjArrayMatches) {\n        const items = tjArray.match(/\\(([^)]*)\\)/g) || [];\n        for (const item of items) {\n          const text = item.match(/\\(([^)]*)\\)/)?.[1] || \"\";\n          currentPageText += this.decodeText(text);\n        }\n        currentPageText += \" \";\n      }\n\n      // Check for page breaks (simplified detection)\n      if (match.includes(\"showpage\") || match.includes(\"Page\")) {\n        pageBreakCount++;\n        if (currentPageText.trim()) {\n          pages.push(currentPageText.trim());\n          currentPageText = \"\";\n        }\n      }\n    }\n\n    // Add remaining text as last page\n    if (currentPageText.trim()) {\n      pages.push(currentPageText.trim());\n    }\n\n    return pages;\n  }\n\n  /**\n   * Decode PDF text encoding\n   */\n  private decodeText(text: string): string {\n    // Handle common PDF escape sequences\n    return text\n      .replace(/\\\\n/g, \"\\n\")\n      .replace(/\\\\r/g, \"\\r\")\n      .replace(/\\\\t/g, \"\\t\")\n      .replace(/\\\\\\(/g, \"(\")\n      .replace(/\\\\\\)/g, \")\")\n      .replace(/\\\\\\\\/g, \"\\\\\");\n  }\n}\n"],"names":[],"mappings":";;;;AAAA;;;;;;AAUO,MAAM;IACX,MAAM,MAAM,IAAY,EAAE,QAAgB,EAA2B;QACnE,MAAM,MAAM,SAAS,WAAW,GAAG,KAAK,CAAC,KAAK,GAAG;QAEjD,IAAI,QAAQ,OAAO;YACjB,MAAM,IAAI,MAAM,CAAC,uBAAuB,EAAE,IAAI,wBAAwB,CAAC;QACzE;QAEA,MAAM,QAAsB,EAAE;QAE9B,IAAI;YACF,+CAA+C;YAC/C,MAAM,WAAW,MAAM,IAAA,uIAAG,EAAC,MAAM;gBAC/B,OAAO;YACT;YAEA,IAAI,aAAa;YACjB,WAAW,MAAM,SAAS,SAAU;gBAClC;gBAEA,iCAAiC;gBACjC,MAAM,cAAc,OAAO,IAAI,CAAC;gBAEhC,0EAA0E;gBAC1E,2EAA2E;gBAC3E,MAAM,IAAI,CAAC;oBACT;oBACA,MAAM,CAAC,MAAM,EAAE,WAAW,yCAAyC,CAAC;oBACpE;gBACF;YACF;YAEA,qDAAqD;YACrD,iDAAiD;YACjD,MAAM,YAAY,MAAM,IAAI,CAAC,kBAAkB,CAAC;YAChD,IAAK,IAAI,IAAI,GAAG,IAAI,MAAM,MAAM,IAAI,IAAI,UAAU,MAAM,EAAE,IAAK;gBAC7D,IAAI,SAAS,CAAC,EAAE,CAAC,IAAI,IAAI;oBACvB,KAAK,CAAC,EAAE,CAAC,IAAI,GAAG,SAAS,CAAC,EAAE;gBAC9B;YACF;YAEA,OAAO;gBACL;gBACA,UAAU;oBACR,WAAW,MAAM,MAAM;oBACvB,YAAY;gBACd;YACF;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,sBAAsB;YACpC,MAAM,IAAI,MAAM,CAAC,qBAAqB,EAAE,AAAC,MAAgB,OAAO,EAAE;QACpE;IACF;IAEA;;;GAGC,GACD,MAAc,mBAAmB,MAAc,EAAqB;QAClE,yCAAyC;QACzC,yCAAyC;QACzC,MAAM,UAAU,OAAO,QAAQ,CAAC;QAChC,MAAM,QAAkB,EAAE;QAE1B,8DAA8D;QAC9D,MAAM,cAAc,QAAQ,KAAK,CAAC,oBAAoB,EAAE;QAExD,IAAI,kBAAkB;QACtB,IAAI,iBAAiB;QAErB,KAAK,MAAM,SAAS,YAAa;YAC/B,wCAAwC;YACxC,MAAM,YAAY,MAAM,KAAK,CAAC,wBAAwB,EAAE;YACxD,MAAM,iBAAiB,MAAM,KAAK,CAAC,yBAAyB,EAAE;YAE9D,KAAK,MAAM,MAAM,UAAW;gBAC1B,MAAM,OAAO,GAAG,KAAK,CAAC,gBAAgB,CAAC,EAAE,IAAI;gBAC7C,mBAAmB,IAAI,CAAC,UAAU,CAAC,QAAQ;YAC7C;YAEA,KAAK,MAAM,WAAW,eAAgB;gBACpC,MAAM,QAAQ,QAAQ,KAAK,CAAC,mBAAmB,EAAE;gBACjD,KAAK,MAAM,QAAQ,MAAO;oBACxB,MAAM,OAAO,KAAK,KAAK,CAAC,gBAAgB,CAAC,EAAE,IAAI;oBAC/C,mBAAmB,IAAI,CAAC,UAAU,CAAC;gBACrC;gBACA,mBAAmB;YACrB;YAEA,+CAA+C;YAC/C,IAAI,MAAM,QAAQ,CAAC,eAAe,MAAM,QAAQ,CAAC,SAAS;gBACxD;gBACA,IAAI,gBAAgB,IAAI,IAAI;oBAC1B,MAAM,IAAI,CAAC,gBAAgB,IAAI;oBAC/B,kBAAkB;gBACpB;YACF;QACF;QAEA,kCAAkC;QAClC,IAAI,gBAAgB,IAAI,IAAI;YAC1B,MAAM,IAAI,CAAC,gBAAgB,IAAI;QACjC;QAEA,OAAO;IACT;IAEA;;GAEC,GACD,AAAQ,WAAW,IAAY,EAAU;QACvC,qCAAqC;QACrC,OAAO,KACJ,OAAO,CAAC,QAAQ,MAChB,OAAO,CAAC,QAAQ,MAChB,OAAO,CAAC,QAAQ,MAChB,OAAO,CAAC,SAAS,KACjB,OAAO,CAAC,SAAS,KACjB,OAAO,CAAC,SAAS;IACtB;AACF"}},
    {"offset": {"line": 128, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/parsers/docling-parser.ts"],"sourcesContent":["import type { DocumentParser, ParsedDocument, ParsedPage } from \"./index\";\nimport { BasicParser } from \"./basic-parser\";\n\n/**\n * Docling parser that calls a running docling-serve instance.\n * Falls back to BasicParser if the service is unreachable or not configured.\n *\n * Env:\n * - DOCLING_SERVE_URL: full URL to docling-serve /v1/convert/file endpoint\n */\nexport class DoclingParser implements DocumentParser {\n  async parse(file: Buffer, filename: string): Promise<ParsedDocument> {\n    const endpoint =\n      process.env.DOCLING_SERVE_URL ||\n      process.env.DOCLING_ENDPOINT ||\n      \"\";\n\n    if (!endpoint) {\n      console.warn(\n        \"DOCLING_SERVE_URL not set. Falling back to BasicParser for this ingest.\"\n      );\n      return this.fallbackToBasic(file, filename);\n    }\n\n    try {\n      const formData = new FormData();\n      // docling-serve expects \"files\" (plural)\n      formData.append(\"files\", new Blob([file]), filename);\n\n      const response = await fetch(endpoint, {\n        method: \"POST\",\n        body: formData,\n      });\n\n      if (!response.ok) {\n        const errorText = await response.text();\n        throw new Error(\n          `Docling-serve responded with status ${response.status}: ${errorText}`\n        );\n      }\n\n      const data = (await response.json()) as Record<string, unknown>;\n\n      // docling-serve returns { document: { md_content, pages, ... } } or { documents: [...] }\n      const doc = (data.document as Record<string, unknown>) ||\n                  ((data.documents as Record<string, unknown>[])?.[0]) ||\n                  data;\n\n      // Extract markdown content or text\n      const mdContent = (doc.md_content as string) ||\n                       (doc.text as string) ||\n                       (doc.content as string) ||\n                       \"\";\n\n      // Try to get pages array, or create single page from markdown\n      const rawPages = (doc.pages as unknown[]) || [];\n\n      let pages: ParsedPage[];\n\n      if (rawPages.length > 0) {\n        pages = rawPages.map((p: unknown, idx: number) => {\n          const page = p as Record<string, unknown>;\n          return {\n            pageNumber: Number(page.page_no ?? page.pageNumber ?? page.page ?? idx + 1) || idx + 1,\n            text: (page.text as string) || (page.content as string) || (page.md_content as string) || \"\",\n            imageBuffer: undefined,\n          };\n        });\n      } else if (mdContent) {\n        // If no pages array but we have markdown, split by page breaks or treat as single page\n        const pageTexts = mdContent.split(/<!-- page-break -->|\\n---\\n/).filter(Boolean);\n        pages = pageTexts.map((text, idx) => ({\n          pageNumber: idx + 1,\n          text: text.trim(),\n          imageBuffer: undefined,\n        }));\n      } else {\n        pages = [];\n      }\n\n      return {\n        pages,\n        metadata: {\n          pageCount: pages.length,\n          parserUsed: \"docling\",\n          title: (doc.title as string) || undefined,\n          author: (doc.author as string) || undefined,\n        },\n      };\n    } catch (error) {\n      console.warn(\n        \"Docling-serve unavailable; falling back to BasicParser. Error:\",\n        (error as Error).message\n      );\n      return this.fallbackToBasic(file, filename);\n    }\n  }\n\n  private async fallbackToBasic(\n    file: Buffer,\n    filename: string\n  ): Promise<ParsedDocument> {\n    const basic = new BasicParser();\n    const parsed = await basic.parse(file, filename);\n    return {\n      ...parsed,\n      metadata: {\n        ...parsed.metadata,\n        parserUsed: \"docling\",\n      },\n    };\n  }\n}\n"],"names":[],"mappings":";;;;AACA;;;;;;AASO,MAAM;IACX,MAAM,MAAM,IAAY,EAAE,QAAgB,EAA2B;QACnE,MAAM,WACJ,QAAQ,GAAG,CAAC,iBAAiB,IAC7B,QAAQ,GAAG,CAAC,gBAAgB,IAC5B;QAEF,IAAI,CAAC,UAAU;YACb,QAAQ,IAAI,CACV;YAEF,OAAO,IAAI,CAAC,eAAe,CAAC,MAAM;QACpC;QAEA,IAAI;YACF,MAAM,WAAW,IAAI;YACrB,yCAAyC;YACzC,SAAS,MAAM,CAAC,SAAS,IAAI,KAAK;gBAAC;aAAK,GAAG;YAE3C,MAAM,WAAW,MAAM,MAAM,UAAU;gBACrC,QAAQ;gBACR,MAAM;YACR;YAEA,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,MAAM,YAAY,MAAM,SAAS,IAAI;gBACrC,MAAM,IAAI,MACR,CAAC,oCAAoC,EAAE,SAAS,MAAM,CAAC,EAAE,EAAE,WAAW;YAE1E;YAEA,MAAM,OAAQ,MAAM,SAAS,IAAI;YAEjC,yFAAyF;YACzF,MAAM,MAAM,AAAC,KAAK,QAAQ,IACZ,KAAK,SAAS,EAAgC,CAAC,EAAE,IACnD;YAEZ,mCAAmC;YACnC,MAAM,YAAY,AAAC,IAAI,UAAU,IACf,IAAI,IAAI,IACR,IAAI,OAAO,IACZ;YAEjB,8DAA8D;YAC9D,MAAM,WAAW,AAAC,IAAI,KAAK,IAAkB,EAAE;YAE/C,IAAI;YAEJ,IAAI,SAAS,MAAM,GAAG,GAAG;gBACvB,QAAQ,SAAS,GAAG,CAAC,CAAC,GAAY;oBAChC,MAAM,OAAO;oBACb,OAAO;wBACL,YAAY,OAAO,KAAK,OAAO,IAAI,KAAK,UAAU,IAAI,KAAK,IAAI,IAAI,MAAM,MAAM,MAAM;wBACrF,MAAM,AAAC,KAAK,IAAI,IAAgB,KAAK,OAAO,IAAgB,KAAK,UAAU,IAAe;wBAC1F,aAAa;oBACf;gBACF;YACF,OAAO,IAAI,WAAW;gBACpB,uFAAuF;gBACvF,MAAM,YAAY,UAAU,KAAK,CAAC,+BAA+B,MAAM,CAAC;gBACxE,QAAQ,UAAU,GAAG,CAAC,CAAC,MAAM,MAAQ,CAAC;wBACpC,YAAY,MAAM;wBAClB,MAAM,KAAK,IAAI;wBACf,aAAa;oBACf,CAAC;YACH,OAAO;gBACL,QAAQ,EAAE;YACZ;YAEA,OAAO;gBACL;gBACA,UAAU;oBACR,WAAW,MAAM,MAAM;oBACvB,YAAY;oBACZ,OAAO,AAAC,IAAI,KAAK,IAAe;oBAChC,QAAQ,AAAC,IAAI,MAAM,IAAe;gBACpC;YACF;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,IAAI,CACV,kEACA,AAAC,MAAgB,OAAO;YAE1B,OAAO,IAAI,CAAC,eAAe,CAAC,MAAM;QACpC;IACF;IAEA,MAAc,gBACZ,IAAY,EACZ,QAAgB,EACS;QACzB,MAAM,QAAQ,IAAI,yJAAW;QAC7B,MAAM,SAAS,MAAM,MAAM,KAAK,CAAC,MAAM;QACvC,OAAO;YACL,GAAG,MAAM;YACT,UAAU;gBACR,GAAG,OAAO,QAAQ;gBAClB,YAAY;YACd;QACF;IACF;AACF"}}]
}