{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 280, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/embeddings.ts"],"sourcesContent":["import { OpenAIEmbeddings } from \"@langchain/openai\";\n\n// Use OpenAI's text-embedding-3-small model (1536 dimensions)\nconst embeddings = new OpenAIEmbeddings({\n  modelName: \"text-embedding-3-small\",\n  openAIApiKey: process.env.OPENAI_API_KEY,\n});\n\n/**\n * Generate embedding for a single text\n */\nexport async function embedText(text: string): Promise<number[]> {\n  const result = await embeddings.embedQuery(text);\n  return result;\n}\n\n/**\n * Generate embeddings for multiple texts in batch\n */\nexport async function embedTexts(texts: string[]): Promise<number[][]> {\n  const results = await embeddings.embedDocuments(texts);\n  return results;\n}\n\n/**\n * Get the embedding dimension (for Qdrant collection setup)\n */\nexport function getEmbeddingDimension(): number {\n  return 1536;\n}\n"],"names":[],"mappings":";;;;;;;;AAAA;AAAA;;AAEA,8DAA8D;AAC9D,MAAM,aAAa,IAAI,iLAAgB,CAAC;IACtC,WAAW;IACX,cAAc,QAAQ,GAAG,CAAC,cAAc;AAC1C;AAKO,eAAe,UAAU,IAAY;IAC1C,MAAM,SAAS,MAAM,WAAW,UAAU,CAAC;IAC3C,OAAO;AACT;AAKO,eAAe,WAAW,KAAe;IAC9C,MAAM,UAAU,MAAM,WAAW,cAAc,CAAC;IAChD,OAAO;AACT;AAKO,SAAS;IACd,OAAO;AACT"}},
    {"offset": {"line": 377, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/qdrant.ts"],"sourcesContent":["import { QdrantClient } from \"@qdrant/js-client-rest\";\n\n// Qdrant configuration\nconst QDRANT_URL = process.env.QDRANT_URL || \"http://localhost:6333\";\nconst QDRANT_API_KEY = process.env.QDRANT_API_KEY;\nconst COLLECTION_NAME = process.env.QDRANT_COLLECTION_NAME || \"knowledge_base\";\n\n// OpenAI embedding dimension (text-embedding-3-small)\nconst EMBEDDING_DIMENSION = 1536;\n\nlet client: QdrantClient | null = null;\n\n/**\n * Get or create the Qdrant client singleton\n */\nexport function getClient(): QdrantClient {\n  if (!client) {\n    client = new QdrantClient({\n      url: QDRANT_URL,\n      apiKey: QDRANT_API_KEY,\n      // Skip version compatibility check (can fail behind reverse proxies)\n      checkCompatibility: false,\n      // Add headers for Cloudflare compatibility (undici/fetch needs these)\n      headers: {\n        \"User-Agent\": \"qdrant-js-client/1.16.1\",\n      },\n    });\n  }\n  return client;\n}\n\n/**\n * Get the collection name\n */\nexport function getCollectionName(): string {\n  return COLLECTION_NAME;\n}\n\n/**\n * Ensure the knowledge base collection exists with proper configuration\n */\nexport async function ensureCollection(): Promise<void> {\n  const qdrant = getClient();\n\n  try {\n    // Check if collection exists\n    const collections = await qdrant.getCollections();\n    const exists = collections.collections.some(\n      (c) => c.name === COLLECTION_NAME\n    );\n\n    if (!exists) {\n      // Create collection with cosine similarity for OpenAI embeddings\n      await qdrant.createCollection(COLLECTION_NAME, {\n        vectors: {\n          size: EMBEDDING_DIMENSION,\n          distance: \"Cosine\",\n        },\n        // Enable payload indexing for common filter fields\n        optimizers_config: {\n          indexing_threshold: 0,\n        },\n      });\n\n      // Create payload indexes for efficient filtering\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"doc_id\",\n        field_schema: \"keyword\",\n      });\n\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"status\",\n        field_schema: \"keyword\",\n      });\n\n      await qdrant.createPayloadIndex(COLLECTION_NAME, {\n        field_name: \"doc_type\",\n        field_schema: \"keyword\",\n      });\n\n      console.log(`Created Qdrant collection: ${COLLECTION_NAME}`);\n    }\n  } catch (error) {\n    console.error(\"Error ensuring Qdrant collection:\", error);\n    throw error;\n  }\n}\n\n/**\n * Upsert vectors into the collection\n */\nexport async function upsertVectors(\n  points: Array<{\n    id: string;\n    vector: number[];\n    payload: Record<string, unknown>;\n  }>\n): Promise<void> {\n  const qdrant = getClient();\n\n  await qdrant.upsert(COLLECTION_NAME, {\n    points: points.map((p) => ({\n      id: p.id,\n      vector: p.vector,\n      payload: p.payload,\n    })),\n  });\n}\n\n/**\n * Search for similar vectors\n */\nexport async function searchVectors(\n  vector: number[],\n  limit: number = 10,\n  filter?: Record<string, unknown>\n): Promise<\n  Array<{\n    id: string;\n    score: number;\n    payload: Record<string, unknown>;\n  }>\n> {\n  const qdrant = getClient();\n\n  const results = await qdrant.search(COLLECTION_NAME, {\n    vector,\n    limit,\n    filter: filter as never,\n    with_payload: true,\n    score_threshold: 0.5, // Only return relevant results\n  });\n\n  return results.map((r) => ({\n    id: String(r.id),\n    score: r.score,\n    payload: (r.payload as Record<string, unknown>) || {},\n  }));\n}\n\n/**\n * Delete vectors by filter (e.g., by doc_id)\n */\nexport async function deleteVectorsByFilter(\n  filter: Record<string, unknown>\n): Promise<void> {\n  const qdrant = getClient();\n\n  await qdrant.delete(COLLECTION_NAME, {\n    filter: filter as never,\n  });\n}\n\n/**\n * Get collection info for debugging\n */\nexport async function getCollectionInfo(): Promise<unknown> {\n  const qdrant = getClient();\n  return qdrant.getCollection(COLLECTION_NAME);\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;AAAA;;AAEA,uBAAuB;AACvB,MAAM,aAAa,QAAQ,GAAG,CAAC,UAAU,IAAI;AAC7C,MAAM,iBAAiB,QAAQ,GAAG,CAAC,cAAc;AACjD,MAAM,kBAAkB,QAAQ,GAAG,CAAC,sBAAsB,IAAI;AAE9D,sDAAsD;AACtD,MAAM,sBAAsB;AAE5B,IAAI,SAA8B;AAK3B,SAAS;IACd,IAAI,CAAC,QAAQ;QACX,SAAS,IAAI,qMAAY,CAAC;YACxB,KAAK;YACL,QAAQ;YACR,qEAAqE;YACrE,oBAAoB;YACpB,sEAAsE;YACtE,SAAS;gBACP,cAAc;YAChB;QACF;IACF;IACA,OAAO;AACT;AAKO,SAAS;IACd,OAAO;AACT;AAKO,eAAe;IACpB,MAAM,SAAS;IAEf,IAAI;QACF,6BAA6B;QAC7B,MAAM,cAAc,MAAM,OAAO,cAAc;QAC/C,MAAM,SAAS,YAAY,WAAW,CAAC,IAAI,CACzC,CAAC,IAAM,EAAE,IAAI,KAAK;QAGpB,IAAI,CAAC,QAAQ;YACX,iEAAiE;YACjE,MAAM,OAAO,gBAAgB,CAAC,iBAAiB;gBAC7C,SAAS;oBACP,MAAM;oBACN,UAAU;gBACZ;gBACA,mDAAmD;gBACnD,mBAAmB;oBACjB,oBAAoB;gBACtB;YACF;YAEA,iDAAiD;YACjD,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,MAAM,OAAO,kBAAkB,CAAC,iBAAiB;gBAC/C,YAAY;gBACZ,cAAc;YAChB;YAEA,QAAQ,GAAG,CAAC,CAAC,2BAA2B,EAAE,iBAAiB;QAC7D;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,qCAAqC;QACnD,MAAM;IACR;AACF;AAKO,eAAe,cACpB,MAIE;IAEF,MAAM,SAAS;IAEf,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnC,QAAQ,OAAO,GAAG,CAAC,CAAC,IAAM,CAAC;gBACzB,IAAI,EAAE,EAAE;gBACR,QAAQ,EAAE,MAAM;gBAChB,SAAS,EAAE,OAAO;YACpB,CAAC;IACH;AACF;AAKO,eAAe,cACpB,MAAgB,EAChB,QAAgB,EAAE,EAClB,MAAgC;IAQhC,MAAM,SAAS;IAEf,MAAM,UAAU,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnD;QACA;QACA,QAAQ;QACR,cAAc;QACd,iBAAiB;IACnB;IAEA,OAAO,QAAQ,GAAG,CAAC,CAAC,IAAM,CAAC;YACzB,IAAI,OAAO,EAAE,EAAE;YACf,OAAO,EAAE,KAAK;YACd,SAAS,AAAC,EAAE,OAAO,IAAgC,CAAC;QACtD,CAAC;AACH;AAKO,eAAe,sBACpB,MAA+B;IAE/B,MAAM,SAAS;IAEf,MAAM,OAAO,MAAM,CAAC,iBAAiB;QACnC,QAAQ;IACV;AACF;AAKO,eAAe;IACpB,MAAM,SAAS;IACf,OAAO,OAAO,aAAa,CAAC;AAC9B"}},
    {"offset": {"line": 509, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/storage.ts"],"sourcesContent":["import * as Minio from \"minio\";\n\n// MinIO configuration\nconst MINIO_ENDPOINT = process.env.MINIO_ENDPOINT || \"localhost\";\nconst MINIO_PORT = parseInt(process.env.MINIO_PORT || \"9000\", 10);\n// Support both typical MinIO env names and username/password aliases\nconst MINIO_ACCESS_KEY =\n  process.env.MINIO_ACCESS_KEY ||\n  process.env.MINIO_USERNAME || // alias for username\n  \"\";\nconst MINIO_SECRET_KEY =\n  process.env.MINIO_SECRET_KEY ||\n  process.env.MINIO_PASSWORD || // alias for password\n  \"\";\nconst MINIO_BUCKET = process.env.MINIO_BUCKET || \"knowledge-docs\";\nconst MINIO_USE_SSL = process.env.MINIO_USE_SSL === \"true\";\n\nlet client: Minio.Client | null = null;\n\n/**\n * Get or create the MinIO client singleton\n */\nexport function getStorageClient(): Minio.Client {\n  if (!client) {\n    const { endPoint, port, useSSL } = resolveMinioEndpoint();\n\n    client = new Minio.Client({\n      endPoint,\n      port,\n      useSSL,\n      accessKey: MINIO_ACCESS_KEY,\n      secretKey: MINIO_SECRET_KEY,\n    });\n  }\n  return client;\n}\n\nfunction resolveMinioEndpoint(): {\n  endPoint: string;\n  port: number;\n  useSSL: boolean;\n} {\n  // If MINIO_ENDPOINT includes scheme, parse it; otherwise use raw values.\n  if (\n    MINIO_ENDPOINT.startsWith(\"http://\") ||\n    MINIO_ENDPOINT.startsWith(\"https://\")\n  ) {\n    try {\n      const url = new URL(MINIO_ENDPOINT);\n      const endPoint = url.hostname;\n      const useSSL = url.protocol === \"https:\";\n      const port =\n        url.port && Number(url.port) > 0\n          ? Number(url.port)\n          : useSSL\n            ? 443\n            : 80;\n      return { endPoint, port, useSSL };\n    } catch {\n      // Fall through to defaults\n    }\n  }\n\n  return {\n    endPoint: MINIO_ENDPOINT,\n    port: MINIO_PORT,\n    useSSL: MINIO_USE_SSL,\n  };\n}\n\n/**\n * Get the bucket name\n */\nexport function getBucketName(): string {\n  return MINIO_BUCKET;\n}\n\n/**\n * Ensure the bucket exists\n */\nexport async function ensureBucket(): Promise<void> {\n  const minio = getStorageClient();\n\n  const exists = await minio.bucketExists(MINIO_BUCKET);\n  if (!exists) {\n    await minio.makeBucket(MINIO_BUCKET);\n    console.log(`Created MinIO bucket: ${MINIO_BUCKET}`);\n  }\n}\n\n/**\n * Upload a file to storage\n */\nexport async function uploadFile(\n  objectName: string,\n  buffer: Buffer,\n  contentType: string = \"application/octet-stream\"\n): Promise<string> {\n  const minio = getStorageClient();\n\n  await minio.putObject(MINIO_BUCKET, objectName, buffer, buffer.length, {\n    \"Content-Type\": contentType,\n  });\n\n  return objectName;\n}\n\n/**\n * Upload original PDF document\n * Path: documents/{doc_id}/original.pdf\n */\nexport async function uploadDocument(\n  docId: string,\n  buffer: Buffer,\n  originalFilename: string\n): Promise<string> {\n  const ext = originalFilename.split(\".\").pop() || \"pdf\";\n  const objectName = `documents/${docId}/original.${ext}`;\n\n  await uploadFile(objectName, buffer, getMimeType(ext));\n  return objectName;\n}\n\n/**\n * Upload page image\n * Path: documents/{doc_id}/pages/page-{n}.png\n */\nexport async function uploadPageImage(\n  docId: string,\n  pageNumber: number,\n  buffer: Buffer\n): Promise<string> {\n  const objectName = `documents/${docId}/pages/page-${pageNumber}.png`;\n\n  await uploadFile(objectName, buffer, \"image/png\");\n  return objectName;\n}\n\n/**\n * Get a presigned URL for accessing a file\n * Default expiry: 1 hour (3600 seconds)\n */\nexport async function getSignedUrl(\n  objectName: string,\n  expirySeconds: number = 3600\n): Promise<string> {\n  const minio = getStorageClient();\n\n  const url = await minio.presignedGetObject(\n    MINIO_BUCKET,\n    objectName,\n    expirySeconds\n  );\n\n  return url;\n}\n\n/**\n * Delete a file from storage\n */\nexport async function deleteFile(objectName: string): Promise<void> {\n  const minio = getStorageClient();\n  await minio.removeObject(MINIO_BUCKET, objectName);\n}\n\n/**\n * Delete all files for a document\n */\nexport async function deleteDocumentFiles(docId: string): Promise<void> {\n  const minio = getStorageClient();\n  const prefix = `documents/${docId}/`;\n\n  const objectsList: string[] = [];\n  const stream = minio.listObjects(MINIO_BUCKET, prefix, true);\n\n  for await (const obj of stream) {\n    if (obj.name) {\n      objectsList.push(obj.name);\n    }\n  }\n\n  if (objectsList.length > 0) {\n    await minio.removeObjects(MINIO_BUCKET, objectsList);\n  }\n}\n\n/**\n * Check if a file exists\n */\nexport async function fileExists(objectName: string): Promise<boolean> {\n  const minio = getStorageClient();\n\n  try {\n    await minio.statObject(MINIO_BUCKET, objectName);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get file as buffer\n */\nexport async function getFile(objectName: string): Promise<Buffer> {\n  const minio = getStorageClient();\n\n  const stream = await minio.getObject(MINIO_BUCKET, objectName);\n  const chunks: Buffer[] = [];\n\n  for await (const chunk of stream) {\n    chunks.push(Buffer.from(chunk));\n  }\n\n  return Buffer.concat(chunks);\n}\n\n/**\n * Get MIME type from file extension\n */\nfunction getMimeType(ext: string): string {\n  const mimeTypes: Record<string, string> = {\n    pdf: \"application/pdf\",\n    png: \"image/png\",\n    jpg: \"image/jpeg\",\n    jpeg: \"image/jpeg\",\n    pptx: \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n    docx: \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n    xlsx: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n    txt: \"text/plain\",\n    json: \"application/json\",\n  };\n\n  return mimeTypes[ext.toLowerCase()] || \"application/octet-stream\";\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;AAAA;;;;;;AAEA,sBAAsB;AACtB,MAAM,iBAAiB,QAAQ,GAAG,CAAC,cAAc,IAAI;AACrD,MAAM,aAAa,SAAS,QAAQ,GAAG,CAAC,UAAU,IAAI,QAAQ;AAC9D,qEAAqE;AACrE,MAAM,mBACJ,QAAQ,GAAG,CAAC,gBAAgB,IAC5B,QAAQ,GAAG,CAAC,cAAc,IAAI,qBAAqB;AACnD;AACF,MAAM,mBACJ,QAAQ,GAAG,CAAC,gBAAgB,IAC5B,QAAQ,GAAG,CAAC,cAAc,IAAI,qBAAqB;AACnD;AACF,MAAM,eAAe,QAAQ,GAAG,CAAC,YAAY,IAAI;AACjD,MAAM,gBAAgB,QAAQ,GAAG,CAAC,aAAa,KAAK;AAEpD,IAAI,SAA8B;AAK3B,SAAS;IACd,IAAI,CAAC,QAAQ;QACX,MAAM,EAAE,QAAQ,EAAE,IAAI,EAAE,MAAM,EAAE,GAAG;QAEnC,SAAS,IAAI,oHAAY,CAAC;YACxB;YACA;YACA;YACA,WAAW;YACX,WAAW;QACb;IACF;IACA,OAAO;AACT;AAEA,SAAS;IAKP,yEAAyE;IACzE,IACE,eAAe,UAAU,CAAC,cAC1B,eAAe,UAAU,CAAC,aAC1B;QACA,IAAI;YACF,MAAM,MAAM,IAAI,IAAI;YACpB,MAAM,WAAW,IAAI,QAAQ;YAC7B,MAAM,SAAS,IAAI,QAAQ,KAAK;YAChC,MAAM,OACJ,IAAI,IAAI,IAAI,OAAO,IAAI,IAAI,IAAI,IAC3B,OAAO,IAAI,IAAI,IACf,SACE,MACA;YACR,OAAO;gBAAE;gBAAU;gBAAM;YAAO;QAClC,EAAE,OAAM;QACN,2BAA2B;QAC7B;IACF;IAEA,OAAO;QACL,UAAU;QACV,MAAM;QACN,QAAQ;IACV;AACF;AAKO,SAAS;IACd,OAAO;AACT;AAKO,eAAe;IACpB,MAAM,QAAQ;IAEd,MAAM,SAAS,MAAM,MAAM,YAAY,CAAC;IACxC,IAAI,CAAC,QAAQ;QACX,MAAM,MAAM,UAAU,CAAC;QACvB,QAAQ,GAAG,CAAC,CAAC,sBAAsB,EAAE,cAAc;IACrD;AACF;AAKO,eAAe,WACpB,UAAkB,EAClB,MAAc,EACd,cAAsB,0BAA0B;IAEhD,MAAM,QAAQ;IAEd,MAAM,MAAM,SAAS,CAAC,cAAc,YAAY,QAAQ,OAAO,MAAM,EAAE;QACrE,gBAAgB;IAClB;IAEA,OAAO;AACT;AAMO,eAAe,eACpB,KAAa,EACb,MAAc,EACd,gBAAwB;IAExB,MAAM,MAAM,iBAAiB,KAAK,CAAC,KAAK,GAAG,MAAM;IACjD,MAAM,aAAa,CAAC,UAAU,EAAE,MAAM,UAAU,EAAE,KAAK;IAEvD,MAAM,WAAW,YAAY,QAAQ,YAAY;IACjD,OAAO;AACT;AAMO,eAAe,gBACpB,KAAa,EACb,UAAkB,EAClB,MAAc;IAEd,MAAM,aAAa,CAAC,UAAU,EAAE,MAAM,YAAY,EAAE,WAAW,IAAI,CAAC;IAEpE,MAAM,WAAW,YAAY,QAAQ;IACrC,OAAO;AACT;AAMO,eAAe,aACpB,UAAkB,EAClB,gBAAwB,IAAI;IAE5B,MAAM,QAAQ;IAEd,MAAM,MAAM,MAAM,MAAM,kBAAkB,CACxC,cACA,YACA;IAGF,OAAO;AACT;AAKO,eAAe,WAAW,UAAkB;IACjD,MAAM,QAAQ;IACd,MAAM,MAAM,YAAY,CAAC,cAAc;AACzC;AAKO,eAAe,oBAAoB,KAAa;IACrD,MAAM,QAAQ;IACd,MAAM,SAAS,CAAC,UAAU,EAAE,MAAM,CAAC,CAAC;IAEpC,MAAM,cAAwB,EAAE;IAChC,MAAM,SAAS,MAAM,WAAW,CAAC,cAAc,QAAQ;IAEvD,WAAW,MAAM,OAAO,OAAQ;QAC9B,IAAI,IAAI,IAAI,EAAE;YACZ,YAAY,IAAI,CAAC,IAAI,IAAI;QAC3B;IACF;IAEA,IAAI,YAAY,MAAM,GAAG,GAAG;QAC1B,MAAM,MAAM,aAAa,CAAC,cAAc;IAC1C;AACF;AAKO,eAAe,WAAW,UAAkB;IACjD,MAAM,QAAQ;IAEd,IAAI;QACF,MAAM,MAAM,UAAU,CAAC,cAAc;QACrC,OAAO;IACT,EAAE,OAAM;QACN,OAAO;IACT;AACF;AAKO,eAAe,QAAQ,UAAkB;IAC9C,MAAM,QAAQ;IAEd,MAAM,SAAS,MAAM,MAAM,SAAS,CAAC,cAAc;IACnD,MAAM,SAAmB,EAAE;IAE3B,WAAW,MAAM,SAAS,OAAQ;QAChC,OAAO,IAAI,CAAC,OAAO,IAAI,CAAC;IAC1B;IAEA,OAAO,OAAO,MAAM,CAAC;AACvB;AAEA;;CAEC,GACD,SAAS,YAAY,GAAW;IAC9B,MAAM,YAAoC;QACxC,KAAK;QACL,KAAK;QACL,KAAK;QACL,MAAM;QACN,MAAM;QACN,MAAM;QACN,MAAM;QACN,KAAK;QACL,MAAM;IACR;IAEA,OAAO,SAAS,CAAC,IAAI,WAAW,GAAG,IAAI;AACzC"}},
    {"offset": {"line": 679, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/lib/retrieval.ts"],"sourcesContent":["import { embedText } from \"./embeddings\";\nimport { searchVectors } from \"./qdrant\";\nimport { getSignedUrl } from \"./storage\";\n\nexport interface RetrievedChunk {\n  chunkId: string;\n  docId: string;\n  docName: string;\n  pageNumber: number;\n  chunkIndex: number;\n  text: string;\n  imageUrl: string;\n  score: number;\n  parserUsed: string;\n}\n\nexport interface RetrievalOptions {\n  topK?: number;\n  docId?: string; // Filter by specific document\n  status?: string; // Filter by status (default: \"ready\")\n  diversify?: boolean; // Apply MMR diversification\n  diversityWeight?: number; // MMR lambda (0-1, higher = more diverse)\n}\n\nexport interface CitationInfo {\n  docName: string;\n  pageNumber: number;\n  imageUrl?: string; // Signed URL for page image\n  snippet: string;\n}\n\n/**\n * Retrieve relevant context for a query\n */\nexport async function retrieveContext(\n  query: string,\n  options: RetrievalOptions = {}\n): Promise<RetrievedChunk[]> {\n  const {\n    topK = 10,\n    docId,\n    status = \"ready\",\n    diversify = true,\n    diversityWeight = 0.7,\n  } = options;\n\n  // Generate query embedding\n  const queryEmbedding = await embedText(query);\n\n  // Build filter\n  const filter: Record<string, unknown> = {\n    must: [{ key: \"status\", match: { value: status } }],\n  };\n\n  if (docId) {\n    (filter.must as Array<unknown>).push({\n      key: \"doc_id\",\n      match: { value: docId },\n    });\n  }\n\n  // Retrieve more candidates if we're going to diversify\n  const candidateCount = diversify ? topK * 3 : topK;\n\n  // Search Qdrant\n  const results = await searchVectors(queryEmbedding, candidateCount, filter);\n\n  // Map results to chunks\n  let chunks: RetrievedChunk[] = results.map((r) => ({\n    chunkId: r.id,\n    docId: (r.payload.doc_id as string) || \"\",\n    docName: (r.payload.doc_name as string) || \"\",\n    pageNumber: (r.payload.page_number as number) || 0,\n    chunkIndex: (r.payload.chunk_index as number) || 0,\n    text: (r.payload.text as string) || \"\",\n    imageUrl: (r.payload.image_url as string) || \"\",\n    score: r.score,\n    parserUsed: (r.payload.parser_used as string) || \"\",\n  }));\n\n  // Apply MMR diversification if enabled\n  if (diversify && chunks.length > topK) {\n    chunks = applyMMR(chunks, topK, diversityWeight);\n  }\n\n  // Deduplicate by page (keep highest scoring chunk per page)\n  chunks = deduplicateByPage(chunks);\n\n  return chunks.slice(0, topK);\n}\n\n/**\n * Apply Maximal Marginal Relevance (MMR) to diversify results\n * This balances relevance (score) with diversity (avoid redundant chunks)\n */\nfunction applyMMR(\n  chunks: RetrievedChunk[],\n  topK: number,\n  lambda: number\n): RetrievedChunk[] {\n  if (chunks.length <= topK) return chunks;\n\n  const selected: RetrievedChunk[] = [];\n  const remaining = new Set(chunks.map((_, i) => i));\n\n  // Start with the highest scoring chunk\n  const firstIdx = 0;\n  selected.push(chunks[firstIdx]);\n  remaining.delete(firstIdx);\n\n  while (selected.length < topK && remaining.size > 0) {\n    let bestIdx = -1;\n    let bestScore = -Infinity;\n\n    for (const idx of remaining) {\n      const candidate = chunks[idx];\n\n      // Calculate relevance score (normalized)\n      const relevance = candidate.score;\n\n      // Calculate max similarity to already selected chunks\n      let maxSimilarity = 0;\n      for (const sel of selected) {\n        const sim = textSimilarity(candidate.text, sel.text);\n        maxSimilarity = Math.max(maxSimilarity, sim);\n      }\n\n      // MMR score: λ * relevance - (1-λ) * max_similarity\n      const mmrScore = lambda * relevance - (1 - lambda) * maxSimilarity;\n\n      if (mmrScore > bestScore) {\n        bestScore = mmrScore;\n        bestIdx = idx;\n      }\n    }\n\n    if (bestIdx !== -1) {\n      selected.push(chunks[bestIdx]);\n      remaining.delete(bestIdx);\n    } else {\n      break;\n    }\n  }\n\n  return selected;\n}\n\n/**\n * Simple text similarity based on Jaccard coefficient of words\n */\nfunction textSimilarity(text1: string, text2: string): number {\n  const words1 = new Set(text1.toLowerCase().split(/\\s+/));\n  const words2 = new Set(text2.toLowerCase().split(/\\s+/));\n\n  const intersection = new Set([...words1].filter((w) => words2.has(w)));\n  const union = new Set([...words1, ...words2]);\n\n  return intersection.size / union.size;\n}\n\n/**\n * Deduplicate chunks by page, keeping the highest scoring chunk per page\n */\nfunction deduplicateByPage(chunks: RetrievedChunk[]): RetrievedChunk[] {\n  const pageMap = new Map<string, RetrievedChunk>();\n\n  for (const chunk of chunks) {\n    const key = `${chunk.docId}:${chunk.pageNumber}`;\n    const existing = pageMap.get(key);\n\n    if (!existing || chunk.score > existing.score) {\n      pageMap.set(key, chunk);\n    }\n  }\n\n  // Sort by original score descending\n  return Array.from(pageMap.values()).sort((a, b) => b.score - a.score);\n}\n\n/**\n * Format retrieved chunks for use in LLM context\n */\nexport function formatContextForLLM(chunks: RetrievedChunk[]): string {\n  if (chunks.length === 0) {\n    return \"No relevant context found in the knowledge base.\";\n  }\n\n  const formattedChunks = chunks.map((chunk, index) => {\n    return `[${index + 1}] Document: \"${chunk.docName}\", Page ${chunk.pageNumber}\n${chunk.text}`;\n  });\n\n  return formattedChunks.join(\"\\n\\n---\\n\\n\");\n}\n\n/**\n * Extract citation information from retrieved chunks\n */\nexport async function extractCitations(\n  chunks: RetrievedChunk[]\n): Promise<CitationInfo[]> {\n  const citations: CitationInfo[] = [];\n\n  for (const chunk of chunks) {\n    // Get signed URL for page image\n    let signedImageUrl: string | undefined;\n    if (chunk.imageUrl) {\n      try {\n        signedImageUrl = await getSignedUrl(chunk.imageUrl);\n      } catch {\n        // Image URL generation failed, continue without it\n      }\n    }\n\n    citations.push({\n      docName: chunk.docName,\n      pageNumber: chunk.pageNumber,\n      imageUrl: signedImageUrl,\n      snippet:\n        chunk.text.length > 200\n          ? chunk.text.substring(0, 200) + \"...\"\n          : chunk.text,\n    });\n  }\n\n  return citations;\n}\n\n/**\n * Check if query can be answered from the knowledge base\n */\nexport async function hasRelevantContext(\n  query: string,\n  threshold: number = 0.6\n): Promise<boolean> {\n  const chunks = await retrieveContext(query, { topK: 3 });\n  return chunks.length > 0 && chunks[0].score >= threshold;\n}\n"],"names":[],"mappings":";;;;;;;;;;AAAA;AACA;AACA;;;;;;;;AAgCO,eAAe,gBACpB,KAAa,EACb,UAA4B,CAAC,CAAC;IAE9B,MAAM,EACJ,OAAO,EAAE,EACT,KAAK,EACL,SAAS,OAAO,EAChB,YAAY,IAAI,EAChB,kBAAkB,GAAG,EACtB,GAAG;IAEJ,2BAA2B;IAC3B,MAAM,iBAAiB,MAAM,IAAA,uIAAS,EAAC;IAEvC,eAAe;IACf,MAAM,SAAkC;QACtC,MAAM;YAAC;gBAAE,KAAK;gBAAU,OAAO;oBAAE,OAAO;gBAAO;YAAE;SAAE;IACrD;IAEA,IAAI,OAAO;QACR,OAAO,IAAI,CAAoB,IAAI,CAAC;YACnC,KAAK;YACL,OAAO;gBAAE,OAAO;YAAM;QACxB;IACF;IAEA,uDAAuD;IACvD,MAAM,iBAAiB,YAAY,OAAO,IAAI;IAE9C,gBAAgB;IAChB,MAAM,UAAU,MAAM,IAAA,uIAAa,EAAC,gBAAgB,gBAAgB;IAEpE,wBAAwB;IACxB,IAAI,SAA2B,QAAQ,GAAG,CAAC,CAAC,IAAM,CAAC;YACjD,SAAS,EAAE,EAAE;YACb,OAAO,AAAC,EAAE,OAAO,CAAC,MAAM,IAAe;YACvC,SAAS,AAAC,EAAE,OAAO,CAAC,QAAQ,IAAe;YAC3C,YAAY,AAAC,EAAE,OAAO,CAAC,WAAW,IAAe;YACjD,YAAY,AAAC,EAAE,OAAO,CAAC,WAAW,IAAe;YACjD,MAAM,AAAC,EAAE,OAAO,CAAC,IAAI,IAAe;YACpC,UAAU,AAAC,EAAE,OAAO,CAAC,SAAS,IAAe;YAC7C,OAAO,EAAE,KAAK;YACd,YAAY,AAAC,EAAE,OAAO,CAAC,WAAW,IAAe;QACnD,CAAC;IAED,uCAAuC;IACvC,IAAI,aAAa,OAAO,MAAM,GAAG,MAAM;QACrC,SAAS,SAAS,QAAQ,MAAM;IAClC;IAEA,4DAA4D;IAC5D,SAAS,kBAAkB;IAE3B,OAAO,OAAO,KAAK,CAAC,GAAG;AACzB;AAEA;;;CAGC,GACD,SAAS,SACP,MAAwB,EACxB,IAAY,EACZ,MAAc;IAEd,IAAI,OAAO,MAAM,IAAI,MAAM,OAAO;IAElC,MAAM,WAA6B,EAAE;IACrC,MAAM,YAAY,IAAI,IAAI,OAAO,GAAG,CAAC,CAAC,GAAG,IAAM;IAE/C,uCAAuC;IACvC,MAAM,WAAW;IACjB,SAAS,IAAI,CAAC,MAAM,CAAC,SAAS;IAC9B,UAAU,MAAM,CAAC;IAEjB,MAAO,SAAS,MAAM,GAAG,QAAQ,UAAU,IAAI,GAAG,EAAG;QACnD,IAAI,UAAU,CAAC;QACf,IAAI,YAAY,CAAC;QAEjB,KAAK,MAAM,OAAO,UAAW;YAC3B,MAAM,YAAY,MAAM,CAAC,IAAI;YAE7B,yCAAyC;YACzC,MAAM,YAAY,UAAU,KAAK;YAEjC,sDAAsD;YACtD,IAAI,gBAAgB;YACpB,KAAK,MAAM,OAAO,SAAU;gBAC1B,MAAM,MAAM,eAAe,UAAU,IAAI,EAAE,IAAI,IAAI;gBACnD,gBAAgB,KAAK,GAAG,CAAC,eAAe;YAC1C;YAEA,oDAAoD;YACpD,MAAM,WAAW,SAAS,YAAY,CAAC,IAAI,MAAM,IAAI;YAErD,IAAI,WAAW,WAAW;gBACxB,YAAY;gBACZ,UAAU;YACZ;QACF;QAEA,IAAI,YAAY,CAAC,GAAG;YAClB,SAAS,IAAI,CAAC,MAAM,CAAC,QAAQ;YAC7B,UAAU,MAAM,CAAC;QACnB,OAAO;YACL;QACF;IACF;IAEA,OAAO;AACT;AAEA;;CAEC,GACD,SAAS,eAAe,KAAa,EAAE,KAAa;IAClD,MAAM,SAAS,IAAI,IAAI,MAAM,WAAW,GAAG,KAAK,CAAC;IACjD,MAAM,SAAS,IAAI,IAAI,MAAM,WAAW,GAAG,KAAK,CAAC;IAEjD,MAAM,eAAe,IAAI,IAAI;WAAI;KAAO,CAAC,MAAM,CAAC,CAAC,IAAM,OAAO,GAAG,CAAC;IAClE,MAAM,QAAQ,IAAI,IAAI;WAAI;WAAW;KAAO;IAE5C,OAAO,aAAa,IAAI,GAAG,MAAM,IAAI;AACvC;AAEA;;CAEC,GACD,SAAS,kBAAkB,MAAwB;IACjD,MAAM,UAAU,IAAI;IAEpB,KAAK,MAAM,SAAS,OAAQ;QAC1B,MAAM,MAAM,GAAG,MAAM,KAAK,CAAC,CAAC,EAAE,MAAM,UAAU,EAAE;QAChD,MAAM,WAAW,QAAQ,GAAG,CAAC;QAE7B,IAAI,CAAC,YAAY,MAAM,KAAK,GAAG,SAAS,KAAK,EAAE;YAC7C,QAAQ,GAAG,CAAC,KAAK;QACnB;IACF;IAEA,oCAAoC;IACpC,OAAO,MAAM,IAAI,CAAC,QAAQ,MAAM,IAAI,IAAI,CAAC,CAAC,GAAG,IAAM,EAAE,KAAK,GAAG,EAAE,KAAK;AACtE;AAKO,SAAS,oBAAoB,MAAwB;IAC1D,IAAI,OAAO,MAAM,KAAK,GAAG;QACvB,OAAO;IACT;IAEA,MAAM,kBAAkB,OAAO,GAAG,CAAC,CAAC,OAAO;QACzC,OAAO,CAAC,CAAC,EAAE,QAAQ,EAAE,aAAa,EAAE,MAAM,OAAO,CAAC,QAAQ,EAAE,MAAM,UAAU,CAAC;AACjF,EAAE,MAAM,IAAI,EAAE;IACZ;IAEA,OAAO,gBAAgB,IAAI,CAAC;AAC9B;AAKO,eAAe,iBACpB,MAAwB;IAExB,MAAM,YAA4B,EAAE;IAEpC,KAAK,MAAM,SAAS,OAAQ;QAC1B,gCAAgC;QAChC,IAAI;QACJ,IAAI,MAAM,QAAQ,EAAE;YAClB,IAAI;gBACF,iBAAiB,MAAM,IAAA,uIAAY,EAAC,MAAM,QAAQ;YACpD,EAAE,OAAM;YACN,mDAAmD;YACrD;QACF;QAEA,UAAU,IAAI,CAAC;YACb,SAAS,MAAM,OAAO;YACtB,YAAY,MAAM,UAAU;YAC5B,UAAU;YACV,SACE,MAAM,IAAI,CAAC,MAAM,GAAG,MAChB,MAAM,IAAI,CAAC,SAAS,CAAC,GAAG,OAAO,QAC/B,MAAM,IAAI;QAClB;IACF;IAEA,OAAO;AACT;AAKO,eAAe,mBACpB,KAAa,EACb,YAAoB,GAAG;IAEvB,MAAM,SAAS,MAAM,gBAAgB,OAAO;QAAE,MAAM;IAAE;IACtD,OAAO,OAAO,MAAM,GAAG,KAAK,MAAM,CAAC,EAAE,CAAC,KAAK,IAAI;AACjD"}},
    {"offset": {"line": 858, "column": 0}, "map": {"version":3,"sources":["file:///Volumes/External/dev/Projects/Work%20Projects/RAG/new_rag/questions/src/app/api/copilotkit/route.ts"],"sourcesContent":["import {\n  CopilotRuntime,\n  LangChainAdapter,\n  copilotRuntimeNextJSAppRouterEndpoint,\n} from \"@copilotkit/runtime\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage, SystemMessage, AIMessage } from \"@langchain/core/messages\";\nimport { NextRequest } from \"next/server\";\nimport {\n  retrieveContext,\n  formatContextForLLM,\n  extractCitations,\n  type RetrievedChunk,\n} from \"@/lib/retrieval\";\n\n// Create the LangChain model\nconst model = new ChatOpenAI({\n  modelName: \"gpt-4o\",\n  temperature: 0.3, // Lower temperature for more factual responses\n});\n\n// RAG System Prompt\nconst RAG_SYSTEM_PROMPT = `You are an internal knowledge assistant. Answer questions ONLY using the provided context from our knowledge base.\n\nRULES:\n1. Only answer from the context provided below - do not use external knowledge\n2. Cite your sources using this format: [Doc: {document name}, Page: {page number}]\n3. Include citations inline with your response for every piece of information\n4. If the answer is NOT in the context, say exactly: \"I don't have information about that in my knowledge base.\"\n5. Be concise, accurate, and helpful\n6. If multiple documents discuss the same topic, synthesize the information and cite all relevant sources\n7. Do not make up or infer information not explicitly stated in the context\n\nCONTEXT FROM KNOWLEDGE BASE:\n{context}\n\nRemember: Only use information from the context above. Always cite your sources.`;\n\n// Store citations for the current response (used by the UI)\nlet lastCitations: Array<{\n  docName: string;\n  pageNumber: number;\n  imageUrl?: string;\n  snippet: string;\n}> = [];\n\n// Create the service adapter using LangChain with RAG\nconst serviceAdapter = new LangChainAdapter({\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  chainFn: async ({ messages, tools }): Promise<any> => {\n    // Extract the last user message for retrieval\n    const lastMessage = messages[messages.length - 1];\n    const userQuery =\n      typeof lastMessage.content === \"string\"\n        ? lastMessage.content\n        : Array.isArray(lastMessage.content)\n          ? lastMessage.content\n              .filter((c): c is { type: \"text\"; text: string } => c.type === \"text\")\n              .map((c) => c.text)\n              .join(\" \")\n          : \"\";\n\n    // Retrieve relevant context\n    let chunks: RetrievedChunk[] = [];\n    let contextText = \"\";\n\n    if (userQuery.trim()) {\n      try {\n        chunks = await retrieveContext(userQuery, {\n          topK: 8,\n          diversify: true,\n          diversityWeight: 0.7,\n        });\n        contextText = formatContextForLLM(chunks);\n\n        // Extract citations for the UI\n        lastCitations = await extractCitations(chunks);\n      } catch (error) {\n        console.error(\"Error retrieving context:\", error);\n        contextText =\n          \"Error retrieving from knowledge base. Please try again.\";\n        lastCitations = [];\n      }\n    }\n\n    // Build the system prompt with context\n    const systemPrompt = RAG_SYSTEM_PROMPT.replace(\"{context}\", contextText);\n\n    // Prepend system message and transform messages\n    const langChainMessages = [\n      new SystemMessage(systemPrompt),\n      ...messages.slice(0, -1).map((msg) => {\n        const content =\n          typeof msg.content === \"string\"\n            ? msg.content\n            : Array.isArray(msg.content)\n              ? msg.content\n                  .filter((c): c is { type: \"text\"; text: string } => c.type === \"text\")\n                  .map((c) => c.text)\n                  .join(\" \")\n              : \"\";\n\n        // Check message type using getType() method or _getType()\n        const msgType = String(msg.getType?.() || (msg as unknown as { _getType?: () => string })._getType?.() || \"human\");\n        if (msgType === \"human\") {\n          return new HumanMessage(content);\n        } else if (msgType === \"ai\") {\n          return new AIMessage(content);\n        }\n        return new HumanMessage(content);\n      }),\n      new HumanMessage(userQuery),\n    ];\n\n    // Bind tools if available\n    const modelWithTools = tools?.length ? model.bindTools(tools) : model;\n    return modelWithTools.stream(langChainMessages);\n  },\n});\n\n// Create the CopilotKit runtime\nconst runtime = new CopilotRuntime();\n\nexport const POST = async (req: NextRequest) => {\n  const { handleRequest } = copilotRuntimeNextJSAppRouterEndpoint({\n    runtime,\n    serviceAdapter,\n    endpoint: \"/api/copilotkit\",\n  });\n\n  return handleRequest(req);\n};\n\n// Export citations endpoint for UI to fetch\nexport const GET = async () => {\n  return Response.json({ citations: lastCitations });\n};\n"],"names":[],"mappings":";;;;;;AAAA;AAKA;AAAA;AACA;AAAA;AAAA;AAAA;AAEA;;;;;;;;;AAOA,6BAA6B;AAC7B,MAAM,QAAQ,IAAI,qLAAU,CAAC;IAC3B,WAAW;IACX,aAAa;AACf;AAEA,oBAAoB;AACpB,MAAM,oBAAoB,CAAC;;;;;;;;;;;;;;gFAcqD,CAAC;AAEjF,4DAA4D;AAC5D,IAAI,gBAKC,EAAE;AAEP,sDAAsD;AACtD,MAAM,iBAAiB,IAAI,8KAAgB,CAAC;IAC1C,8DAA8D;IAC9D,SAAS,OAAO,EAAE,QAAQ,EAAE,KAAK,EAAE;QACjC,8CAA8C;QAC9C,MAAM,cAAc,QAAQ,CAAC,SAAS,MAAM,GAAG,EAAE;QACjD,MAAM,YACJ,OAAO,YAAY,OAAO,KAAK,WAC3B,YAAY,OAAO,GACnB,MAAM,OAAO,CAAC,YAAY,OAAO,IAC/B,YAAY,OAAO,CAChB,MAAM,CAAC,CAAC,IAA2C,EAAE,IAAI,KAAK,QAC9D,GAAG,CAAC,CAAC,IAAM,EAAE,IAAI,EACjB,IAAI,CAAC,OACR;QAER,4BAA4B;QAC5B,IAAI,SAA2B,EAAE;QACjC,IAAI,cAAc;QAElB,IAAI,UAAU,IAAI,IAAI;YACpB,IAAI;gBACF,SAAS,MAAM,IAAA,4IAAe,EAAC,WAAW;oBACxC,MAAM;oBACN,WAAW;oBACX,iBAAiB;gBACnB;gBACA,cAAc,IAAA,gJAAmB,EAAC;gBAElC,+BAA+B;gBAC/B,gBAAgB,MAAM,IAAA,6IAAgB,EAAC;YACzC,EAAE,OAAO,OAAO;gBACd,QAAQ,KAAK,CAAC,6BAA6B;gBAC3C,cACE;gBACF,gBAAgB,EAAE;YACpB;QACF;QAEA,uCAAuC;QACvC,MAAM,eAAe,kBAAkB,OAAO,CAAC,aAAa;QAE5D,gDAAgD;QAChD,MAAM,oBAAoB;YACxB,IAAI,oLAAa,CAAC;eACf,SAAS,KAAK,CAAC,GAAG,CAAC,GAAG,GAAG,CAAC,CAAC;gBAC5B,MAAM,UACJ,OAAO,IAAI,OAAO,KAAK,WACnB,IAAI,OAAO,GACX,MAAM,OAAO,CAAC,IAAI,OAAO,IACvB,IAAI,OAAO,CACR,MAAM,CAAC,CAAC,IAA2C,EAAE,IAAI,KAAK,QAC9D,GAAG,CAAC,CAAC,IAAM,EAAE,IAAI,EACjB,IAAI,CAAC,OACR;gBAER,0DAA0D;gBAC1D,MAAM,UAAU,OAAO,IAAI,OAAO,QAAQ,AAAC,IAA+C,QAAQ,QAAQ;gBAC1G,IAAI,YAAY,SAAS;oBACvB,OAAO,IAAI,kLAAY,CAAC;gBAC1B,OAAO,IAAI,YAAY,MAAM;oBAC3B,OAAO,IAAI,4KAAS,CAAC;gBACvB;gBACA,OAAO,IAAI,kLAAY,CAAC;YAC1B;YACA,IAAI,kLAAY,CAAC;SAClB;QAED,0BAA0B;QAC1B,MAAM,iBAAiB,OAAO,SAAS,MAAM,SAAS,CAAC,SAAS;QAChE,OAAO,eAAe,MAAM,CAAC;IAC/B;AACF;AAEA,gCAAgC;AAChC,MAAM,UAAU,IAAI,4KAAc;AAE3B,MAAM,OAAO,OAAO;IACzB,MAAM,EAAE,aAAa,EAAE,GAAG,IAAA,mMAAqC,EAAC;QAC9D;QACA;QACA,UAAU;IACZ;IAEA,OAAO,cAAc;AACvB;AAGO,MAAM,MAAM;IACjB,OAAO,SAAS,IAAI,CAAC;QAAE,WAAW;IAAc;AAClD"}}]
}